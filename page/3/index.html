<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="Chenxr&#39;s blogs">
<meta property="og:url" content="http://yoursite.com/page/3/index.html">
<meta property="og:site_name" content="Chenxr&#39;s blogs">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chenxr&#39;s blogs">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/3/">





  <title>Chenxr's blogs</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chenxr's blogs</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/数据挖掘-数据(预)处理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/数据挖掘-数据(预)处理/" itemprop="url">数据挖掘-数据(预)处理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:44:59+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<p>数据清洗</p>
<p>​    缺失值处理</p>
<p>​    不处理</p>
<p>​    删除记录</p>
<p>​    插补（均值/中位数/众数，使用固定值，最近邻插补，回归方法，插值法）（拉格朗日，牛顿，hermite，分段，样条）</p>
<p>异常值处理</p>
<p>​    不处理，删除记录，视为缺失值，平均值修正</p>
<p>数据集成</p>
<p>实体识别（同名同义，异名同义，单位不统一）</p>
<p>冗余属性识别</p>
<p>数据变换</p>
<p>​    简单函数变化</p>
<p>​    数据规范化/标准化（最大-最小规范化，零-均值规范化，小数定标规范化，归一化）</p>
<p>​    连续属性离散化（等宽法，等频法，聚类法）</p>
<p>属性构造</p>
<p>​    小波变换</p>
<p>数据规约</p>
<p>​    属性规约（合并属性，逐步向前选择，逐步向后删除，决策树归纳，主成分分析）</p>
<p>​    数值规约（直方图，聚类，抽样，参数回归）</p>
<p>数据融合</p>
<h1 id="聚集-Aggregation"><a href="#聚集-Aggregation" class="headerlink" title="聚集 Aggregation"></a>聚集 Aggregation</h1><p>将两个或多个对象合并成单个对象，用于精简数据，但是难免会丢失细节</p>
<p>但是可以让数据变得“稳定”，即更小的方差</p>
<p>比如将月降水量聚集为年降水量</p>
<p><img src="/2019/03/08/数据挖掘-数据(预)处理/image-20190811124005265.png" alt="image-20190811124005265"></p>
<h1 id="采样-Sampling"><a href="#采样-Sampling" class="headerlink" title="采样 Sampling"></a>采样 Sampling</h1><p>初步调查中获取全部的数据太难，因此需要采样</p>
<p>数据挖掘/分析时使用所有数据太耗费时间和资源，因此也需要采样</p>
<p>一个样本是<strong>representative</strong>的，如果它能够有近似于整体数据的性质</p>
<h2 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h2><ul>
<li>简单随机抽样 Simple Random Sampling<ul>
<li>无放回 Sampling without replacement</li>
<li>有放回 Sampling with replacement</li>
</ul>
</li>
<li>分层抽样 Stratified sampling</li>
<li>蒙特卡洛方法（见<code>数据科学-蒙塔卡洛方法</code>）</li>
</ul>
<h1 id="降维-Dimensionality-Reduction"><a href="#降维-Dimensionality-Reduction" class="headerlink" title="降维 Dimensionality Reduction"></a>降维 Dimensionality Reduction</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><ul>
<li><p>避免维度灾难</p>
</li>
<li><p>减少分析所用的时间和资源</p>
</li>
<li><p>更容易可视化</p>
</li>
<li><p>可能会减少冗余的特征</p>
</li>
</ul>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><ul>
<li>主成分分析 Principal Components Analysis (PCA)</li>
<li>奇异值分解Singular Value Decomposition(SVD)</li>
</ul>
<p>详见<code>数据科学-特征工程</code></p>
<h1 id="特征子集选择-Feature-subset-selection"><a href="#特征子集选择-Feature-subset-selection" class="headerlink" title="特征子集选择 Feature subset selection"></a>特征子集选择 Feature subset selection</h1><p>另一种降维方法</p>
<p>详见<code>数据科学-特征工程</code></p>
<h1 id="特征创建-Feature-creation"><a href="#特征创建-Feature-creation" class="headerlink" title="特征创建 Feature creation"></a>特征创建 Feature creation</h1><p>详见<code>`数据科学-特征工程</code></p>
<h1 id="离散化和二值化-Discretization-and-Binarization"><a href="#离散化和二值化-Discretization-and-Binarization" class="headerlink" title="离散化和二值化 Discretization and Binarization"></a>离散化和二值化 Discretization and Binarization</h1><p>离散化将连续属性变为离散属性，常用于分类任务中</p>
<p>二值化将连续属性或类别属性变为(一个或一串)二进制变量，通常用于关联分析</p>
<p>通常先将连续属性变为类别属性，再变为二进制变量，比如身高变为高中矮</p>
<h1 id="属性转换-Attribute-Transformation"><a href="#属性转换-Attribute-Transformation" class="headerlink" title="属性转换 Attribute Transformation"></a>属性转换 Attribute Transformation</h1><p>属性转换是指通过一个函数，将某个属性的值映射为新的值，比如指数函数，对数函数，绝对值，e的指数函数等</p>
<h2 id="归一化-Normalization"><a href="#归一化-Normalization" class="headerlink" title="归一化 Normalization"></a>归一化 Normalization</h2><h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><ul>
<li><p>归一化后加快了梯度下降求最优解的速度</p>
</li>
<li><p>归一化有可能提高精度</p>
</li>
</ul>
<h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><ul>
<li>线性归一化 </li>
</ul>
<p>这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min</p>
<ul>
<li>标准差标准化 standardization</li>
</ul>
<p>经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：</p>
<p>其中μ为所有样本数据的均值，σ为所有样本数据的标准差</p>
<ul>
<li><p>非线性归一化 </p>
<p>  经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/数据挖掘-蒙塔卡洛方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/数据挖掘-蒙塔卡洛方法/" itemprop="url">数据挖掘-蒙塔卡洛方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:44:59+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="求积分-Evaluating-Integrals"><a href="#求积分-Evaluating-Integrals" class="headerlink" title="求积分 Evaluating Integrals"></a>求积分 Evaluating Integrals</h1><p>函数的期望值为</p>
<script type="math/tex; mode=display">
E[f(X)] = \int f(X)P_X(X)dX</script><p>方差为</p>
<script type="math/tex; mode=display">
Var(X) = E[(X-E(X))^2]\\
= E(X^2) - E(X)^2</script><p>要求的积分为</p>
<script type="math/tex; mode=display">
F = \int_a^b f(x)dx</script><p><img src="/2019/03/08/数据挖掘-蒙塔卡洛方法/image-20190811125540738.png" alt="image-20190811125540738"></p>
<p>蒙特卡洛的思想</p>
<p><img src="/2019/03/08/数据挖掘-蒙塔卡洛方法/image-20190811125710932.png" alt="image-20190811125710932"></p>
<script type="math/tex; mode=display">
<F^N> = (b-a)\frac1{N}\sum_{i=1}^Nf(X_i)\\
X_i = a + \xi(b-a)\\
\xi 是[0,1]中均匀分布的随机数</script><p>其中</p>
<script type="math/tex; mode=display">
Pr(\lim_{N \to \infty}<F^N> = F) = 1</script><script type="math/tex; mode=display">
E[<F^N>] = E[(b-a)\frac1{N}\sum_{i=1}^Nf(x_i)] \\
 = (b-a)\frac1{N}\sum_{i=1}^NE[f(x_i)] \\
 = (b-a)\frac1{N}\sum_{i=1}^N \int_a^bf(x)pdf(x)dx \\
 = \frac1{N}\sum_{i=1}^N \int_a^bf(x)dx \\
 = \int_a^bf(x)dx \\
 = F</script><p>还可以泛化到任意pdf中</p>
<script type="math/tex; mode=display">
<F^N> = \frac1{N}\sum_{i=1}^N \frac {f(X_i)}{pdf(X_i)}\\</script><script type="math/tex; mode=display">
E[<F^N>] = E[\frac1{N}\sum_{i=1}^N\frac {f(X_i)}{pdf(X_i)}] \\
 = \frac1{N}\sum_{i=1}^NE[\frac {f(X_i)}{pdf(X_i)}] \\
 = \frac1{N}\sum_{i=1}^N \int_\Omega \frac {f(x)pdf(x)}{pdf(x)}dx \\
 = \frac1{N}\sum_{i=1}^N \int_\Omega f(x)dx \\
 = \int_\Omega f(x)dx \\
 = F</script>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/深度学习-Pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/深度学习-Pytorch/" itemprop="url">深度学习-Pytorch</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:44:59+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<p><a href="https://ptorch.com/news/45.html" target="_blank" rel="noopener">https://ptorch.com/news/45.html</a></p>
<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><p><a href="https://blog.csdn.net/Tan915730/article/details/78954482" target="_blank" rel="noopener">https://blog.csdn.net/Tan915730/article/details/78954482</a></p>
<p>函数名下划线<a href="https://blog.csdn.net/qq_27261889/article/details/86777230" target="_blank" rel="noopener">https://blog.csdn.net/qq_27261889/article/details/86777230</a></p>
<p>函数，数据类型查询<a href="https://blog.csdn.net/zzulp/article/details/80573331" target="_blank" rel="noopener">https://blog.csdn.net/zzulp/article/details/80573331</a></p>
<h2 id="帮助文档"><a href="#帮助文档" class="headerlink" title="帮助文档"></a>帮助文档</h2><p><a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/index.html</a></p>
<h2 id="类型及类型转换"><a href="#类型及类型转换" class="headerlink" title="类型及类型转换"></a>类型及类型转换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inputs = [[[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]]]]</span><br><span class="line">inputs = torch.tensor(inputs, dtype=torch.float32) <span class="comment"># 类型转换</span></span><br><span class="line">print(inputs.dtype) <span class="comment"># float32</span></span><br></pre></td></tr></table></figure>
<h2 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h2><p>直接运行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inputs = [[[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]]]]</span><br><span class="line">inputs = torch.tensor(inputs, dtype=torch.float32) <span class="comment"># 类型转换</span></span><br><span class="line">model = nn.Sigmoid()</span><br><span class="line">outputs = model(inputs)</span><br><span class="line">print(outputs)</span><br><span class="line"><span class="comment"># tensor([[[[0.7311, 0.8808, 0.9526, 0.9820],</span></span><br><span class="line"><span class="comment">#           [0.7311, 0.8808, 0.9526, 0.9820],</span></span><br><span class="line"><span class="comment">#           [0.7311, 0.8808, 0.9526, 0.9820],</span></span><br><span class="line"><span class="comment">#           [0.7311, 0.8808, 0.9526, 0.9820]]]])</span></span><br></pre></td></tr></table></figure>
<p>通过继承nn.Module，重载__init__和forward</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sig</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__() </span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">inputs = [[[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]]]]</span><br><span class="line">inputs = torch.tensor(inputs, dtype=torch.float32) <span class="comment"># 类型转换</span></span><br><span class="line">model = sig()</span><br><span class="line">outputs = model(inputs)</span><br><span class="line">print(outputs)</span><br><span class="line"><span class="comment"># tensor([[[[0.7311, 0.8808, 0.9526, 0.9820],</span></span><br><span class="line"><span class="comment">#           [0.7311, 0.8808, 0.9526, 0.9820],</span></span><br><span class="line"><span class="comment">#           [0.7311, 0.8808, 0.9526, 0.9820],</span></span><br><span class="line"><span class="comment">#           [0.7311, 0.8808, 0.9526, 0.9820]]]])</span></span><br></pre></td></tr></table></figure>
<h2 id="cpu与gpu"><a href="#cpu与gpu" class="headerlink" title="cpu与gpu"></a>cpu与gpu</h2><p><a href="https://blog.csdn.net/liulina603/article/details/80180355" target="_blank" rel="noopener">https://blog.csdn.net/liulina603/article/details/80180355</a></p>
<p><a href="https://ptorch.com/news/160.html" target="_blank" rel="noopener">https://ptorch.com/news/160.html</a></p>
<h2 id="随即种子设定"><a href="#随即种子设定" class="headerlink" title="随即种子设定"></a>随即种子设定</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">7</span>) <span class="comment"># cpu</span></span><br><span class="line">torch.cuda.manual_seed(<span class="number">7</span>) <span class="comment"># gpu</span></span><br><span class="line">np.random.seed(<span class="number">7</span>) <span class="comment"># numpy</span></span><br><span class="line">random.seed(<span class="number">7</span>) <span class="comment"># random and transforms</span></span><br><span class="line">torch.backends.cudnn.deterministic=<span class="literal">True</span> <span class="comment"># cudnn</span></span><br></pre></td></tr></table></figure>
<h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h2><p><a href="https://blog.csdn.net/github_39611196/article/details/82998092" target="_blank" rel="noopener">https://blog.csdn.net/github_39611196/article/details/82998092</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">output, next_hidden = rnn(Variable(input, Variable(hidden))</span><br></pre></td></tr></table></figure>
<h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><h2 id="Dataset-amp-amp-Dataloader"><a href="#Dataset-amp-amp-Dataloader" class="headerlink" title="Dataset &amp;&amp; Dataloader"></a>Dataset &amp;&amp; Dataloader</h2><p>torch.utils.data.Dataset</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">trainset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#定义好 image 的路径</span></span><br><span class="line">        self.images = file_train</span><br><span class="line">        self.target = number_train</span><br><span class="line">        self.loader = loader</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        fn = self.images[index]</span><br><span class="line">        img = self.loader(fn)</span><br><span class="line">        target = self.target[index]</span><br><span class="line">        <span class="keyword">return</span> img,target</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.images)</span><br></pre></td></tr></table></figure>
<p>torch.utils.data.Dataloader</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">        transforms.Resize(<span class="number">32</span>),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(</span><br><span class="line">            mean=np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]),</span><br><span class="line">            std=np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])),</span><br><span class="line">    ])</span><br><span class="line"><span class="comment"># prepare dataset by ImageFolder, data should be classified by directory</span></span><br><span class="line">train_set = torchvision.datasets.ImageFolder(root = <span class="string">'./mnist/train'</span>, transform = transform)</span><br><span class="line"><span class="comment"># print(train_set[0][0])</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size = <span class="number">32</span>, shuffle = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> train_loader:</span><br><span class="line">    print(i[<span class="number">0</span>].shape) <span class="comment"># torch.Size([32, 3, 32, 32])</span></span><br><span class="line">    print(i[<span class="number">1</span>].shape) <span class="comment"># torch.Size([32]) </span></span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"><span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">    print(data.shape) <span class="comment"># 同上</span></span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h1 id="各种处理-转换"><a href="#各种处理-转换" class="headerlink" title="各种处理/转换"></a>各种处理/转换</h1><h2 id="view"><a href="#view" class="headerlink" title="view"></a>view</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">img_pil =  Image.open(<span class="string">'./mnist/test/0/mnist_test_10.png'</span>)</span><br><span class="line">img_pil = img_pil.convert(<span class="string">'RGB'</span>)</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">32</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(</span><br><span class="line">        mean=np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]),</span><br><span class="line">        std=np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])),</span><br><span class="line">])</span><br><span class="line">img_pil = transform(img_pil)</span><br><span class="line">print(img_pil.shape) <span class="comment"># 3 32 32</span></span><br><span class="line">img_pil = img_pil.view(<span class="number">1</span>,img_pil.size(<span class="number">0</span>), img_pil.size(<span class="number">1</span>), img_pil.size(<span class="number">2</span>))</span><br><span class="line">print(img_pil.shape) <span class="comment"># 1 3 32 32</span></span><br></pre></td></tr></table></figure>
<h2 id="transforms"><a href="#transforms" class="headerlink" title="transforms"></a>transforms</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">32</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(</span><br><span class="line">    	mean=np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]),</span><br><span class="line">        std=np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])),</span><br><span class="line">])</span><br><span class="line">img_pil =  Image.open(<span class="string">'./mnist/test/0/mnist_test_10.png'</span>)</span><br><span class="line">img_pil = img_pil.convert(<span class="string">'RGB'</span>)</span><br><span class="line">img_pil = transform(img_pil)</span><br></pre></td></tr></table></figure>
<h2 id="squeeze"><a href="#squeeze" class="headerlink" title="squeeze"></a>squeeze</h2><h2 id="unsqueeze"><a href="#unsqueeze" class="headerlink" title="unsqueeze"></a>unsqueeze</h2><h1 id="基本运算"><a href="#基本运算" class="headerlink" title="基本运算"></a>基本运算</h1><p><a href="https://blog.csdn.net/th_num/article/details/80783037" target="_blank" rel="noopener">https://blog.csdn.net/th_num/article/details/80783037</a></p>
<h2 id="max"><a href="#max" class="headerlink" title="max"></a>max</h2><h2 id="eq"><a href="#eq" class="headerlink" title="eq"></a>eq</h2><h2 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h2><h2 id="sin"><a href="#sin" class="headerlink" title="sin"></a>sin</h2><h2 id="mean"><a href="#mean" class="headerlink" title="mean"></a>mean</h2><h1 id="深度学习运算"><a href="#深度学习运算" class="headerlink" title="深度学习运算"></a>深度学习运算</h1><h2 id="层"><a href="#层" class="headerlink" title="层"></a>层</h2><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><h4 id="Conv（1d-2d-3d）"><a href="#Conv（1d-2d-3d）" class="headerlink" title="Conv（1d/2d/3d）"></a>Conv（1d/2d/3d）</h4><h4 id="ConvTranspose（1d-2d-3d）"><a href="#ConvTranspose（1d-2d-3d）" class="headerlink" title="ConvTranspose（1d/2d/3d）"></a>ConvTranspose（1d/2d/3d）</h4><h4 id="Unfold"><a href="#Unfold" class="headerlink" title="Unfold"></a>Unfold</h4><h4 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h4><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><h4 id="MaxPool（1d-2d-3d）"><a href="#MaxPool（1d-2d-3d）" class="headerlink" title="MaxPool（1d/2d/3d）"></a>MaxPool（1d/2d/3d）</h4><h4 id="MaxUnpool（1d-2d-3d）"><a href="#MaxUnpool（1d-2d-3d）" class="headerlink" title="MaxUnpool（1d/2d/3d）"></a>MaxUnpool（1d/2d/3d）</h4><h4 id="AvgPool（1d-2d-3d）"><a href="#AvgPool（1d-2d-3d）" class="headerlink" title="AvgPool（1d/2d/3d）"></a>AvgPool（1d/2d/3d）</h4><h4 id="FractionalMaxPool2d"><a href="#FractionalMaxPool2d" class="headerlink" title="FractionalMaxPool2d"></a>FractionalMaxPool2d</h4><h4 id="LPPool（1d-2d）"><a href="#LPPool（1d-2d）" class="headerlink" title="LPPool（1d/2d）"></a>LPPool（1d/2d）</h4><h4 id="AdaptiveMaxPool（1d-2d-3d）"><a href="#AdaptiveMaxPool（1d-2d-3d）" class="headerlink" title="AdaptiveMaxPool（1d/2d/3d）"></a>AdaptiveMaxPool（1d/2d/3d）</h4><h4 id="AdaptiveAvgPool（1d-2d-3d）"><a href="#AdaptiveAvgPool（1d-2d-3d）" class="headerlink" title="AdaptiveAvgPool（1d/2d/3d）"></a>AdaptiveAvgPool（1d/2d/3d）</h4><h3 id="填充层"><a href="#填充层" class="headerlink" title="填充层"></a>填充层</h3><h4 id="ReflectionPad（1d-2d）"><a href="#ReflectionPad（1d-2d）" class="headerlink" title="ReflectionPad（1d/2d）"></a>ReflectionPad（1d/2d）</h4><h4 id="ReplicationPad（1d-2d-3d）"><a href="#ReplicationPad（1d-2d-3d）" class="headerlink" title="ReplicationPad（1d/2d/3d）"></a>ReplicationPad（1d/2d/3d）</h4><h4 id="ZeroPad2d"><a href="#ZeroPad2d" class="headerlink" title="ZeroPad2d"></a>ZeroPad2d</h4><h4 id="ConstantPad（1d-2d-3d）"><a href="#ConstantPad（1d-2d-3d）" class="headerlink" title="ConstantPad（1d/2d/3d）"></a>ConstantPad（1d/2d/3d）</h4><h3 id="激活层"><a href="#激活层" class="headerlink" title="激活层"></a>激活层</h3><h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h4><p>LeakyReLU</p>
<p>PReLU</p>
<p>ReLU6</p>
<p>RReLU</p>
<p>SELU</p>
<p>CELU</p>
<h4 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h4><p>LogSigmoid</p>
<h4 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h4><p>Hardtanh</p>
<p>Tanhshrink</p>
<h4 id="Softplus"><a href="#Softplus" class="headerlink" title="Softplus"></a>Softplus</h4><h4 id="Softshrink"><a href="#Softshrink" class="headerlink" title="Softshrink"></a>Softshrink</h4><h4 id="Softsign"><a href="#Softsign" class="headerlink" title="Softsign"></a>Softsign</h4><h4 id="Threshold"><a href="#Threshold" class="headerlink" title="Threshold"></a>Threshold</h4><h4 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h4><h4 id="Hardshrink"><a href="#Hardshrink" class="headerlink" title="Hardshrink"></a>Hardshrink</h4><h3 id="末端激活层"><a href="#末端激活层" class="headerlink" title="末端激活层"></a>末端激活层</h3><h4 id="Softmin"><a href="#Softmin" class="headerlink" title="Softmin"></a>Softmin</h4><h4 id="Softmax（1d-2d）"><a href="#Softmax（1d-2d）" class="headerlink" title="Softmax（1d/2d）"></a>Softmax（1d/2d）</h4><h4 id="LogSoftmax"><a href="#LogSoftmax" class="headerlink" title="LogSoftmax"></a>LogSoftmax</h4><h4 id="AdaptiveLogSoftmaxWithLoss"><a href="#AdaptiveLogSoftmaxWithLoss" class="headerlink" title="AdaptiveLogSoftmaxWithLoss"></a>AdaptiveLogSoftmaxWithLoss</h4><h3 id="正则化层"><a href="#正则化层" class="headerlink" title="正则化层"></a>正则化层</h3><h4 id="BatchNorm（1d-2d-3d）"><a href="#BatchNorm（1d-2d-3d）" class="headerlink" title="BatchNorm（1d/2d/3d）"></a>BatchNorm（1d/2d/3d）</h4><h4 id="GroupNorm"><a href="#GroupNorm" class="headerlink" title="GroupNorm"></a>GroupNorm</h4><h4 id="InstanceNorm（1d-2d-3d）"><a href="#InstanceNorm（1d-2d-3d）" class="headerlink" title="InstanceNorm（1d/2d/3d）"></a>InstanceNorm（1d/2d/3d）</h4><h4 id="LayerNorm"><a href="#LayerNorm" class="headerlink" title="LayerNorm"></a>LayerNorm</h4><h4 id="LocalResponseNorm"><a href="#LocalResponseNorm" class="headerlink" title="LocalResponseNorm"></a>LocalResponseNorm</h4><h3 id="递归层"><a href="#递归层" class="headerlink" title="递归层"></a>递归层</h3><h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>以及各自的cell版本</p>
<h3 id="线性层"><a href="#线性层" class="headerlink" title="线性层"></a>线性层</h3><h4 id="linear"><a href="#linear" class="headerlink" title="linear"></a>linear</h4><h4 id="bilinear"><a href="#bilinear" class="headerlink" title="bilinear"></a>bilinear</h4><h3 id="Dropout层"><a href="#Dropout层" class="headerlink" title="Dropout层"></a>Dropout层</h3><h4 id="Dropout（1d-2d-3d）"><a href="#Dropout（1d-2d-3d）" class="headerlink" title="Dropout（1d/2d/3d）"></a>Dropout（1d/2d/3d）</h4><h4 id="AlphaDropout"><a href="#AlphaDropout" class="headerlink" title="AlphaDropout"></a>AlphaDropout</h4><h3 id="稀疏层-嵌入层"><a href="#稀疏层-嵌入层" class="headerlink" title="稀疏层/嵌入层"></a>稀疏层/嵌入层</h3><h4 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h4><h4 id="EmbeddingBag"><a href="#EmbeddingBag" class="headerlink" title="EmbeddingBag"></a>EmbeddingBag</h4><h2 id="运算函数"><a href="#运算函数" class="headerlink" title="运算函数"></a>运算函数</h2><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><a href="https://blog.csdn.net/shanglianlm/article/details/85019768" target="_blank" rel="noopener">https://blog.csdn.net/shanglianlm/article/details/85019768</a></p>
<p><a href="https://blog.csdn.net/jacke121/article/details/82812218" target="_blank" rel="noopener">https://blog.csdn.net/jacke121/article/details/82812218</a></p>
<h4 id="L1Loss"><a href="#L1Loss" class="headerlink" title="L1Loss"></a>L1Loss</h4><h4 id="SmoothL1Loss"><a href="#SmoothL1Loss" class="headerlink" title="SmoothL1Loss"></a>SmoothL1Loss</h4><h4 id="MSELoss"><a href="#MSELoss" class="headerlink" title="MSELoss"></a>MSELoss</h4><h4 id="BCELoss"><a href="#BCELoss" class="headerlink" title="BCELoss"></a>BCELoss</h4><h4 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h4><h4 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss"></a>NLLLoss</h4><h4 id="NLLLoss2d"><a href="#NLLLoss2d" class="headerlink" title="NLLLoss2d"></a>NLLLoss2d</h4><h3 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h3><h4 id="CosineSimilarity"><a href="#CosineSimilarity" class="headerlink" title="CosineSimilarity"></a>CosineSimilarity</h4><h4 id="PairwiseDistance"><a href="#PairwiseDistance" class="headerlink" title="PairwiseDistance"></a>PairwiseDistance</h4><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><h3 id="calculate-gain"><a href="#calculate-gain" class="headerlink" title="calculate_gain"></a>calculate_gain</h3><h3 id="uniform"><a href="#uniform" class="headerlink" title="uniform_"></a>uniform_</h3><h3 id="normal"><a href="#normal" class="headerlink" title="normal_"></a>normal_</h3><h3 id="constant"><a href="#constant" class="headerlink" title="constant_"></a>constant_</h3><h3 id="eye"><a href="#eye" class="headerlink" title="eye_"></a>eye_</h3><h3 id="dirac"><a href="#dirac" class="headerlink" title="dirac_"></a>dirac_</h3><h3 id="xavier-uniform"><a href="#xavier-uniform" class="headerlink" title="xavier_uniform_"></a>xavier_uniform_</h3><h3 id="xavier-normal"><a href="#xavier-normal" class="headerlink" title="xavier_normal_"></a>xavier_normal_</h3><h3 id="kaiming-uniform"><a href="#kaiming-uniform" class="headerlink" title="kaiming_uniform_"></a>kaiming_uniform_</h3><h3 id="kaiming-normal"><a href="#kaiming-normal" class="headerlink" title="kaiming_normal_"></a>kaiming_normal_</h3><h3 id="orthogonal"><a href="#orthogonal" class="headerlink" title="orthogonal_"></a>orthogonal_</h3><h3 id="sparse"><a href="#sparse" class="headerlink" title="sparse_"></a>sparse_</h3><h2 id="avg-pool2d"><a href="#avg-pool2d" class="headerlink" title="avg_pool2d"></a>avg_pool2d</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">inputs = [[[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], [<span class="number">1</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">9</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]]]]</span><br><span class="line">inputs = torch.tensor(inputs, dtype=torch.float32) <span class="comment"># 类型转换</span></span><br><span class="line">outputs = nn.functional.avg_pool2d(inputs, <span class="number">2</span>)</span><br><span class="line">print(inputs)</span><br><span class="line">print(outputs)</span><br><span class="line"><span class="comment"># tensor([[[[1., 2., 3., 4.],</span></span><br><span class="line"><span class="comment">#           [1., 5., 3., 4.],</span></span><br><span class="line"><span class="comment">#           [1., 2., 3., 9.],</span></span><br><span class="line"><span class="comment">#           [0., 2., 3., 4.]]]])</span></span><br><span class="line"><span class="comment"># tensor([[[[2.2500, 3.5000],</span></span><br><span class="line"><span class="comment">#           [1.2500, 4.7500]]]])</span></span><br></pre></td></tr></table></figure>
<h2 id="Sigmoid-1"><a href="#Sigmoid-1" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">inputs = [[[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]]]]</span><br><span class="line">inputs = torch.tensor(inputs, dtype=torch.float32) <span class="comment"># 类型转换</span></span><br><span class="line">model = nn.Sigmoid()</span><br><span class="line">outputs = model(inputs)</span><br><span class="line">print(outputs)</span><br><span class="line"><span class="comment"># tensor([[[[0.7311, 0.8808, 0.9526, 0.9820],</span></span><br><span class="line"><span class="comment">#           [0.7311, 0.8808, 0.9526, 0.9820],</span></span><br><span class="line"><span class="comment">#           [0.7311, 0.8808, 0.9526, 0.9820],</span></span><br><span class="line"><span class="comment">#           [0.7311, 0.8808, 0.9526, 0.9820]]]])</span></span><br></pre></td></tr></table></figure>
<h1 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h1><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2_norm)</span><br></pre></td></tr></table></figure>
<h3 id="momentum"><a href="#momentum" class="headerlink" title="momentum"></a>momentum</h3><h2 id="ASGD"><a href="#ASGD" class="headerlink" title="ASGD"></a>ASGD</h2><h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><h2 id="Adamax"><a href="#Adamax" class="headerlink" title="Adamax"></a>Adamax</h2><h2 id="SparseAdam"><a href="#SparseAdam" class="headerlink" title="SparseAdam"></a>SparseAdam</h2><h2 id="LBFGS"><a href="#LBFGS" class="headerlink" title="LBFGS"></a>LBFGS</h2><h2 id="Rprop"><a href="#Rprop" class="headerlink" title="Rprop"></a>Rprop</h2><h1 id="内置经典网络"><a href="#内置经典网络" class="headerlink" title="内置经典网络"></a>内置经典网络</h1><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h2><h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><h2 id="Inception-v3"><a href="#Inception-v3" class="headerlink" title="Inception v3"></a>Inception v3</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/深度学习-编程框架/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/深度学习-编程框架/" itemprop="url">深度学习-编程框架</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:44:59+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="keras"><a href="#keras" class="headerlink" title="keras"></a>keras</h1><h2 id="preprocessing"><a href="#preprocessing" class="headerlink" title="preprocessing"></a>preprocessing</h2><h2 id="text-文本预处理"><a href="#text-文本预处理" class="headerlink" title="text(文本预处理)"></a>text(文本预处理)</h2><p>关于Tokenizer的使用，需要自己看源码，官方文档没有Tokenizer的介绍，下面是一个使用例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> text</span><br><span class="line"></span><br><span class="line">tokenizer = text.Tokenizer()</span><br></pre></td></tr></table></figure>
<h1 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/面试-AI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/面试-AI/" itemprop="url">面试-AI</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:44:59+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li>如何从100个特征中选20个特征，有哪些方法<ul>
<li>PCA</li>
</ul>
</li>
<li>解释一下主成分分析算法（PCA），简述下使用PCA算法的数学步骤</li>
<li>使用 PCA算法有哪些缺点</li>
<li>垃圾短信多分类任务(如何分开发票，广告，商铺信息等)，有什么思路</li>
<li>朴素贝叶斯为什么被称为“朴素”？详细介绍一下朴素贝叶斯分类器</li>
<li>L1正则和L2正则的区别</li>
<li>什么是深度学习，深度学习和机器学习的区别是什么</li>
<li>在无监督学习中，如何进行文件聚类</li>
<li>谈一谈无监督学习；都有哪些算法</li>
<li>如何找到与某些查询语句/搜索相关的文件</li>
<li>简述支持向量机（SVM）的工作原理；解释SVM如何学习非线性边界；什么是Kernal，解释一下；SVM中用到了哪些内核？SVM的优化技术有哪些；SVM如何学习超平面？论述下其数学运算细节</li>
<li>训练决策树时，其参数是什么</li>
<li>在决策树的某个节点处进行分割，其分割标准是什么</li>
<li>基尼系数的计算公式是什么</li>
<li>熵的计算公式是什么</li>
<li>决策树如何决定在哪个特征处必须进行分割</li>
<li>简述随机森林及其优点</li>
<li>简述boosting算法；你所了解的Boosting技术有哪些？</li>
<li>梯度提升算法（gradient boosting）是怎样工作的</li>
<li>简述AdaBoost算法工作原理</li>
<li>在不平衡数据集中，你会选择什么模型：随机森林还是Boosting？为什么</li>
<li>简述LSTM的工作原理，它是如何记住文本的</li>
<li>你怎样使用Ensemble技术</li>
<li>决策树和随机森林，你更喜欢哪一个；你会用决策树还是随机森林来解决分类问题？随机森林有什么优点</li>
<li>逻辑回归和随机森林有什么区别</li>
<li>如何定义K-Means聚类算法中K的值；列举至少3中定义K-Means聚类算法中K的方法</li>
<li>你还知道哪些聚类算法</li>
<li>介绍一下DB-SCAM算法</li>
<li>简述下分层凝聚聚类（Hierarchical Agglomerativeclustering）的工作原理</li>
<li>谈谈卷积神经网络的工作原理？详细说明其实现细节</li>
<li>如何设计一个神经网络？如何做到“深度”</li>
<li>解释一下卷积神经网络中的反向传播</li>
<li>采用监督学习解决分类问题，你会选择哪个模型？假设有 40-50个分类</li>
<li>如何部署机器学习模型</li>
<li>我们大部分情况下都要用C++从零开始搭建一个机器学习模型，这一点你能做到吗</li>
<li>如何实现非线性回归</li>
<li>什么是Lasso回归和Ridge回归</li>
<li>Sigmoid 函数的范围是什么</li>
<li>标准正态分布的均值和方差分别是多少</li>
<li>抛10次硬币，4次是正面的概率是多少？</li>
<li>从用户行为来看，你需要模拟一个欺诈活动，你会如何解决这个问题？这是可能是一个异常检测问题或分类问题</li>
<li>文本分类的方法有哪些？你会怎么做分类</li>
<li>什么是双词搭配(Bigrams)和三词搭配(Trigrams）？用一个文本语句解释一下双词搭配和三词搭配的TF-IDF技术</li>
<li>解释下TF-IDF技术，如何克服TF-IDF的缺点</li>
<li>根据经验来看，TF-IDF技术在文件分类或聚类上效果并不好，你将如何改进</li>
<li>什么是word2vec模型，有哪些应用</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/面试-Python/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/面试-Python/" itemprop="url">面试-Python</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:44:59+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li>解释下python中的可变对象和不可变对象</li>
<li>你在python中使用过什么数据结构</li>
<li>说出scikit-learn能够实现逻辑回归的包的名称</li>
<li>如何获取Python列表中元素的索引</li>
<li>如果合并两个pandas数据集</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/机器学习-决策树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/机器学习-决策树/" itemprop="url">机器学习-决策树</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:44:59+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<p>决策树是用于分类和预测的主要技术之一，决策树学习是以实例为基础的归纳学习算法，它着眼于从一组无次序、无规则的实例中推理出以决策树表示的分类规则。构造决策树的目的是找出属性和类别间的关系，用它来预测将来未知类别的记录的类别。它采用自顶向下的递归方式，在决策树的内部节点进行属性的比较，并根据不同属性值判断从该节点向下的分支，在决策树的叶节点得到结论。</p>
<p>主要的决策树算法有ID3、C4.5 (C5.0) 、CART、PUBLIC、SLIQ和SPRINT算法等。它们在选择测试属性采用的技术、生成的决策树的结构、剪枝的方法以及时刻，能否处理大数据集等方面都有各自的不同之处。</p>
<p>ID3：计算信息增益，选择信息增益最大的特征作为当前节点的决策特征，信息增益表示分类目标的熵减去当前属性的熵，增益越大，分类能力越强</p>
<p>熵等于负的概率乘以log概率<br>C4.5：使用信息增益率来进行属性的选择，准确率高，但是子构造树的过程中需要进行多次的扫描和排序，所以它的运算效率较低</p>
<p>cart：根据当前特征计算他们的基尼增益，选择基尼增益最小的特征作为划分特征，总体的类别越杂乱，GINI指数越大</p>
<p>等于1-概率平方和<br>好的决策树</p>
<p>叶子节点数尽量少<br>叶子节点的深度尽量小(太深可能会过拟合)<br>解决过拟合</p>
<p>剪枝：<br>前置剪枝：在分裂节点的时候设计比较苛刻的条件，如果不满足就直接停止分裂<br>后置剪枝：建立完树以后，用单个节点代替子树<br>交叉验证<br>随机森林<br>优点：</p>
<p>计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征；<br>缺点：</p>
<p>单颗决策树分类能力弱，并且对连续值变量难以处理；<br>容易过拟合（后续出现了随机森林，减小了过拟合现象）</p>
<h1 id="Hunt算法"><a href="#Hunt算法" class="headerlink" title="Hunt算法"></a>Hunt算法</h1><p>hunt算法是一种采用局部最优策略的决策树构建算法,它同时也是许多决策树算法的基础,包括ID3、C4.5和CART等</p>
<script type="math/tex; mode=display">
\begin{align}
训练机D =& \{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\} \tag{1}\\
属性集A =& \{a_1,a_2,...,a_n\}\\
TreeGen&etare(D,A):\\
生成节&点node\\
if \ D&中样本属于同一种类C(case \ 1)\\
&将node标记为C类叶结点\\
&return\\
end \ i&f\\
if \ A& = \empty \ OR \ D中样本属于在A上取值相同(case \ 2)\\
&将node标记为叶结点,类别为D中种类最多的类别\\
&return\\
end \ i&f\\
从A中&选择最优划分属性a_*\\
for \ a&_* 的每一个属性值a_*^v\\
&为node生成一个分支，令D_v表示D在a_*上取值为a_*^v的样本子集\\
&if \ D_v为空(case \ 3)\\
& \quad 将分支节点标记为叶结点，其类别标记为D中样本最多的类\\
&else\\
& \quad 以TreeGenetare(D_v,A \backslash \{a_*\})为分支结点\\
&end \ if\\

end \ &for\\
\end{align}</script><p>对于case2：设定为该结点所含样本最多的类别，利用当前结点的<strong>后验分布</strong></p>
<p>对于case3：设定为其父结点所含样本最多的类别，把父结点的样本分布作为当前结点的<strong>先验分布</strong></p>
<p>决策树学习的关键是算法的这一行：选择最优划分属性</p>
<p>什么样的划分属性是最优的：我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”越来越高，可以高效地从根结点到达叶结点，得到决策结果</p>
<p>三种度量结点“纯度”的指标：</p>
<ul>
<li><p>信息增益</p>
</li>
<li><p>增益率</p>
</li>
<li>基尼指数</li>
</ul>
<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>香农提出了“信息熵”的概念，解决了对信息的量化度量问题。香农用“信息熵”的概念来描述信源的不确定性</p>
<p>可以解释为一个数据集的信息熵（information entropy），度量样本集合纯度，p则为每一类样本所占的比率</p>
<script type="math/tex; mode=display">
Ent(D) = -\sum_{k=1}^{|y|}p_k \log_2p_k</script><p>信息熵越小，纯度越高</p>
<p>具体详见数据科学-数据度量-信息论度量</p>
<h2 id="信息增益-information-gain"><a href="#信息增益-information-gain" class="headerlink" title="信息增益 information gain"></a>信息增益 information gain</h2><script type="math/tex; mode=display">
假设对样本集D在离散属性a上进行划分，产生V个分支节点\\
其中第v个分支节点包含了D中所有在属性a上取值为a^v的样本，记为D^v\\
那么属性a对样本集D进行划分所获得的信息增益为\\
Gain(D,a) = Ent(D) - \sum_{v=1}^V \frac{|D^v|}{|D|}Ent(D^v)</script><p>一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大</p>
<h3 id="ID3-Iterative-Dichotomiser"><a href="#ID3-Iterative-Dichotomiser" class="headerlink" title="ID3 Iterative Dichotomiser"></a>ID3 Iterative Dichotomiser</h3><p>ID3算法中，“选择最优划分属性”为</p>
<script type="math/tex; mode=display">
a_* = {\arg \max}_{a \in A} Gain(D,a)</script><p>不足：</p>
<p>若把“编号”也作为一个候选划分属性，则属性“编号”的信息增益通常远大于其他候选属性，信息增益准则通常对可取值数目较多的属性有所偏好</p>
<p>用的信息增益gain</p>
<h2 id="增益率-information-gain-ratio"><a href="#增益率-information-gain-ratio" class="headerlink" title="增益率 information gain ratio"></a>增益率 information gain ratio</h2><script type="math/tex; mode=display">
Gain\_ratio = \frac{Gain(D,a)}{IV(a)}\\
其中\\
IV(A) = -\sum_{v=1}^V \frac{|D^v|}{|D|} \log_2\frac{|D^v|}{|D|}</script><p>增益率准对可取值数目较少的属性有所偏好</p>
<h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h3><p>C4.5决策树算法综合了信息增益准则和信息率准则的特点：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的</p>
<p>使用连续属性离散化技术</p>
<h2 id="基尼指数-gini-index"><a href="#基尼指数-gini-index" class="headerlink" title="基尼指数 gini index"></a>基尼指数 gini index</h2><script type="math/tex; mode=display">
基尼值\\
Gini(D) = \sum_{k=1}^{|y|} \sum_{k'\ne k}p_kp_{k'}\\
= 1-\sum_{k=1}^{|y|}p_k^2\\
直观来说，Gini(D)反映了数据集D中随机抽取两个样本，其类别标记不一致的概率\\
因此Gini(D)越小,则数据集D的纯度越高</script><script type="math/tex; mode=display">
基尼指数\\
Gini\_index(D,a) = \sum_{v=1}^{|V|} \frac{|D^v|}{|D|}Gini(D^v)\\</script><h3 id="CART-Classification-And-Regression-Tree"><a href="#CART-Classification-And-Regression-Tree" class="headerlink" title="CART Classification And Regression Tree"></a>CART Classification And Regression Tree</h3><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p>CART算法中，“选择最优划分属性”为</p>
<script type="math/tex; mode=display">
a_* = {\arg \max}_{a \in A} Gini\_index(D,a)</script><h4 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h4><h4 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h4><p>entropy，条件熵</p>
<p>卡方自动交互检测（CHAID） Chi-squared Automatic Interaction Detection</p>
<p>决策树桩/单层决策树 Decision stump</p>
<p>SLIQ</p>
<h1 id="剪枝-1"><a href="#剪枝-1" class="headerlink" title="剪枝"></a>剪枝</h1><p>通过主动去掉一些分支来降低过拟合的风险</p>
<p>使用留出法，将数据集D划分为两个互斥的集合：训练集S和测试集T</p>
<h2 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h2><p>在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点</p>
<p><img src="/2019/03/08/机器学习-决策树/image-20190815140411606.png" alt="image-20190815140411606"></p>
<p><img src="/2019/03/08/机器学习-决策树/image-20190815140650466.png" alt="image-20190815140650466"></p>
<p>优点：</p>
<ul>
<li>降低过拟合的风险</li>
<li>减少了训练时间开销和<br>  测试时间开销</li>
</ul>
<p>不足：</p>
<ul>
<li>基于“贪心”本质禁止某些分支展开，带来了欠拟合的风险</li>
</ul>
<h2 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h2><p>先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点</p>
<p><img src="/2019/03/08/机器学习-决策树/image-20190815140757822.png" alt="image-20190815140757822"></p>
<p><img src="/2019/03/08/机器学习-决策树/image-20190815140915514.png" alt="image-20190815140915514"></p>
<p>优点：</p>
<ul>
<li>保留了更多的分支</li>
<li>欠拟合风险很小</li>
<li>泛化能力优于预剪枝决策树</li>
</ul>
<p>不足：</p>
<p>训练时间开销比未减枝和预剪枝决策树大得多，因为需要生产完全决策树，还要所有非叶节点逐一考察</p>
<h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><h2 id="连续属性离散化技术"><a href="#连续属性离散化技术" class="headerlink" title="连续属性离散化技术"></a>连续属性离散化技术</h2><p>使用二分法（C4.5决策树算法的方法）</p>
<script type="math/tex; mode=display">
对于连续属性a，有n个不同的取值，将n个取值从小到大排序:\\
\{a_1,a_2,...,a_n\}\\
划分点t(数值)将D划分为两个子集D_t^-和D_t^+\\
\{ \underbrace{a_1,a_2,...a_i}_{D_t^-} ,\underbrace{|}_t \underbrace{a_{i+1}...,a_n}_{D_t^+}\}\\
t \in [a_i,a_{i+1})\\</script><script type="math/tex; mode=display">
可考察包含n-1个元素的候选划分集合\\
T_a = \{\frac{a_i+a_{i+1}}{2}|1\le i \le n-1\}\\
然后可以像离散属性值来考察这些划分点，选取最优的划分点进行样本集合的划分，比如信息增益：\\
Gain(D,a) = \max_{t \in T_a} Gain(D,a,t) = \max_{t \in T_a}Ent(D) - \sum_{\lambda \in \{-,+\}} \frac{|D^\lambda_t|}{|D|}Ent(D^\lambda_t)</script><p>与离散属性不同，若当前结点划分属性为连续属性，该连续属性还可被再次选作后代结点的最优划分属性</p>
<h1 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h1><p>如果把每个属性看作坐标空间中的一个坐标轴<br>d个属性描述的样本看作d维空间中的一个数据点<br>对样本分类看作在坐标空间中寻找不同类样本之间的分类边界</p>
<p>决策树形成的分类边界的明显特点：轴平行，分类边界由若干个与坐标轴平行的分段组成</p>
<p>优点： 学习结果解释性强，每个划分都对应一个属性取值</p>
<p><img src="/2019/03/08/机器学习-决策树/image-20190815144727591.png" alt="image-20190815144727591"></p>
<p>不足：复杂分类时，决策树会很复杂，要进行大量的属性测试，预测时间开销大</p>
<p><img src="/2019/03/08/机器学习-决策树/image-20190815144954840.png" alt="image-20190815144954840"></p>
<h1 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h1><p>可以实现红线这样的斜划分</p>
<p><img src="/2019/03/08/机器学习-决策树/image-20190815145239610.png" alt="image-20190815145239610"></p>
<p><img src="/2019/03/08/机器学习-决策树/image-20190815145138196.png" alt="image-20190815145138196"></p>
<p><img src="/2019/03/08/机器学习-决策树/image-20190815145157742.png" alt="image-20190815145157742"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/机器学习-Sklearn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/机器学习-Sklearn/" itemprop="url">机器学习-Sklearn</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:44:59+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h1><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><p>sklearn为所有模型提供了非常相似的接口，这样使得我们可以更加快速的熟悉所有模型的用法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拟合模型</span></span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">model.predict(X_test)</span><br><span class="line"><span class="comment"># 返回预测属于某标签的概率</span></span><br><span class="line">model.predict_proba(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得这个模型的参数</span></span><br><span class="line">model.get_params()</span><br><span class="line"><span class="comment"># 为模型进行打分</span></span><br><span class="line">model.score(data_X, data_y) <span class="comment"># 线性回归：R square； 分类问题： acc</span></span><br></pre></td></tr></table></figure>
<p><img src="/2019/03/08/机器学习-Sklearn/4.png" alt="4"></p>
<h2 id="datesets"><a href="#datesets" class="headerlink" title="datesets"></a>datesets</h2><p>sklearn包含了一些经典数据集，可以使用datasets加载</p>
<p><img src="/2019/03/08/机器学习-Sklearn/1.png" alt="1"></p>
<p><img src="/2019/03/08/机器学习-Sklearn/2.png" alt="1"></p>
<p>以iris为例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris() <span class="comment"># 导入数据集</span></span><br><span class="line">X = iris.data <span class="comment"># 获得其特征向量</span></span><br><span class="line">y = iris.target <span class="comment"># 获得样本label</span></span><br></pre></td></tr></table></figure>
<h3 id="make-classification-产生分类数据和类别"><a href="#make-classification-产生分类数据和类别" class="headerlink" title="make_classification 产生分类数据和类别"></a>make_classification 产生分类数据和类别</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">6</span>, n_features=<span class="number">5</span>, n_informative=<span class="number">2</span>,</span><br><span class="line">    n_redundant=<span class="number">2</span>, n_classes=<span class="number">2</span>, n_clusters_per_class=<span class="number">2</span>, scale=<span class="number">1.0</span>,</span><br><span class="line">    random_state=<span class="number">20</span>)</span><br><span class="line">print(X)</span><br><span class="line"><span class="comment"># [[-0.6600737  -0.0558978   0.82286793  1.1003977  -0.93493796]</span></span><br><span class="line"><span class="comment">#  [ 0.4113583   0.06249216 -0.90760075 -1.41296696  2.059838  ]</span></span><br><span class="line"><span class="comment">#  [ 1.52452016 -0.01867812  0.20900899  1.34422289 -1.61299022]</span></span><br><span class="line"><span class="comment">#  [-1.25725859  0.02347952 -0.28764782 -1.32091378 -0.88549315]</span></span><br><span class="line"><span class="comment">#  [-3.28323172  0.03899168 -0.43251277 -2.86249859 -1.10457948]</span></span><br><span class="line"><span class="comment">#  [ 1.68841011  0.06754955 -1.02805579 -0.83132182  0.93286635]]</span></span><br><span class="line">print(y)</span><br><span class="line"><span class="comment"># [0 1 1 0 0 1]</span></span><br></pre></td></tr></table></figure>
<h3 id="make-moons-2d维度创造两个相插的半圆的数据集"><a href="#make-moons-2d维度创造两个相插的半圆的数据集" class="headerlink" title="make_moons 2d维度创造两个相插的半圆的数据集"></a>make_moons 2d维度创造两个相插的半圆的数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"></span><br><span class="line">X, y = make_moons(n_samples=<span class="number">100</span>, shuffle=<span class="literal">True</span>, noise=<span class="literal">None</span>, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h3 id="make-circles-2d维度创造一个圆形数据集"><a href="#make-circles-2d维度创造一个圆形数据集" class="headerlink" title="make_circles 2d维度创造一个圆形数据集"></a>make_circles 2d维度创造一个圆形数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</span><br><span class="line"></span><br><span class="line">X, y = make_circles(n_samples=<span class="number">100</span>, shuffle=<span class="literal">True</span>, noise=<span class="literal">None</span>, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h2 id="preprocessing"><a href="#preprocessing" class="headerlink" title="preprocessing"></a>preprocessing</h2><p>用于数据预处理</p>
<h3 id="StandardScaler-MinMaxScaler-数据标准化-归一化"><a href="#StandardScaler-MinMaxScaler-数据标准化-归一化" class="headerlink" title="StandardScaler/MinMaxScaler 数据标准化/归一化"></a>StandardScaler/MinMaxScaler 数据标准化/归一化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line">train_data = [[<span class="number">9</span>, <span class="number">3</span>], [<span class="number">5</span>, <span class="number">7</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">1</span>]]</span><br><span class="line"><span class="comment"># 1. 基于mean和std的标准化，去均值和方差归一化。且是针对每一个特征维度来做的，而不是针对样本</span></span><br><span class="line">scaler = preprocessing.StandardScaler().fit(train_data)</span><br><span class="line">scaler.transform(train_data)</span><br><span class="line">print(scaler.transform(train_data))</span><br><span class="line"><span class="comment"># [[ 1.52611672 -0.22941573]</span></span><br><span class="line"><span class="comment">#  [ 0.2409658   1.60591014]</span></span><br><span class="line"><span class="comment">#  [-1.04418513 -0.22941573]</span></span><br><span class="line"><span class="comment">#  [-0.7228974  -1.14707867]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 将每个特征值归一化到一个固定范围</span></span><br><span class="line"><span class="comment">#feature_range: 定义归一化范围，注用（）括起来</span></span><br><span class="line">scaler = preprocessing.MinMaxScaler(feature_range=(<span class="number">0</span>, <span class="number">1</span>)).fit(train_data)</span><br><span class="line">scaler.transform(train_data)</span><br><span class="line">print(scaler.transform(train_data))</span><br><span class="line"><span class="comment"># [[1.         0.33333333]</span></span><br><span class="line"><span class="comment">#  [0.5        1.        ]</span></span><br><span class="line"><span class="comment">#  [0.         0.33333333]</span></span><br><span class="line"><span class="comment">#  [0.125      0.        ]]</span></span><br></pre></td></tr></table></figure>
<h3 id="normalize正则化"><a href="#normalize正则化" class="headerlink" title="normalize正则化"></a>normalize正则化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line">train_data = [[<span class="number">9</span>, <span class="number">3</span>], [<span class="number">5</span>, <span class="number">7</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">1</span>]]</span><br><span class="line"><span class="comment"># l1正则化</span></span><br><span class="line">X_normalized = preprocessing.normalize(train_data, norm=<span class="string">'l1'</span>)</span><br><span class="line">print(X_normalized)</span><br><span class="line"><span class="comment"># [[0.75       0.25      ]</span></span><br><span class="line"><span class="comment">#  [0.41666667 0.58333333]</span></span><br><span class="line"><span class="comment">#  [0.25       0.75      ]</span></span><br><span class="line"><span class="comment">#  [0.66666667 0.33333333]]</span></span><br><span class="line"><span class="comment"># l2正则化</span></span><br><span class="line">X_normalized = preprocessing.normalize(train_data, norm=<span class="string">'l2'</span>)</span><br><span class="line">print(X_normalized)</span><br><span class="line"><span class="comment"># [[0.9486833  0.31622777]</span></span><br><span class="line"><span class="comment">#  [0.58123819 0.81373347]</span></span><br><span class="line"><span class="comment">#  [0.31622777 0.9486833 ]</span></span><br><span class="line"><span class="comment">#  [0.89442719 0.4472136 ]]</span></span><br></pre></td></tr></table></figure>
<h3 id="one-hot-编码"><a href="#one-hot-编码" class="headerlink" title="one-hot 编码"></a>one-hot 编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line">data = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]]</span><br><span class="line">encoder = preprocessing.OneHotEncoder().fit(data)</span><br><span class="line">print(encoder.transform(data).toarray())</span><br><span class="line"><span class="comment"># [[1. 0. 1. 0. 0. 0. 0. 0. 1.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. 1. 0. 1. 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [1. 0. 0. 0. 1. 0. 1. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 1. 0. 0. 0. 0. 1. 0.]]</span></span><br></pre></td></tr></table></figure>
<h3 id="LabelEncoder-编码"><a href="#LabelEncoder-编码" class="headerlink" title="LabelEncoder 编码"></a>LabelEncoder 编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">encoder=LabelEncoder()</span><br><span class="line">encoder.fit([<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">6</span>])</span><br><span class="line">t=encoder.transform([<span class="number">1</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">2</span>])</span><br><span class="line">print(t)</span><br><span class="line"><span class="comment"># [0 3 3 1]</span></span><br></pre></td></tr></table></figure>
<h3 id="PolynomialFeatures-构造多项式特征"><a href="#PolynomialFeatures-构造多项式特征" class="headerlink" title="PolynomialFeatures 构造多项式特征"></a>PolynomialFeatures 构造多项式特征</h3><p>例如如果有a，b两个特征，那么它的2次多项式为（1,a,b,a^2,ab, b^2）</p>
<p>PolynomialFeatures有三个参数</p>
<p>degree：控制多项式的度</p>
<p>interaction_only： 默认为False，如果指定为True，那么就不会有特征自己和自己结合的项，上面的二次项中没有a^2和b^2</p>
<p>include_bias：默认为True。如果为True的话，那么就会有上面的”1“那一项</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line"></span><br><span class="line">poly = PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>, interaction_only=<span class="literal">False</span>)</span><br><span class="line">X_ploly = poly.fit_transform(X)</span><br><span class="line">X_ploly_df = pd.DataFrame(X_ploly, columns=poly.get_feature_names())</span><br><span class="line">print(X_ploly_df.head())</span><br><span class="line"><span class="comment">#     x0   x1   x2   x3   x0^2  x0 x1  ...   x1^2  x1 x2  x1 x3  x2^2  x2 x3  x3^2</span></span><br><span class="line"><span class="comment"># 0  5.1  3.5  1.4  0.2  26.01  17.85  ...  12.25   4.90   0.70  1.96   0.28  0.04</span></span><br><span class="line"><span class="comment"># 1  4.9  3.0  1.4  0.2  24.01  14.70  ...   9.00   4.20   0.60  1.96   0.28  0.04</span></span><br><span class="line"><span class="comment"># 2  4.7  3.2  1.3  0.2  22.09  15.04  ...  10.24   4.16   0.64  1.69   0.26  0.04</span></span><br><span class="line"><span class="comment"># 3  4.6  3.1  1.5  0.2  21.16  14.26  ...   9.61   4.65   0.62  2.25   0.30  0.04</span></span><br><span class="line"><span class="comment"># 4  5.0  3.6  1.4  0.2  25.00  18.00  ...  12.96   5.04   0.72  1.96   0.28  0.04</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [5 rows x 14 columns]</span></span><br></pre></td></tr></table></figure>
<h2 id="decomposition"><a href="#decomposition" class="headerlink" title="decomposition"></a>decomposition</h2><h3 id="PCA-主成分分析"><a href="#PCA-主成分分析" class="headerlink" title="PCA 主成分分析"></a>PCA 主成分分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">X = np.array([[<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">-2</span>, <span class="number">-1</span>], [<span class="number">-3</span>, <span class="number">-2</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca.fit(X)</span><br><span class="line">print(pca.explained_variance_ratio_)  </span><br><span class="line"><span class="comment"># [0.9924... 0.0075...]</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(pca.singular_values_)  </span><br><span class="line"><span class="comment"># [6.30061... 0.54980...]</span></span><br></pre></td></tr></table></figure>
<h2 id="model-selection"><a href="#model-selection" class="headerlink" title="model_selection"></a>model_selection</h2><h3 id="train-test-split-数据拆分"><a href="#train-test-split-数据拆分" class="headerlink" title="train_test_split 数据拆分"></a>train_test_split 数据拆分</h3><p>用于数据集的拆分，比如训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]]</span><br><span class="line">y = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">参数</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">arrays：样本数组，包含特征向量和标签</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">test_size：</span></span><br><span class="line"><span class="string">　　float-获得多大比重的测试样本 （默认：0.25）</span></span><br><span class="line"><span class="string">　　int - 获得多少个测试样本</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">train_size: 同test_size</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">random_state:</span></span><br><span class="line"><span class="string">　　int - 随机种子（种子固定，实验可复现）</span></span><br><span class="line"><span class="string">　　</span></span><br><span class="line"><span class="string">shuffle - 是否在分割之前对数据进行洗牌（默认True）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">返回</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">分割后的列表，长度=2*len(arrays), </span></span><br><span class="line"><span class="string">　　(train-test split)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(X_train)</span><br><span class="line"><span class="comment"># [[0, 0, 3], [0, 2, 1]]</span></span><br><span class="line">print(X_test)</span><br><span class="line"><span class="comment"># [[1, 1, 0], [1, 0, 2]]</span></span><br><span class="line">print(y_train)</span><br><span class="line"><span class="comment"># [1, 3]</span></span><br><span class="line">print(y_test)</span><br><span class="line"><span class="comment"># [2, 4]</span></span><br></pre></td></tr></table></figure>
<h3 id="cross-val-score-交叉验证"><a href="#cross-val-score-交叉验证" class="headerlink" title="cross_val_score 交叉验证"></a>cross_val_score 交叉验证</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">cross_val_score(model, X, y=<span class="literal">None</span>, scoring=<span class="literal">None</span>, cv=<span class="literal">None</span>, n_jobs=<span class="number">1</span>)</span><br><span class="line"><span class="string">"""参数</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">    model：拟合数据的模型</span></span><br><span class="line"><span class="string">    cv ： k-fold</span></span><br><span class="line"><span class="string">    scoring: 打分参数-‘accuracy’、‘f1’、‘precision’、‘recall’ 、‘roc_auc’、'neg_log_loss'等等</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h3 id="validation-curve-检验曲线（列表形式）"><a href="#validation-curve-检验曲线（列表形式）" class="headerlink" title="validation_curve 检验曲线（列表形式）"></a>validation_curve 检验曲线（列表形式）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> validation_curve</span><br><span class="line">train_score, test_score = validation_curve(model, X, y, param_name, param_range, cv=<span class="literal">None</span>, scoring=<span class="literal">None</span>, n_jobs=<span class="number">1</span>)</span><br><span class="line"><span class="string">"""参数</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">    model:用于fit和predict的对象</span></span><br><span class="line"><span class="string">    X, y: 训练集的特征和标签</span></span><br><span class="line"><span class="string">    param_name：将被改变的参数的名字</span></span><br><span class="line"><span class="string">    param_range： 参数的改变范围</span></span><br><span class="line"><span class="string">    cv：k-fold</span></span><br><span class="line"><span class="string">   </span></span><br><span class="line"><span class="string">返回值</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">   train_score: 训练集得分（array）</span></span><br><span class="line"><span class="string">    test_score: 验证集得分（array）</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h3 id="GridSearchCV-自动调参"><a href="#GridSearchCV-自动调参" class="headerlink" title="GridSearchCV 自动调参"></a>GridSearchCV 自动调参</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm, datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">parameters = &#123;<span class="string">'kernel'</span>: (<span class="string">'linear'</span>, <span class="string">'rbf'</span>), <span class="string">'C'</span>: [<span class="number">1</span>, <span class="number">10</span>]&#125;</span><br><span class="line">svc = svm.SVC(gamma=<span class="string">"scale"</span>)</span><br><span class="line">clf = GridSearchCV(svc, parameters, cv=<span class="number">5</span>)</span><br><span class="line">clf.fit(iris.data, iris.target)  <span class="comment"># 运行网格搜索</span></span><br><span class="line">print(clf.best_estimator_)  <span class="comment"># 给出最好模型</span></span><br><span class="line"><span class="comment"># SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,</span></span><br><span class="line"><span class="comment">#   decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',</span></span><br><span class="line"><span class="comment">#   max_iter=-1, probability=False, random_state=None, shrinking=True,</span></span><br><span class="line"><span class="comment">#   tol=0.001, verbose=False)</span></span><br><span class="line">print(clf.best_params_)  <span class="comment"># 描述了已取得最佳结果的参数的组合</span></span><br><span class="line"><span class="comment"># &#123;'C': 1, 'kernel': 'linear'&#125;</span></span><br><span class="line">print(clf.best_score_)  <span class="comment"># 成员提供优化过程期间观察到的最好的评分</span></span><br><span class="line"><span class="comment"># 0.98</span></span><br></pre></td></tr></table></figure>
<p>配合pipeline使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline, FeatureUnion</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">selection = SelectKBest(k=<span class="number">1</span>)</span><br><span class="line">combined_features = FeatureUnion([(<span class="string">"pca"</span>, pca), (<span class="string">"univ_select"</span>, selection)])</span><br><span class="line">X_features = combined_features.fit(X, y).transform(X)</span><br><span class="line">svm = SVC(kernel=<span class="string">"linear"</span>)</span><br><span class="line">pipeline = Pipeline([(<span class="string">"features"</span>, combined_features), (<span class="string">"svm"</span>, svm)])</span><br><span class="line">param_grid = dict(features__pca__n_components=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                  features__univ_select__k=[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                  svm__C=[<span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=<span class="number">10</span>)</span><br><span class="line">grid_search.fit(X, y)</span><br><span class="line">print(grid_search.best_estimator_)</span><br><span class="line"><span class="comment"># Pipeline(memory=None,</span></span><br><span class="line"><span class="comment">#      steps=[('features', FeatureUnion(n_jobs=None,</span></span><br><span class="line"><span class="comment">#        transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,</span></span><br><span class="line"><span class="comment">#   svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=2, score_func=&lt;function f_classif at 0x126163620&gt;))],</span></span><br><span class="line"><span class="comment">#        transformer...r', max_iter=-1, probability=False, random_state=None,</span></span><br><span class="line"><span class="comment">#   shrinking=True, tol=0.001, verbose=False))])</span></span><br></pre></td></tr></table></figure>
<h2 id="feature-selection"><a href="#feature-selection" class="headerlink" title="feature_selection"></a>feature_selection</h2><h3 id="SelectKBest-最大分数特征选择器"><a href="#SelectKBest-最大分数特征选择器" class="headerlink" title="SelectKBest 最大分数特征选择器"></a>SelectKBest 最大分数特征选择器</h3><p>最大分数特征选择器selection，它的基本原理是根据方差分析计算类标签与特征之间的F值，进而选择F值最大的特征</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest, chi2</span><br><span class="line">X, y = load_digits(return_X_y=<span class="literal">True</span>)</span><br><span class="line">print(X.shape)</span><br><span class="line"><span class="comment"># (1797, 64)</span></span><br><span class="line">X_new = SelectKBest(chi2, k=<span class="number">20</span>).fit_transform(X, y) <span class="comment"># chi2是score_fun，可以换成其他的</span></span><br><span class="line">print(X_new.shape)</span><br><span class="line"><span class="comment"># (1797, 20)</span></span><br></pre></td></tr></table></figure>
<h2 id="metrics"><a href="#metrics" class="headerlink" title="metrics"></a>metrics</h2><h3 id="roc-curve-auc"><a href="#roc-curve-auc" class="headerlink" title="roc_curve/auc"></a>roc_curve/auc</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> auc</span><br><span class="line">y = np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">pred = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])</span><br><span class="line">fpr, tpr, thresholds = roc_curve(y, pred, pos_label=<span class="number">2</span>)</span><br><span class="line">print(tpr)</span><br><span class="line"><span class="comment"># [0.  0.5 0.5 1.  1. ]</span></span><br><span class="line">print(fpr)</span><br><span class="line"><span class="comment"># [0.  0.  0.5 0.5 1. ]</span></span><br><span class="line">print(thresholds)</span><br><span class="line"><span class="comment"># [1.8  0.8  0.4  0.35 0.1 ]</span></span><br><span class="line">print(auc(fpr, tpr))</span><br><span class="line"><span class="comment"># 0.75</span></span><br></pre></td></tr></table></figure>
<p>roc_auc_score(true_y, pred_proba_y)</p>
<p>直接根据真实值（必须是二值）、预测值（可以是0/1, 也可以是proba值）计算出auc值，中间过程的roc计算省略，但是只能用于二元分类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用法似乎不太对，下次来看</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> auc</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">pred = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>])</span><br><span class="line">fpr, tpr, thresholds = roc_curve(y, pred, pos_label=<span class="number">2</span>)</span><br><span class="line">print(tpr)</span><br><span class="line">print(fpr)</span><br><span class="line">print(thresholds)</span><br><span class="line">print(auc(fpr, tpr))</span><br><span class="line">print(roc_auc_score(y, pred))</span><br></pre></td></tr></table></figure>
<h3 id="accuracy-score-正确率"><a href="#accuracy-score-正确率" class="headerlink" title="accuracy_score 正确率"></a>accuracy_score 正确率</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="comment"># The best performance is 1 with normalize == True and the number of samples with normalize == False.</span></span><br><span class="line">print(accuracy_score(y_true, y_pred))</span><br><span class="line"><span class="comment"># 0.5</span></span><br><span class="line">print(accuracy_score(y_true, y_pred, normalize=<span class="literal">False</span>))</span><br><span class="line"><span class="comment"># 2</span></span><br></pre></td></tr></table></figure>
<h3 id="recall-score-召回率"><a href="#recall-score-召回率" class="headerlink" title="recall_score 召回率"></a>recall_score 召回率</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">print(recall_score(y_true, y_pred, average=<span class="string">'macro'</span>))</span><br><span class="line"><span class="comment"># 0.3333333333333333</span></span><br><span class="line">print(recall_score(y_true, y_pred, average=<span class="string">'micro'</span>))</span><br><span class="line"><span class="comment"># 0.3333333333333333</span></span><br><span class="line">print(recall_score(y_true, y_pred, average=<span class="string">'weighted'</span>))</span><br><span class="line"><span class="comment"># 0.3333333333333333</span></span><br><span class="line">print(recall_score(y_true, y_pred, average=<span class="literal">None</span>))</span><br><span class="line"><span class="comment"># [1. 0. 0.]</span></span><br></pre></td></tr></table></figure>
<h2 id="calibration"><a href="#calibration" class="headerlink" title="calibration"></a>calibration</h2><h3 id="calibration-curve-预测模型校准曲线"><a href="#calibration-curve-预测模型校准曲线" class="headerlink" title="calibration_curve 预测模型校准曲线"></a>calibration_curve 预测模型校准曲线</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不是很懂</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.calibration <span class="keyword">import</span> calibration_curve</span><br><span class="line"></span><br><span class="line">model = LogisticRegression(penalty=<span class="string">'l2'</span>, dual=<span class="literal">False</span>, tol=<span class="number">0.0001</span>, C=<span class="number">1.0</span>,</span><br><span class="line">    fit_intercept=<span class="literal">True</span>, intercept_scaling=<span class="number">1</span>, class_weight=<span class="literal">None</span>,</span><br><span class="line">    random_state=<span class="literal">None</span>, solver=<span class="string">'liblinear'</span>, max_iter=<span class="number">100</span>, multi_class=<span class="string">'ovr'</span>,</span><br><span class="line">    verbose=<span class="number">0</span>, warm_start=<span class="literal">False</span>, n_jobs=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">100000</span>, n_features=<span class="number">20</span>,</span><br><span class="line">                                    n_informative=<span class="number">2</span>, n_redundant=<span class="number">10</span>,</span><br><span class="line">                                    random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">model.fit(X, y)</span><br><span class="line">res_y = model.predict_proba(X)[:, <span class="number">1</span>]</span><br><span class="line">fraction_of_positives, mean_predicted_value = \</span><br><span class="line">            calibration_curve(y, res_y, n_bins=<span class="number">6</span>)</span><br><span class="line">print(fraction_of_positives)</span><br><span class="line"><span class="comment"># [0.04732775 0.23074366 0.42315634 0.59755379 0.7659805  0.94946347]</span></span><br><span class="line">print(mean_predicted_value)</span><br><span class="line"><span class="comment"># [0.04991174 0.24241127 0.41556163 0.58441272 0.75749505 0.95017895]</span></span><br></pre></td></tr></table></figure>
<h2 id="linear-model"><a href="#linear-model" class="headerlink" title="linear_model"></a>linear_model</h2><h3 id="LinearRegression-线性回归"><a href="#LinearRegression-线性回归" class="headerlink" title="LinearRegression 线性回归"></a>LinearRegression 线性回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line">model = LinearRegression(fit_intercept=<span class="literal">True</span>, normalize=<span class="literal">False</span>, </span><br><span class="line">    copy_X=<span class="literal">True</span>, n_jobs=<span class="number">1</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">参数</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">    fit_intercept：是否计算截距。False-模型没有截距</span></span><br><span class="line"><span class="string">    normalize： 当fit_intercept设置为False时，该参数将被忽略。 如果为真，则回归前的回归系数X将通过减去平均值并除以l2-范数而归一化。</span></span><br><span class="line"><span class="string">     n_jobs：指定线程数</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p><img src="/2019/03/08/机器学习-Sklearn/3.png" alt="3"></p>
<h3 id="LogisticRegression-逻辑回归"><a href="#LogisticRegression-逻辑回归" class="headerlink" title="LogisticRegression 逻辑回归"></a>LogisticRegression 逻辑回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment"># 定义逻辑回归模型</span></span><br><span class="line">model = LogisticRegression(penalty=<span class="string">'l2'</span>, dual=<span class="literal">False</span>, tol=<span class="number">0.0001</span>, C=<span class="number">1.0</span>, </span><br><span class="line">    fit_intercept=<span class="literal">True</span>, intercept_scaling=<span class="number">1</span>, class_weight=<span class="literal">None</span>, </span><br><span class="line">    random_state=<span class="literal">None</span>, solver=<span class="string">'liblinear'</span>, max_iter=<span class="number">100</span>, multi_class=<span class="string">'ovr'</span>, </span><br><span class="line">    verbose=<span class="number">0</span>, warm_start=<span class="literal">False</span>, n_jobs=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""参数</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">    penalty：使用指定正则化项（默认：l2）</span></span><br><span class="line"><span class="string">    dual: n_samples &gt; n_features取False（默认）</span></span><br><span class="line"><span class="string">    C：正则化强度的反，值越小正则化强度越大</span></span><br><span class="line"><span class="string">    n_jobs: 指定线程数</span></span><br><span class="line"><span class="string">    random_state：随机数生成器</span></span><br><span class="line"><span class="string">    fit_intercept: 是否需要常量</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="naive-bayes"><a href="#naive-bayes" class="headerlink" title="naive_bayes"></a>naive_bayes</h2><h3 id="GaussianNB-高斯贝叶斯"><a href="#GaussianNB-高斯贝叶斯" class="headerlink" title="GaussianNB 高斯贝叶斯"></a>GaussianNB 高斯贝叶斯</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> naive_bayes</span><br><span class="line">model = naive_bayes.GaussianNB() <span class="comment"># 高斯贝叶斯</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">文本分类问题常用MultinomialNB</span></span><br><span class="line"><span class="string">参数</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">    alpha：平滑参数</span></span><br><span class="line"><span class="string">    fit_prior：是否要学习类的先验概率；false-使用统一的先验概率</span></span><br><span class="line"><span class="string">    class_prior: 是否指定类的先验概率；若指定则不能根据参数调整</span></span><br><span class="line"><span class="string">    binarize: 二值化的阈值，若为None，则假设输入由二进制向量组成</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h3 id="MultinomialNB-多项式分布贝叶斯"><a href="#MultinomialNB-多项式分布贝叶斯" class="headerlink" title="MultinomialNB 多项式分布贝叶斯"></a>MultinomialNB 多项式分布贝叶斯</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> naive_bayes</span><br><span class="line">model = naive_bayes.MultinomialNB(alpha=<span class="number">1.0</span>, fit_prior=<span class="literal">True</span>, class_prior=<span class="literal">None</span>) <span class="comment"># 多项分布贝叶斯，使用离散变量</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">文本分类问题常用MultinomialNB</span></span><br><span class="line"><span class="string">参数</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">    alpha：平滑参数</span></span><br><span class="line"><span class="string">    fit_prior：是否要学习类的先验概率；false-使用统一的先验概率</span></span><br><span class="line"><span class="string">    class_prior: 是否指定类的先验概率；若指定则不能根据参数调整</span></span><br><span class="line"><span class="string">    binarize: 二值化的阈值，若为None，则假设输入由二进制向量组成</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h3 id="BernoulliNB-伯努利贝叶斯"><a href="#BernoulliNB-伯努利贝叶斯" class="headerlink" title="BernoulliNB 伯努利贝叶斯"></a>BernoulliNB 伯努利贝叶斯</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> naive_bayes</span><br><span class="line">model = naive_bayes.BernoulliNB(alpha=<span class="number">1.0</span>, binarize=<span class="number">0.0</span>, fit_prior=<span class="literal">True</span>, class_prior=<span class="literal">None</span>) <span class="comment"># 伯努利贝叶斯</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">文本分类问题常用MultinomialNB</span></span><br><span class="line"><span class="string">参数</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">    alpha：平滑参数</span></span><br><span class="line"><span class="string">    fit_prior：是否要学习类的先验概率；false-使用统一的先验概率</span></span><br><span class="line"><span class="string">    class_prior: 是否指定类的先验概率；若指定则不能根据参数调整</span></span><br><span class="line"><span class="string">    binarize: 二值化的阈值，若为None，则假设输入由二进制向量组成</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="tree"><a href="#tree" class="headerlink" title="tree"></a>tree</h2><h3 id="DecisionTreeClassifier-决策树"><a href="#DecisionTreeClassifier-决策树" class="headerlink" title="DecisionTreeClassifier 决策树"></a>DecisionTreeClassifier 决策树</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree </span><br><span class="line">model = tree.DecisionTreeClassifier(criterion=<span class="string">'gini'</span>, max_depth=<span class="literal">None</span>, </span><br><span class="line">    min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, </span><br><span class="line">    max_features=<span class="literal">None</span>, random_state=<span class="literal">None</span>, max_leaf_nodes=<span class="literal">None</span>, </span><br><span class="line">    min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=<span class="literal">None</span>,</span><br><span class="line">     class_weight=<span class="literal">None</span>, presort=<span class="literal">False</span>)</span><br><span class="line"><span class="string">"""参数</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">    criterion ：特征选择准则gini/entropy</span></span><br><span class="line"><span class="string">    max_depth：树的最大深度，None-尽量下分</span></span><br><span class="line"><span class="string">    min_samples_split：分裂内部节点，所需要的最小样本树</span></span><br><span class="line"><span class="string">    min_samples_leaf：叶子节点所需要的最小样本数</span></span><br><span class="line"><span class="string">    max_features: 寻找最优分割点时的最大特征数</span></span><br><span class="line"><span class="string">    max_leaf_nodes：优先增长到最大叶子节点数</span></span><br><span class="line"><span class="string">    min_impurity_decrease：如果这种分离导致杂质的减少大于或等于这个值，则节点将被拆分。</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="svm"><a href="#svm" class="headerlink" title="svm"></a>svm</h2><h3 id="SVC-SVM-c语言"><a href="#SVC-SVM-c语言" class="headerlink" title="SVC SVM(c语言)"></a>SVC SVM(c语言)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">model = SVC(C=<span class="number">1.0</span>, kernel=’rbf’, gamma=’auto’)</span><br><span class="line"><span class="string">"""参数</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">    C：误差项的惩罚参数C</span></span><br><span class="line"><span class="string">    gamma: 核相关系数。浮点数，If gamma is ‘auto’ then 1/n_features will be used instead.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="neighbors"><a href="#neighbors" class="headerlink" title="neighbors"></a>neighbors</h2><h3 id="KNeighborsClassifier"><a href="#KNeighborsClassifier" class="headerlink" title="KNeighborsClassifier"></a>KNeighborsClassifier</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors</span><br><span class="line"><span class="comment">#定义kNN分类模型</span></span><br><span class="line">model = neighbors.KNeighborsClassifier(n_neighbors=<span class="number">5</span>, n_jobs=<span class="number">1</span>) <span class="comment"># 分类</span></span><br><span class="line"><span class="string">"""参数</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">    n_neighbors： 使用邻居的数目</span></span><br><span class="line"><span class="string">    n_jobs：并行任务数</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h3 id="KNeighborsRegressor"><a href="#KNeighborsRegressor" class="headerlink" title="KNeighborsRegressor"></a>KNeighborsRegressor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors</span><br><span class="line"><span class="comment">#定义kNN分类模型</span></span><br><span class="line">model = neighbors.KNeighborsRegressor(n_neighbors=<span class="number">5</span>, n_jobs=<span class="number">1</span>) <span class="comment"># 回归</span></span><br><span class="line"><span class="string">"""参数</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">    n_neighbors： 使用邻居的数目</span></span><br><span class="line"><span class="string">    n_jobs：并行任务数</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="neural-network"><a href="#neural-network" class="headerlink" title="neural_network"></a>neural_network</h2><h3 id="MLPClassifier-多层感知机"><a href="#MLPClassifier-多层感知机" class="headerlink" title="MLPClassifier 多层感知机"></a>MLPClassifier 多层感知机</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="comment"># 定义多层感知机分类算法</span></span><br><span class="line">model = MLPClassifier(activation=<span class="string">'relu'</span>, solver=<span class="string">'adam'</span>, alpha=<span class="number">0.0001</span>)</span><br><span class="line"><span class="string">"""参数</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">    hidden_layer_sizes: 元祖</span></span><br><span class="line"><span class="string">    activation：激活函数</span></span><br><span class="line"><span class="string">    solver ：优化算法&#123;‘lbfgs’, ‘sgd’, ‘adam’&#125;</span></span><br><span class="line"><span class="string">    alpha：L2惩罚(正则化项)参数。</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="cluster"><a href="#cluster" class="headerlink" title="cluster"></a>cluster</h2><h3 id="KMeans"><a href="#KMeans" class="headerlink" title="KMeans"></a>KMeans</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">              [<span class="number">10</span>, <span class="number">2</span>], [<span class="number">10</span>, <span class="number">4</span>], [<span class="number">10</span>, <span class="number">0</span>]])</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">2</span>, random_state=<span class="number">0</span>).fit(X)</span><br><span class="line"></span><br><span class="line">kmeans.labels_</span><br><span class="line"><span class="comment"># array([1, 1, 1, 0, 0, 0], dtype=int32)</span></span><br><span class="line"></span><br><span class="line">kmeans.predict([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">12</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="comment"># array([1, 0], dtype=int32)</span></span><br><span class="line"></span><br><span class="line">kmeans.cluster_centers_</span><br><span class="line"><span class="comment"># array([[10.,  2.],</span></span><br><span class="line"><span class="comment">#        [ 1.,  2.]])</span></span><br></pre></td></tr></table></figure>
<h3 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DBSCAN(eps=0.5,  # 邻域半径</span></span><br><span class="line"><span class="comment"># min_samples=5,    # 最小样本点数，MinPts</span></span><br><span class="line"><span class="comment"># metric='euclidean',</span></span><br><span class="line"><span class="comment"># metric_params=None,</span></span><br><span class="line"><span class="comment"># algorithm='auto', # 'auto','ball_tree','kd_tree','brute',4个可选的参数 寻找最近邻点的算法，例如直接密度可达的点</span></span><br><span class="line"><span class="comment"># leaf_size=30, # balltree,cdtree的参数</span></span><br><span class="line"><span class="comment"># p=None, # </span></span><br><span class="line"><span class="comment"># n_jobs=1)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">              [<span class="number">8</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">8</span>], [<span class="number">25</span>, <span class="number">80</span>]])</span><br><span class="line">clustering = DBSCAN(eps=<span class="number">3</span>, min_samples=<span class="number">2</span>).fit(X)</span><br><span class="line"></span><br><span class="line">clustering.labels_</span><br><span class="line"><span class="comment">#array([ 0,  0,  0,  1,  1, -1])</span></span><br></pre></td></tr></table></figure>
<h2 id="ensemble"><a href="#ensemble" class="headerlink" title="ensemble"></a>ensemble</h2><h3 id="AdaBoostClassifier"><a href="#AdaBoostClassifier" class="headerlink" title="AdaBoostClassifier"></a>AdaBoostClassifier</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">4</span>,</span><br><span class="line">                           n_informative=<span class="number">2</span>, n_redundant=<span class="number">0</span>,</span><br><span class="line">                           random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">clf = AdaBoostClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">0</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">print(clf.feature_importances_)  <span class="comment"># the higher, the more important the feature</span></span><br><span class="line"><span class="comment"># [0.28 0.42 0.14 0.16]</span></span><br><span class="line">print(clf.predict([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]))</span><br><span class="line"><span class="comment"># [1]</span></span><br></pre></td></tr></table></figure>
<h3 id="RandomForestClassifier-随机森林分类"><a href="#RandomForestClassifier-随机森林分类" class="headerlink" title="RandomForestClassifier 随机森林分类"></a>RandomForestClassifier 随机森林分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">4</span>,</span><br><span class="line">                           n_informative=<span class="number">2</span>, n_redundant=<span class="number">0</span>,</span><br><span class="line">                           random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">clf = RandomForestClassifier(n_estimators=<span class="number">100</span>, max_depth=<span class="number">2</span>,</span><br><span class="line">                             random_state=<span class="number">0</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">    </span><br><span class="line">print(clf.feature_importances_)  <span class="comment"># the higher, the more important the feature</span></span><br><span class="line"><span class="comment"># [0.14205973 0.76664038 0.0282433  0.06305659]</span></span><br><span class="line">print(clf.predict([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]))</span><br><span class="line"><span class="comment"># [1]</span></span><br></pre></td></tr></table></figure>
<h3 id="RandomTreesEmbedding-随机森林数据嵌入"><a href="#RandomTreesEmbedding-随机森林数据嵌入" class="headerlink" title="RandomTreesEmbedding 随机森林数据嵌入"></a>RandomTreesEmbedding 随机森林数据嵌入</h3><p>Totally Random Trees Embedding(以下简称 TRTE)是一种非监督学习的数据转化方法。它将低维的数据集映射到高维，从而让映射到高维的数据更好的运用于分类回归模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 暂时还不懂什么意思</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomTreesEmbedding</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data[:<span class="number">100</span>], iris.target[:<span class="number">100</span>]</span><br><span class="line">rte = RandomTreesEmbedding(n_estimators=<span class="number">2</span>, max_depth=<span class="number">5</span>, min_samples_split=<span class="number">2</span>,</span><br><span class="line">                           min_samples_leaf=<span class="number">1</span>)</span><br><span class="line">rte.fit(X, y)</span><br><span class="line"></span><br><span class="line">feature_X = rte.transform(X)</span><br><span class="line">print(X[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># [5.1 3.5 1.4 0.2]</span></span><br><span class="line">print(feature_X[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#   (0, 6)	1.0</span></span><br><span class="line"><span class="comment">#   (0, 20)	1.0</span></span><br><span class="line">print(feature_X.shape)</span><br><span class="line"><span class="comment"># (100, 41)</span></span><br></pre></td></tr></table></figure>
<h3 id="GradientBoostingClassifier-GBDT的分类树"><a href="#GradientBoostingClassifier-GBDT的分类树" class="headerlink" title="GradientBoostingClassifier GBDT的分类树"></a>GradientBoostingClassifier GBDT的分类树</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不知道为什么准确率一直不变</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line">model = GradientBoostingClassifier(learning_rate=<span class="number">0.2</span>, min_samples_split=<span class="number">1000</span>,</span><br><span class="line">                                   min_samples_leaf=<span class="number">150</span>,max_depth=<span class="number">5</span>,max_features=<span class="string">'sqrt'</span>,subsample=<span class="number">0.6</span>)</span><br><span class="line"></span><br><span class="line">model.fit(X, y)</span><br><span class="line">res_y = model.predict(X)</span><br><span class="line">scores = accuracy_score(res_y, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Accuracy:  [%s]"</span> % scores)</span><br><span class="line"><span class="comment"># Accuracy:  [0.3333333333333333]</span></span><br></pre></td></tr></table></figure>
<h2 id="gaussian-process"><a href="#gaussian-process" class="headerlink" title="gaussian_process"></a>gaussian_process</h2><h3 id="GaussianProcessClassifier-高斯过程分类"><a href="#GaussianProcessClassifier-高斯过程分类" class="headerlink" title="GaussianProcessClassifier 高斯过程分类"></a>GaussianProcessClassifier 高斯过程分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.gaussian_process <span class="keyword">import</span> GaussianProcessClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.gaussian_process.kernels <span class="keyword">import</span> RBF</span><br><span class="line"></span><br><span class="line">X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line">kernel = <span class="number">1.0</span> * RBF(<span class="number">1.0</span>)</span><br><span class="line">gpc = GaussianProcessClassifier(kernel=kernel,</span><br><span class="line">        random_state=<span class="number">0</span>).fit(X, y)</span><br><span class="line"></span><br><span class="line">print(gpc.score(X, y))</span><br><span class="line"><span class="comment"># 0.9866666666666667</span></span><br><span class="line">print(gpc.predict_proba(X[:<span class="number">2</span>, :]))</span><br><span class="line"><span class="comment"># [[0.83548752 0.03228706 0.13222543]</span></span><br><span class="line"><span class="comment">#  [0.79064206 0.06525643 0.14410151]]</span></span><br></pre></td></tr></table></figure>
<h3 id="kernal-核函数"><a href="#kernal-核函数" class="headerlink" title="kernal 核函数"></a>kernal 核函数</h3><p>RBF</p>
<h2 id="discriminant-analysis"><a href="#discriminant-analysis" class="headerlink" title="discriminant_analysis"></a>discriminant_analysis</h2><h3 id="LinearDiscriminantAnalysis-线性判别分析"><a href="#LinearDiscriminantAnalysis-线性判别分析" class="headerlink" title="LinearDiscriminantAnalysis 线性判别分析"></a>LinearDiscriminantAnalysis 线性判别分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line">X = np.array([[<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">-2</span>, <span class="number">-1</span>], [<span class="number">-3</span>, <span class="number">-2</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">clf = LinearDiscriminantAnalysis()</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">print(clf.predict([[<span class="number">-0.8</span>, <span class="number">-1</span>]]))</span><br><span class="line"><span class="comment"># [1]</span></span><br></pre></td></tr></table></figure>
<h3 id="QuadraticDiscriminantAnalysis-二次判别分析"><a href="#QuadraticDiscriminantAnalysis-二次判别分析" class="headerlink" title="QuadraticDiscriminantAnalysis 二次判别分析"></a>QuadraticDiscriminantAnalysis 二次判别分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> QuadraticDiscriminantAnalysis</span><br><span class="line">X = np.array([[<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">-2</span>, <span class="number">-1</span>], [<span class="number">-3</span>, <span class="number">-2</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">clf = QuadraticDiscriminantAnalysis()</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">print(clf.predict([[<span class="number">-0.8</span>, <span class="number">-1</span>]]))</span><br><span class="line"><span class="comment"># [1]</span></span><br></pre></td></tr></table></figure>
<h2 id="载入-保存模型"><a href="#载入-保存模型" class="headerlink" title="载入/保存模型"></a>载入/保存模型</h2><h3 id="pickle"><a href="#pickle" class="headerlink" title="pickle"></a>pickle</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'model.pickle'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(model, f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取模型</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'model.pickle'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    model = pickle.load(f)</span><br><span class="line">model.predict(X_test)</span><br></pre></td></tr></table></figure>
<h3 id="joblib"><a href="#joblib" class="headerlink" title="joblib"></a>joblib</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">joblib.dump(model, <span class="string">'model.pickle'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入模型</span></span><br><span class="line">model = joblib.load(<span class="string">'model.pickle'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h2><h3 id="Pipeline-make-pipeline-流式化处理"><a href="#Pipeline-make-pipeline-流式化处理" class="headerlink" title="Pipeline / make_pipeline 流式化处理"></a>Pipeline / make_pipeline 流式化处理</h3><p>pipeline 实现了对全部步骤的流式化封装和管理，可以很方便地使参数集在新数据集上被重复使用。</p>
<p>pipeline 可以用于下面几处：</p>
<ul>
<li><p>模块化 Feature Transform，只需写很少的代码就能将新的 Feature 更新到训练集中</p>
</li>
<li><p>自动化 Grid Search，只要预先设定好使用的 Model 和参数的候选，就能自动搜索并记录最佳的 Model</p>
</li>
<li><p>自动化 Ensemble Generation，每隔一段时间将现有最好的 K 个 Model 拿来做 Ensemble</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要联网</span></span><br><span class="line">df = pd.read_csv(<span class="string">'./wdbc.csv'</span>, header=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># Breast Cancer Wisconsin dataset</span></span><br><span class="line">X, y = df.values[:, <span class="number">2</span>:], df.values[:, <span class="number">1</span>]</span><br><span class="line">encoder = LabelEncoder()</span><br><span class="line">y = encoder.fit_transform(y)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line">pipe_lr = Pipeline([(<span class="string">'sc'</span>, StandardScaler()),</span><br><span class="line">                    (<span class="string">'pca'</span>, PCA(n_components=<span class="number">2</span>)),</span><br><span class="line">                    (<span class="string">'clf'</span>, LogisticRegression(random_state=<span class="number">1</span>))</span><br><span class="line">                    ])</span><br><span class="line">pipe_lr.fit(X_train, y_train)</span><br><span class="line">print(<span class="string">'Test accuracy: %.3f'</span> % pipe_lr.score(X_test, y_test))</span><br><span class="line"><span class="comment"># 先用 StandardScaler 对数据集每一列做标准化处理（是transformer）</span></span><br><span class="line"><span class="comment"># 再用 PCA 将原始的 30 维度特征压缩的 2 维度（是transformer）</span></span><br><span class="line"><span class="comment"># 最后再用模型 LogisticRegression（是Estimator）</span></span><br><span class="line"><span class="comment"># 调用 Pipeline 时，输入由元组构成的列表，每个元组第一个值为变量名，元组第二个元素是 sklearn 中的 transformer或Estimator</span></span><br></pre></td></tr></table></figure>
<p>作为等价对照，和下面的代码等价</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要联网</span></span><br><span class="line">df = pd.read_csv(<span class="string">'./wdbc.csv'</span>, header=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># Breast Cancer Wisconsin dataset</span></span><br><span class="line">X, y = df.values[:, <span class="number">2</span>:], df.values[:, <span class="number">1</span>]</span><br><span class="line">encoder = LabelEncoder()</span><br><span class="line">y = encoder.fit_transform(y)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">sc = StandardScaler()</span><br><span class="line">sc.fit(X_train)</span><br><span class="line">X_train = sc.transform(X_train)</span><br><span class="line">X_test = sc.transform(X_test)</span><br><span class="line"></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca.fit(X_train)</span><br><span class="line">X_train = pca.transform(X_train)</span><br><span class="line">X_test = pca.transform(X_test)</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(random_state=<span class="number">0</span>, solver=<span class="string">'lbfgs'</span>, multi_class=<span class="string">'multinomial'</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">score = clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line">print(score)</span><br></pre></td></tr></table></figure>
<p>make_pipeline和pipeline差不多</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">print(make_pipeline(StandardScaler(), GaussianNB(priors=<span class="literal">None</span>)))</span><br><span class="line"><span class="comment"># Pipeline(memory=None,</span></span><br><span class="line"><span class="comment">#          steps=[('standardscaler',</span></span><br><span class="line"><span class="comment">#                  StandardScaler(copy=True, with_mean=True, with_std=True)),</span></span><br><span class="line"><span class="comment">#                 ('gaussiannb',</span></span><br><span class="line"><span class="comment">#                  GaussianNB(priors=None, var_smoothing=1e-09))])</span></span><br></pre></td></tr></table></figure>
<h3 id="FeatureUnion-特征组合-Concatenates"><a href="#FeatureUnion-特征组合-Concatenates" class="headerlink" title="FeatureUnion 特征组合(Concatenates)"></a>FeatureUnion 特征组合(Concatenates)</h3><p>将列表里面的元素组合在一起（注意不是pipeline）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> FeatureUnion</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data[:<span class="number">3</span>], iris.target[:<span class="number">3</span>]</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">selection = SelectKBest(k=<span class="number">1</span>)</span><br><span class="line">combined_features = FeatureUnion([(<span class="string">"pca"</span>, pca), (<span class="string">"univ_select"</span>, selection)])</span><br><span class="line">X_features = combined_features.fit(X, y).transform(X)</span><br><span class="line">print(X_features)</span><br><span class="line"><span class="comment"># [[ 0.33478115 -0.01199189  0.2       ]</span></span><br><span class="line"><span class="comment">#  [-0.18764949 -0.14262968  0.2       ]</span></span><br><span class="line"><span class="comment">#  [-0.14713166  0.15462157  0.2       ]]</span></span><br></pre></td></tr></table></figure>
<p>等价于</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data[:<span class="number">3</span>], iris.target[:<span class="number">3</span>]</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">X_features1 = pca.fit(X, y).transform(X)</span><br><span class="line">print(X_features1)</span><br><span class="line"><span class="comment"># [[ 0.33478115 -0.01199189]</span></span><br><span class="line"><span class="comment">#  [-0.18764949 -0.14262968]</span></span><br><span class="line"><span class="comment">#  [-0.14713166  0.15462157]]</span></span><br><span class="line">X_features2 = SelectKBest(k=<span class="number">1</span>).fit(X, y).transform(X)</span><br><span class="line">print(X_features2)</span><br><span class="line"><span class="comment"># [[0.2]</span></span><br><span class="line"><span class="comment">#  [0.2]</span></span><br><span class="line"><span class="comment">#  [0.2]]</span></span><br><span class="line">print(np.hstack((X_features1, X_features2)))</span><br><span class="line"><span class="comment"># [[ 0.33478115 -0.01199189  0.2       ]</span></span><br><span class="line"><span class="comment">#  [-0.18764949 -0.14262968  0.2       ]</span></span><br><span class="line"><span class="comment">#  [-0.14713166  0.15462157  0.2       ]]</span></span><br></pre></td></tr></table></figure>
<h1 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/机器学习-聚类/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/机器学习-聚类/" itemprop="url">机器学习-聚类</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:44:43+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="划分（分裂）方法"><a href="#划分（分裂）方法" class="headerlink" title="划分（分裂）方法"></a>划分（分裂）方法</h1><h2 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h2><p>1.确定分类个数k</p>
<p>2.随机选择k个聚类中心点</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190815153538284.png" alt="image-20190815153538284"></p>
<p>3.每个数据点找到离它最近的中心点，划为一类</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190815153637062.png" alt="image-20190815153637062"></p>
<p>4.每一类中找到新的中心点</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190815153813468.png" alt="image-20190815153813468"></p>
<p>5.一直迭代直到没有变化为止</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190815153846268.png" alt="image-20190815153846268"></p>
<h3 id="加速"><a href="#加速" class="headerlink" title="加速"></a>加速</h3><p>Dan Pelleg and Andrew Moore. Accelerating Exact k-means Algorithms with Geometric Reasoning. Proc. Conference on Knowledge Discovery in Databases 1999, (KDD99) </p>
<h3 id="优化方面"><a href="#优化方面" class="headerlink" title="优化方面"></a>优化方面</h3><p>现在假设进行数据传输，但是由于资源限制，只能传输中心点数据，怎样才能使误差平方之和最小？</p>
<script type="math/tex; mode=display">
encoder \ function: ENCODE: \mathfrak R^m \to [1..k]\\
decoder \ function: DECODE: \mathfrak [1..k] \to R^m\\
定义失真:\\
Distortion = \sum_{i=1}^R (x_i-DECODE[ENCODE(x_i)])^2\\
可写作：DECODE[j] = C_j \ 即数据j所对应的中心点\\
Distortion = \sum_{i=1}^R (x_i-C_{ENCODE(x_i)})^2</script><p>为了让失真Distortion最小，我们需要</p>
<p>(1)xi需要被编码到最近的中心点</p>
<script type="math/tex; mode=display">
C_{ENCODE(x_i)} = {\arg \min}_{c_j \in \{c_1,...,c_k\}} (x_i - c_j)^2</script><p>(2)对于每一个中心点c_j的偏差，它关于c_j的偏导数为0</p>
<script type="math/tex; mode=display">
Distortion = \sum_{i=1}^R (x_i-C_{ENCODE(x_i)})^2\\
= \sum_{j=1}^k \sum_{i \in OwnedBy(c_j)}(x_i-c_j)^2 \quad k个中心点\\
\frac{\partial Distortion}{\partial c_j} = \frac{\partial}{\partial c_j} \sum_{i \in OwnedBy(c_j)}(x_i-c_j)^2\\
= -2c_j\sum_{i \in OwnedBy(c_j)}(x_i-c_j)\\
= 0(for \ a \ minimum)\\
因此c_j = \frac{1}{|OwnedBy(c_j)|}\sum_{i \in OwnedBy(c_j)}x_i</script><p>因此中心点时改类别中，所有数据的质心</p>
<h4 id="算法是否停止"><a href="#算法是否停止" class="headerlink" title="算法是否停止"></a>算法是否停止</h4><p>当失真Distortion不是最小的时候，通过反复执行下面两步改变中心点c1,c2,,,ck</p>
<p>(1)改变编码，使得x_i归属于最近的中心点</p>
<p>(2) 设置每一类的中心点为数据的质心</p>
<p>很容易证明这个过程最终会停止(1,2两步不再改变中心点)</p>
<p>因为当数据有限时，把它们划分为K组的方案也是有限的，因此可能的中心点(质心)组合是有限的</p>
<p>而如果某一次迭代中改变了中心点，那么失真Distortion是下降的，因此最终会走到一个界限上，不能再往前进</p>
<h4 id="最优解"><a href="#最优解" class="headerlink" title="最优解"></a>最优解</h4><p>虽然算法一定会停止，但是不一定是最优解</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190815165806306.png" alt="image-20190815165806306"></p>
<p>一些找到尽量好的解的方法：</p>
<ul>
<li><p>初始化时注意一下</p>
<p>  将第一个中心放在随机选择的数据点上</p>
<p>  将第二个中心放在距离第一个中心尽可能远的位置</p>
<p>  ：</p>
<p>  将第J个中心放在距离第1-j-1个中心尽可能远的位置</p>
</li>
<li><p>运行多次k-means，获得不同的结果</p>
</li>
</ul>
<h4 id="K的选择"><a href="#K的选择" class="headerlink" title="K的选择"></a>K的选择</h4><p>是一个困难的问题</p>
<p>普遍的反法是找到一个使Schwarz Criterion (也叫贝叶斯信息度量/准则 BIC)较小的解</p>
<script type="math/tex; mode=display">
Distortion + \lambda(\#parameters)\log R\\
= Distortion + \lambda mk\log R\\
m = \#dimensions\\
k = \#Centers\\
R = \#Records</script><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>矢量量化</p>
<p>二分k-means，k-medoids（k中心点），clarans算法（基于选择的算法）,k-位数 K-medians</p>
<h1 id="层次分析方法"><a href="#层次分析方法" class="headerlink" title="层次分析方法"></a>层次分析方法</h1><p>1)单链(Single-link):不同两个聚类中离得最近的两个点之间的距离,即MIN</p>
<p>​    容易受到异常值，噪声的影响</p>
<p>2)全链(Complete-link):不同两个聚类中离得最远的两个点之间的距离,即MAX</p>
<p>​    对于非球状簇的表现不好</p>
<p>3)平均链(Average-link，GROUP-AVERAGE):不同两个聚类中所有点对距离的平均值,即AVERAGE</p>
<p>​    对于非球状簇的表现不好</p>
<h2 id="层次凝聚聚类-Hierarchical-Agglomerative-Clustering-AGNES-AGglomerative-NESting"><a href="#层次凝聚聚类-Hierarchical-Agglomerative-Clustering-AGNES-AGglomerative-NESting" class="headerlink" title="层次凝聚聚类 Hierarchical Agglomerative Clustering (AGNES(AGglomerative NESting))"></a>层次凝聚聚类 Hierarchical Agglomerative Clustering (AGNES(AGglomerative NESting))</h2><p>是一种单链接聚类方法 Single Linkage Hierarchical Clustering</p>
<p>1.每个点都是一个簇</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190815191732179.png" alt="image-20190815191732179"></p>
<p>2.找到最接近的两个簇，并且合并为父级簇</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190815191756913.png" alt="image-20190815191756913"></p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190815192930910.png" alt="image-20190815192930910"></p>
<p>3.重复上述过程，直到将整个数据集合并为一个簇</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190815193248733.png" alt="image-20190815193248733"></p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190815193300131.png" alt="image-20190815193300131"></p>
<p>如何确定簇之间的相似度：</p>
<ul>
<li>簇中的点之间的最小距离（可使用Euclidian Minimum Spanning Trees）</li>
<li>簇中的点之间的最大距离</li>
<li>簇中的点之间的平均距离</li>
</ul>
<p>最终得到的是一个树状图dendrogram, or taxonomy, or hierarchy</p>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li>得到的是一个层次结构，而不是没有结构的组织</li>
<li>如果想得到k个组，只需要剪掉k-1个最长的链即可</li>
<li>没有太多的统计或信息论基础在里面</li>
</ul>
<h2 id="BIRCH"><a href="#BIRCH" class="headerlink" title="BIRCH"></a>BIRCH</h2><h2 id="最小生成树聚类-MST"><a href="#最小生成树聚类-MST" class="headerlink" title="最小生成树聚类(MST)"></a>最小生成树聚类(MST)</h2><p>birch算法（平衡迭代规约和聚类），cure算法（代表点聚类），chameleon算法（动态模型），系统聚类（多层次聚类）</p>
<h1 id="基于密度的方法"><a href="#基于密度的方法" class="headerlink" title="基于密度的方法"></a>基于密度的方法</h1><h2 id="Grid-based-clustering"><a href="#Grid-based-clustering" class="headerlink" title="Grid-based clustering"></a>Grid-based clustering</h2><ul>
<li><p>定义一个网格集（grid set）</p>
</li>
<li><p>将数据点分配到合适的网格cell中，计算每个网格的密度density</p>
</li>
<li><p>淘汰密度小于阈值的网格</p>
</li>
<li><p>从相邻的网格中组成簇</p>
<p>  <img src="/2019/03/08/机器学习-聚类/image-20190816124414229.png" alt="image-20190816124414229"></p>
</li>
</ul>
<h3 id="DENCLUE-Density-Clustering"><a href="#DENCLUE-Density-Clustering" class="headerlink" title="DENCLUE Density Clustering"></a>DENCLUE Density Clustering</h3><p>基于统计学和模式识别领域里的“核密度估计”（Kernel Density Estimate）</p>
<p>每个点对总密度函数的贡献用一个“影响函数”（Influence Function）或“核函数”（Kernel Function）来表示</p>
<p>例如高斯核函数</p>
<script type="math/tex; mode=display">
K(y) = \exp(-dist(x,y)^2/2\sigma^2)</script><p>属性空间中某一点的总密度是与该点相关联的每个点的影响函数之和</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190816133553960.png" alt="image-20190816133553960"></p>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul>
<li>对数据点占据的空间推倒密度函数</li>
<li>识别局部最大点（这些是密度吸引点）</li>
<li>通过沿密度增长最大的方向移动，将每个点关联到一个密度最大点</li>
<li>定义与特定的密度吸引点相关联的点构成的簇</li>
<li>丢弃密度吸引点的密度小于用户指定阈值的簇</li>
<li>合并密度大于等于阈值的点路径连接的簇</li>
</ul>
<h2 id="子空间聚类-Subspace-Clustering"><a href="#子空间聚类-Subspace-Clustering" class="headerlink" title="子空间聚类 Subspace Clustering"></a>子空间聚类 Subspace Clustering</h2><p>只考虑一部分属性，而不是全部</p>
<h3 id="CLIQUE-Clustering-in-quest"><a href="#CLIQUE-Clustering-in-quest" class="headerlink" title="CLIQUE  Clustering in quest"></a>CLIQUE  Clustering in quest</h3><p>是密度聚类和网格聚类的结合</p>
<p>对于之前的网格算法，并不适合检查每一个网格，因为它的个数随着维度的增长而指数级增长</p>
<p>有一个重要的<strong>单调性性质Monotone property</strong>：</p>
<p>如果一组数据点在k维中形成基于密度的簇，那么这一组点也存在于这些维度的所有可能子集中基于密度组成的簇（是一个簇的子集）</p>
<p>和Apriori很像</p>
<h4 id="算法描述-1"><a href="#算法描述-1" class="headerlink" title="算法描述"></a>算法描述</h4><p><img src="/2019/03/08/机器学习-聚类/image-20190816132249423.png" alt="image-20190816132249423"> </p>
<h4 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h4><ul>
<li>时间复杂度是指数级，随着维度的增加而增加<ul>
<li>尤其是当低纬度时产生太多的网格cell/unit</li>
</ul>
</li>
<li>由于阈值时固定的，当簇的密度范围很广时，会影响效果<ul>
<li>选择一个好的阈值和网格长度不容易</li>
</ul>
</li>
</ul>
<p>均值漂移聚类，dbscan算法（基于高密度连接区域），optics算法（对象排序识别）</p>
<h1 id="基于网格的方法"><a href="#基于网格的方法" class="headerlink" title="基于网格的方法"></a>基于网格的方法</h1><p>sting算法（统计信息网络），clioue算法（聚类高维空间），wave-cluster算法（小波变换）</p>
<h1 id="基于原型-模型的方法-Prototype-based"><a href="#基于原型-模型的方法-Prototype-based" class="headerlink" title="基于原型/模型的方法 Prototype-based"></a>基于原型/模型的方法 Prototype-based</h1><p>此类算法假设聚类结构可以通过一组原型刻画，在现实任务中极为常见。（“原型”是指样本空间中具有代表性的点）通常情况下，算法先对原型进行初始化，然后对原型进行迭代更新求解</p>
<h2 id="模糊fuzzy-c-means"><a href="#模糊fuzzy-c-means" class="headerlink" title="模糊fuzzy c-means"></a>模糊fuzzy c-means</h2><h3 id="硬hard聚类和软soft聚类"><a href="#硬hard聚类和软soft聚类" class="headerlink" title="硬hard聚类和软soft聚类"></a>硬hard聚类和软soft聚类</h3><p>软聚类允许一个数据点属于多个簇</p>
<p>比如对于k-means，软聚类的目标函数为</p>
<script type="math/tex; mode=display">
SSE(Error \ of \ Sum \ of \ Squares) = \sum_{j=1}^k \sum_{i=1}^m w_{ij} dist(x_i,c_j)^2\\
其中\sum_{j=1}^kw_{ij} = 1,表示x_i属于某个簇的权值\\
为了最小化SSE，要迭代进行下面两步\\
(1)固定c_j，计算w_{ij}\\
(2)固定w_{ij}，计算c_j\\</script><p><img src="/2019/03/08/机器学习-聚类/image-20190816092159244.png" alt="image-20190816092159244"></p>
<p>对于硬聚类，w等于0或者1</p>
<h3 id="算法描述-2"><a href="#算法描述-2" class="headerlink" title="算法描述"></a>算法描述</h3><p>目标函数</p>
<script type="math/tex; mode=display">
SSE = \sum_{j=1}^k \sum_{i=1}^m w_{ij}^p dist(x_i,c_j)^2\\
其中\sum_{j=1}^kw_{ij} = 1,w_{ij} \in [0,1]\\
p表示fuzzifier(p>1),控制着聚类的模糊fuzzy程度\\</script><p>当p = 2</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190818170636452.png" alt="image-20190818170636452"></p>
<script type="math/tex; mode=display">
初始化：随机初始化w_{ij}\\
不断重复：\\
(1)更新质心：c_j = \frac{\sum_{i=1}^m w_{ij}x_i}{\sum_{i=1}^m w_{ij}}\\
(2)更新权值：w_{ij} = \frac {(1/dist(x_i,c_j)^2)^{\frac{1}{p-1}}}{\sum_{j=1}^k(1/dist(x_i,c_j)^2)^{\frac{1}{p-1}}}\\</script><h2 id="k-means（见上面）"><a href="#k-means（见上面）" class="headerlink" title="k-means（见上面）"></a>k-means（见上面）</h2><h2 id="混合高斯模型（GMM）"><a href="#混合高斯模型（GMM）" class="headerlink" title="混合高斯模型（GMM）"></a>混合高斯模型（GMM）</h2><p>首先假设数据点是呈高斯分布的，相对应K-Means假设数据点是圆形spherical的，高斯分布（椭圆形elliptical）给出了更多的可能性。</p>
<p>有两个参数来描述簇的形状：均值和标准差。所以这些簇可以采取任何形状的椭圆形，因为在x，y方向上都有标准差。因此，每个高斯分布被分配给单个簇。所以要做聚类首先应该找到数据集的均值和标准差，将采用EM算法</p>
<p><a href="https://blog.csdn.net/u014665013/article/details/78970184" target="_blank" rel="noopener">https://blog.csdn.net/u014665013/article/details/78970184</a></p>
<h3 id="硬hard聚类和软soft聚类-1"><a href="#硬hard聚类和软soft聚类-1" class="headerlink" title="硬hard聚类和软soft聚类"></a>硬hard聚类和软soft聚类</h3><p>思想是用混合的分布给数据建立模型，通常用正态(高斯)分布</p>
<p>聚类的过程就是估计统计分布的参数的过程</p>
<p>通常使用EM算法</p>
<p>ex：下图中看起来像是两个正态分布的混合</p>
<p>就需要估计每一个正态分布的均值和方差</p>
<p>计算每个点属于每个分布的概率</p>
<script type="math/tex; mode=display">
prob(x_i;\Theta) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}</script><p><img src="/2019/03/08/机器学习-聚类/image-20190816094725656.png" alt="image-20190816094725656"></p>
<h3 id="多元高斯建模-Multivariate-Gaussian-models"><a href="#多元高斯建模-Multivariate-Gaussian-models" class="headerlink" title="多元高斯建模 Multivariate Gaussian models"></a>多元高斯建模 Multivariate Gaussian models</h3><p>即给多维的变量进行高斯建模</p>
<script type="math/tex; mode=display">
\hat \mu = \frac{1}{N}\sum_i x^{(i)}\\
\hat \Sigma = \frac{1}{N}\sum_i (x^{(i)}-\hat \mu)^T(x^{(i)}-\hat \mu)\\
\mathcal N(x;\mu,\Sigma) = \frac{1}{(2\pi)^{(d/2)}}|\Sigma|^{-1/2}e^{-\frac{1}{2}(x- \mu)^T\Sigma^{-1}(x- \mu)}\\
其中|\Sigma|表示\Sigma的行列式，d表示变量维度</script><p>对于单个簇的高斯建模</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190816103504535.png" alt="image-20190816103504535"></p>
<p>要最大化的目标函数为</p>
<script type="math/tex; mode=display">
p(X) = \prod_ip(x_i)\\
= \prod_i \mathcal N(x_i;\mu,\Sigma)\\
\log p(X) = \sum_i\log \mathcal N(x_i;\mu,\Sigma)</script><h3 id="算法描述-3"><a href="#算法描述-3" class="headerlink" title="算法描述"></a>算法描述</h3><p>使用正态分布对簇建模</p>
<p>在多个正态分布下，要最大化的目标函数为</p>
<script type="math/tex; mode=display">
p(X) = \prod_i p(x_i)\\
\prod_i \sum_c p(c)P(x_i|c)\\
\prod_i \sum_c \pi_c \mathcal N(x_i;\mu_c,\Sigma_c)\\
\log p(X) = \sum_i\log [\sum_c \pi_c \mathcal N(x_i;\mu_c,\Sigma_c)]</script><p>如果我们获得了所有参数（均值，协方差，簇的大小，数据集），可以对数据进行聚类</p>
<script type="math/tex; mode=display">
r_{ic} = p(c|x_i) = \frac{p(c)p(x_i|c)}{p(x_i)}\\

= \frac{\pi_c \mathcal N(x_i;\mu_c,\Sigma_c)}
{\sum_j \pi_{ {c}_j} \mathcal N(x_i;\mu_{ {c}_j},\Sigma_{ {c}_j})}\\

= \frac{\pi_c \mathcal N(x_i;\mu_c,\Sigma_c)}{\sum_{c'} \pi_{c'} \mathcal N(x_i;\mu_{c'},\Sigma_{c'})}\\
然后将x_i划分为c_{j*}\\
c_{j*} = {\arg \max}_{j \in 1,2,...,J} r_{ij}</script><p>使用EM算法优化（关于EM见<code>机器学习-EM</code>）</p>
<ul>
<li><p>初始化簇，设置初始参数</p>
</li>
<li><p>E步</p>
<script type="math/tex; mode=display">
  对于每一个数据，每一个簇c\\
  计算r_{ic}</script></li>
<li><p>M步</p>
<script type="math/tex; mode=display">
  对于每一个簇c，更新它的参数：\\
  \pi_c = \frac{\sum_i r_{ic}}{N}\\
  \mu_c = \frac{\sum_i r_{ic}x_i}{\sum_i r_{ic}}\\
  \Sigma_c = \frac{\sum_i r_{ic}(x_i-\mu_c)^T(x_i-\mu_c)}{\sum_i r_{ic}}</script></li>
<li><p>可以证明一次迭代都在增加log p(X)</p>
</li>
<li><p>一直迭代直到收敛，可以证明一定能保证收敛</p>
</li>
</ul>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul>
<li><p>EM的问题见<code>机器学习-EM</code></p>
</li>
<li><p>高斯分布的参数复杂度为O(d^2)，主要是和协方差矩阵相关，而k-means只有O(d)</p>
</li>
</ul>
<h3 id="其他算法"><a href="#其他算法" class="headerlink" title="其他算法"></a>其他算法</h3><ul>
<li><p>Method of moments / Spectral methods</p>
<ul>
<li><a href="https://sites.google.com/site/momentsicml2014/bibliography" target="_blank" rel="noopener">https://sites.google.com/site/momentsicml2014/bibliography</a></li>
</ul>
</li>
<li><p>马尔可夫链蒙特卡洛方法Markov chain Monte Carlo (MCMC)</p>
</li>
</ul>
<h2 id="自组织网络-SOM-Self-Organizing-Maps"><a href="#自组织网络-SOM-Self-Organizing-Maps" class="headerlink" title="自组织网络 SOM Self-Organizing Maps"></a>自组织网络 SOM Self-Organizing Maps</h2><p><img src="/2019/03/08/机器学习-聚类/image-20190816112217966.png" alt="image-20190816112217966"></p>
<p>一种SOM是Kohonen Network</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190816112246905.png" alt="image-20190816112246905"></p>
<p>和k-means相同的是簇的数量在一开始就决定了，不同的是还指定了簇之间的空间关系</p>
<h3 id="算法描述-4"><a href="#算法描述-4" class="headerlink" title="算法描述"></a>算法描述</h3><ul>
<li><p>初始化initialization</p>
<p>  所有的连接权值初始化为一个小的随机数</p>
</li>
<li><p>竞争competition/matching</p>
<p>  对于每种输入模式，神经元neurons计算它们各自的判别函数值discriminate function，为竞争提供基础。具有最小判别函数值的特定神经元被宣布为胜利者winner</p>
</li>
<li><p>合作cooperation</p>
<p>  获胜的神经元决定了兴奋神经元拓扑邻域的空间位置，从而为相邻神经元之间的合作提供了基础</p>
</li>
<li><p>适应adaptation</p>
<p>  受激神经元通过适当调整相关的连接权重，减少与输入模式相关的判别函数值，使得获胜的神经元对相似输入模式的后续应用的响应增强</p>
</li>
</ul>
<h4 id="竞争过程"><a href="#竞争过程" class="headerlink" title="竞争过程"></a>竞争过程</h4><script type="math/tex; mode=display">
如果输入空间是𝐷维（即有𝐷个输入单元），我们可以把输入模式写x=\{x_i: i=1,...,𝐷\}\\输入单元𝑖和神经元𝑗之间在计算层的连接权重可以写成W_j=\{w_{ji}:j=1,...,N;i=1,...,𝐷\}，其中𝑁是神经元的总数\\
然后，我们可以将我们的判别函数定义为输入向量x和每个神经元j的权向量W_j之间的平方欧氏距离\\
d_j(x) = \sum_{i=1}^D (x_i-w_{ji})^2\\
换句话说，权重向量最接近输入向量（即与其最相似）的神经元被宣告为胜利者\\这样，连续的输入空间可以通过神经元之间的一个简单的竞争过程被映射到神经元的离散输出空间</script><h4 id="合作过程"><a href="#合作过程" class="headerlink" title="合作过程"></a>合作过程</h4><p>在神经生物学研究中，我们发现在一组兴奋神经元内存在<strong>横向的相互作用</strong>。当一个神经元被激活时，最近的邻居节点往往比那些远离的邻居节点更兴奋。并且存在一个随距离衰减的<strong>拓扑邻域</strong>（也会随时间衰减）</p>
<p>我们想为我们的SOM中的神经元定义一个类似的拓扑邻域</p>
<script type="math/tex; mode=display">
如果S_{ij}是神经元网格上神经元i和j之间的横向距离，我们取\\
T_{j,I(x)} = e^{-\frac{S_{j,I(x)}^2}{2\sigma^2}} = \exp(-\frac{S_{j,I(x)}^2}{2\sigma^2})\\
作为我们的拓扑邻域，其中I(X)是获胜神经元的索引\\
该函数有几个重要的特性：它在获胜的神经元中是最大的，且关于该神经元对称\\
当距离达到无穷大时，它单调地衰减到零，它是平移不变的（即不依赖于获胜的神经元的位置）\\
SOM的一个特点是\sigma需要随着时间的推移而减少。常见的时间依赖性关系是指数型衰减：\sigma(t)=\sigma_0\exp(−t/\tau_\sigma)</script><h4 id="适应过程"><a href="#适应过程" class="headerlink" title="适应过程"></a>适应过程</h4><p>显然，SOM必须涉及某种自适应或学习过程，通过这个过程，输出节点自组织，形成输入和输出之间的<strong>特征映射</strong></p>
<p>地形邻域topographic neighborhood的一点是，不仅获胜的神经元能够得到权重更新，它的邻居也将更新它们的权重，尽管不如获胜神经元更新的幅度大。在实践中，适当的权重更新方式是</p>
<script type="math/tex; mode=display">
\Delta w_{ji} = \eta(t) T_{j,I(x)}(t) (x_i-w_{ji})\\
其中我们有一个依赖于时间的学习率\eta(t) = \eta_0\exp(−t/\tau_\eta),该更新适用于在多轮迭代中的所有训练模式x\\
每个学习权重更新的效果是将获胜的神经元及其邻居的权向量W_i向输入向量x移动。对该过程的迭代进行会使得网络的拓扑有序</script><p><img src="/2019/03/08/机器学习-聚类/image-20190816123721853.png" alt="image-20190816123721853"></p>
<h3 id="排序和收敛"><a href="#排序和收敛" class="headerlink" title="排序和收敛"></a>排序和收敛</h3><p>如果正确选择参数（𝜎_0,𝜏_𝜎,𝜂_0,𝜏_𝜂），我们可以从完全无序的初始状态开始，并且SOM算法将逐步使得从输入空间得到的激活模式表示有序化。（但是，可能最终处于特征映射具有拓扑缺陷的<strong>亚稳态</strong>）</p>
<p>这个自适应过程有两个显著的阶段：</p>
<ul>
<li><strong>排序或自组织阶段</strong>：在这期间，权重向量进行拓扑排序。通常这将需要多达1000次的SOM算法迭代，并且需要仔细考虑邻域和学习速率参数的选择</li>
<li><strong>收敛阶段</strong>：在此期间特征映射被微调（fine tune），并提供输入空间的精确统计量化。通常这个阶段的迭代次数至少是网络中神经元数量的500倍，而且参数必须仔细选择</li>
</ul>
<h3 id="可视化例子"><a href="#可视化例子" class="headerlink" title="可视化例子"></a>可视化例子</h3><p>假设我们在连续的二维输入空间中有四个数据点（叉），并且希望将其映射到离散一维输出空间中的四个点上。输出节点映射到输入空间中的点（圆圈）。随机初始化权重使得圆圈的起始位置落在随机落在输入空间的中心</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190816122918909.png" alt="image-20190816122918909"></p>
<p>我们随机选择一个数据点进行训练（带圈的叉）。最接近的输出点表示获胜的神经元（方块）。获胜的神经元向数据点移动一定量，并且两个相邻的神经元以较小的量移动（箭头指示方向）</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190816123028119.png" alt="image-20190816123028119"></p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190816123056818.png" alt="image-20190816123056818"></p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190816123148217.png" alt="image-20190816123148217"></p>
<p>最终整个输出网格将自身重新组织以表征输入空间</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190816123354152.png" alt="image-20190816123354152"></p>
<p>一个自组织映射训练的例证。蓝色斑点是训练数据的分布，而小白色斑点是从该分布中抽取得到的当前训练数据。首先（左图）SOM节点被任意地定位在数据空间中。我们选择最接近训练数据的节点作为获胜节点（用黄色突出显示）。它被移向训练数据，包括（在较小的范围内）其网格上的相邻节点。经过多次迭代后，网格趋于接近数据分布</p>
<h3 id="问题-issue"><a href="#问题-issue" class="headerlink" title="问题 issue"></a>问题 issue</h3><ul>
<li>计算复杂度较高</li>
<li>局部最优</li>
<li>空间关系（grid）的决定有些随意</li>
</ul>
<h1 id="基于图的聚类-Graph-Based-Clustering"><a href="#基于图的聚类-Graph-Based-Clustering" class="headerlink" title="基于图的聚类 Graph-Based Clustering"></a>基于图的聚类 Graph-Based Clustering</h1><p>图聚类算法使用邻近图 (proximity graph)</p>
<ul>
<li>从一个邻近图开始，将每一个数据点视为图中的节点，图中的每条边表示两个节点的邻近程度</li>
<li>最初邻近图是全连接的</li>
<li>单链和全链（见层次聚类）可以以图的形式查看</li>
<li>一个簇就是图中的一个连通部分</li>
</ul>
<h2 id="CURE算法"><a href="#CURE算法" class="headerlink" title="CURE算法"></a>CURE算法</h2><p>层次聚类算法的表现会因为距离的计算方式而不同（max，min），CURE试图解决它们的问题，可以处理大型数据、离群点和具有非球形大小和非均匀大小的簇的数据</p>
<p>对于一个簇，选择其中的一部分数据点来作为该簇的代表</p>
<ul>
<li>第一个点为距离簇中心最远的点</li>
<li>之后的点选择距离当前选择的点集最远的点</li>
</ul>
<p>然后将这些选取到的点根据参数α （0≤α≤1）向该簇的质心收缩Shrink，距离质心越远的点（例如离群点）的收缩程度越大，因此CURE对离群点是不太敏感的，这种方法可以有效的降低离群点和噪声带来的不利影响</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190816140740743.png" alt="image-20190816140740743"></p>
<p>在得到上述缩减后的代表点后，两个簇之间的距离就可以定义为这两个簇中距离最近的两个代表点之间的距离</p>
<p>当α =0，就是基于质心的聚类centroid-based</p>
<p>当α =1，就喝单链聚类很像</p>
<h3 id="讨论-1"><a href="#讨论-1" class="headerlink" title="讨论"></a>讨论</h3><p>CURE处理大数据（大量，多维）的能力更强</p>
<h2 id="Chameleon"><a href="#Chameleon" class="headerlink" title="Chameleon"></a>Chameleon</h2><p>使用稀疏化的近邻图</p>
<p>将数据划分为真正的簇（ground true）的子簇</p>
<p>在保留簇特征的基础上合并</p>
<h3 id="稀疏化"><a href="#稀疏化" class="headerlink" title="稀疏化"></a>稀疏化</h3><ul>
<li>在近邻矩阵中的稀疏化能减少99%的数据<ul>
<li>处理时间急剧下降</li>
<li>适合处理大规模数据</li>
</ul>
</li>
<li>可能会使聚类效果更好<ul>
<li>稀疏化保留了最相似邻居的连接，切断了不太相似的</li>
<li>最近距离的邻居更倾向于属于同一类</li>
<li>减小了异常值和噪声的影响</li>
</ul>
</li>
<li>稀疏化促进了图分割算法<ul>
<li>比如chameleon，Hypergraph-based Clustering</li>
</ul>
</li>
</ul>
<p>稀疏化示例图</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190816143720499.png" alt="image-20190816143720499"></p>
<h3 id="合并方案-Merging-Schemes"><a href="#合并方案-Merging-Schemes" class="headerlink" title="合并方案 Merging Schemes"></a>合并方案 Merging Schemes</h3><p>目前层次聚类的合并方案的问题</p>
<p>它们是固定的，比如MIN，CURE，GROUP-AVERAGE</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190816144709727.png" alt="image-20190816144709727"></p>
<p>在Chameleon中，是根据数据集特点，自适应地发现自然簇</p>
<p>使用动态模型来度量簇之间的相似度</p>
<p>特征主要是指簇里面的相对接近度relative closeness以及相对互联度relative interconnectivity</p>
<p>如果合并的新簇共享了子簇的某些特征，那么可以合并起来</p>
<p>合并方案保持了自相似性self-similarity</p>
<ul>
<li><p>相对互联性relative interconnectivity（RI）</p>
<p>  相对互联度是被簇的内部互连度规范化的两个簇的绝对互连度。如果结果簇中的点之间的连接几乎与原来的每个簇一样强，两个簇合并，数学表述为</p>
<script type="math/tex; mode=display">
  RI = \frac{EC(C_i,C_j)}{\frac{1}{2}(EC(C_i)+EC(c_j))}\\
  其中EC(C_i,C_j)表示连接簇C_i和C_j（k-最近邻图的）的边之和\\
  EC(C)表示二分簇C时割边最小和</script></li>
<li><p>相对接近度relative closeness（RC）</p>
<p>  相对接近度是被簇的内部接近度规范化的两个簇的绝对接近度。两个簇合并，仅当结果簇中的点之间的接近程度几乎与原来的每个簇一样，数学表述为</p>
<script type="math/tex; mode=display">
  RC(C_i,C_j) = \frac{\bar S_{EC}(C_i,C_j)}{\frac{m_i}{m_i+m_j}S_{EC}(C_i)+\frac{m_j}{m_i+m_j}S_{EC}(C_j)}\\
  其中m_i和m_j分别是C_i和C_j的大小\\
  \bar S_{EC}(C_i,C_j)是连接簇C_i和C_j的（k-最近邻图的）边的平均权值\\
  S_{EC}(C)表示二分簇C时的边的平均权值\\
  EC表示割边</script></li>
</ul>
<h3 id="算法描述-5"><a href="#算法描述-5" class="headerlink" title="算法描述"></a>算法描述</h3><ul>
<li><p>预处理：</p>
<p>  将数据以图的形式表示，构建k最近邻图k-nearestneighbor (k-NN) graph，去捕捉数据点和它最近的k个邻居的关系</p>
</li>
<li><p>阶段1</p>
<p>  使用多层次图的分割算法去找到内部连接良好的簇（每一个簇最好是“真”簇的子集，或大部分点属于某一个“真”簇）</p>
</li>
<li><p>阶段2</p>
<p>  使用层次凝聚聚类Hierarchical Agglomerative Clustering将子簇合并</p>
<p>  如果合并的新簇共享了子簇的某些特征（RI，RC），那么可以合并起来</p>
</li>
</ul>
<h2 id="SNN-Shared-Nearest-Neighbor-graph"><a href="#SNN-Shared-Nearest-Neighbor-graph" class="headerlink" title="SNN Shared Nearest Neighbor graph"></a>SNN Shared Nearest Neighbor graph</h2><p>是一个将图聚类和密度聚类（比如DBSCAN-like）结合的算法</p>
<p>SNN密度测量一个点周围是否有相似的点</p>
<p>在SNN图中，两点vertices之间边的权重取等于他们的共有邻居</p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190816170446342.png" alt="image-20190816170446342"></p>
<p><img src="/2019/03/08/机器学习-聚类/image-20190816170527446.png" alt="image-20190816170527446"></p>
<p>对于稀疏图，边的权重表示两个数据点的相似度，SNN中表示共有邻居数量</p>
<h3 id="算法描述-6"><a href="#算法描述-6" class="headerlink" title="算法描述"></a>算法描述</h3><p>1.计算相似矩阵</p>
<p>这对应于一个相似度图 similarity matrix,节点是数据点，边的权重是数据点之间的相似度</p>
<p>2.通过只保留k个最相似的邻局来稀疏相似矩阵</p>
<p>这对应于保持相似度图的k个权重最大的连接</p>
<p>3.利用稀疏相似矩阵构造共有最近邻图</p>
<p>在这一步，我们可以应用相似度阈值并找到连接的组件来获得簇（jarvis-patrick算法思路）</p>
<p>4-8步时DBSCAN</p>
<p>4.找出每个点的SNN密度</p>
<p>使用用户指定的参数Eps，查找与每个点相似度大于等于eps的点数，这就是该点的SNN密度</p>
<p>5.找到核心点</p>
<p>使用用户指定的参数minpts，去查找核心点，即所有snn密度大于minpts的点</p>
<p>6.从核心点形成簇</p>
<p>如果两个核心点在一个“半径”（Eps）内，则它们彼此放置在同一个簇中</p>
<p>7.丢弃所有噪声点</p>
<p>所有不在核心点Eps半径范围内的非核心点都将被丢弃。</p>
<p>8.将所有非噪波、非核心点划分给簇</p>
<p>这可以通过将这些点指定给最近的核心点来实现</p>
<h3 id="Jarvis-Patrick聚类"><a href="#Jarvis-Patrick聚类" class="headerlink" title="Jarvis-Patrick聚类"></a>Jarvis-Patrick聚类</h3><ul>
<li>首先找到每个数据点最近的k个邻居</li>
<li>然后进行聚类，两个点归为一个簇当且仅当<ul>
<li>它们的共有邻居超过T个</li>
<li>它们都出现在对方的k近邻中</li>
</ul>
</li>
</ul>
<h3 id="讨论-2"><a href="#讨论-2" class="headerlink" title="讨论"></a>讨论</h3><p>不足</p>
<ul>
<li><p>没有将所有点都聚类</p>
</li>
<li><p>复杂度很高</p>
<p>  O(n * 在Eps参数下找到邻居的时间)</p>
<p>  最坏情况下，复杂度为O(n^2)</p>
<p>  如果是低维空间，有一些高效的办法</p>
<ul>
<li>R*树</li>
<li>k-d树</li>
</ul>
</li>
</ul>
<h2 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h2><p>图团体检测(Graph Community Detection) </p>
<p>当我们的数据可以被表示为网络或图是，可以使用图团体检测方法完成聚类</p>
<h1 id="数据相似度度量"><a href="#数据相似度度量" class="headerlink" title="数据相似度度量"></a>数据相似度度量</h1><p>（欧式距离，曼哈顿距离，闵可夫距离，文档数据相似度，余弦相似度）</p>
<h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><p>purity评价法，RI评价法，F值评价法</p>
<h1 id="讨论-3"><a href="#讨论-3" class="headerlink" title="讨论"></a>讨论</h1><h2 id="数据，簇，聚类算法的特征"><a href="#数据，簇，聚类算法的特征" class="headerlink" title="数据，簇，聚类算法的特征"></a>数据，簇，聚类算法的特征</h2><p>聚类分析受到这些元素特征的影响：</p>
<ul>
<li>数据</li>
<li>簇</li>
<li>聚类算法</li>
</ul>
<h3 id="数据特征"><a href="#数据特征" class="headerlink" title="数据特征"></a>数据特征</h3><ul>
<li>维度</li>
<li>数据集大小</li>
<li>属性值的稀疏性</li>
<li>噪声和异常值</li>
<li>属性类型和数据集类型</li>
<li>属性尺度的差异</li>
<li>数据空间属性</li>
</ul>
<h3 id="簇特征"><a href="#簇特征" class="headerlink" title="簇特征"></a>簇特征</h3><ul>
<li><p>数据分布</p>
</li>
<li><p>形状</p>
</li>
<li><p>不同尺寸</p>
</li>
<li><p>不同密度</p>
</li>
<li><p>分离不良</p>
</li>
<li><p>簇之间的关系</p>
</li>
<li><p>簇类型</p>
<p>  –基于中心、基于连续性、基于密度</p>
</li>
<li><p>子空间簇</p>
</li>
</ul>
<h3 id="聚类算法特征"><a href="#聚类算法特征" class="headerlink" title="聚类算法特征"></a>聚类算法特征</h3><ul>
<li>顺序依赖性</li>
<li>不确定性</li>
<li>参数选择</li>
<li>可扩展性</li>
<li>基于的基础模型</li>
<li>基于的优化方法</li>
</ul>
<h2 id="MIN和EM-聚类的比较"><a href="#MIN和EM-聚类的比较" class="headerlink" title="MIN和EM-聚类的比较"></a>MIN和EM-聚类的比较</h2><p><em>假设EM使用高斯分布</em></p>
<ul>
<li>MIN是层次hierarchical聚类，EM是划分partitional聚类</li>
<li>两个算法都可最终收敛性</li>
<li>Min有一个基于图（基于连续性）的簇概念，而Em集群有一个原型（或基于模型）的簇概念</li>
<li>Min将无法区分分离不良的簇，但Em可以在许多情况下能做到这一点</li>
<li>Min可以发现不同形状和大小的簇；Em簇更偏向球形簇，并且可能会遇到不同大小的簇的问题</li>
<li>Min对于不同密度的簇的处理有困难，而Em通常可以处理这个问题</li>
<li>Min和Em集群都找不到子空间簇</li>
<li>Min可以处理异常值，但噪声容易加入簇；EM可以容忍噪声，但会受到异常值的强烈影响</li>
<li>EM只能应用于质心有意义的数据；MIN只需要有意义的接近度定义</li>
<li>当维数增加时，EM会遇到麻烦，其参数（协方差矩阵中的条目数）随着维数的平方增加而增加；定义好合适邻近度，MIN依然可以很好地工作</li>
<li>EM是为欧几里得数据设计的，尽管已经为其他类型的数据开发了EM聚类的版本。Min使用相似性矩阵来屏蔽数据类型</li>
<li>Min不做分布假设；我们考虑的Em的假设为高斯分布</li>
<li>EM时间复杂度为O(n)（这里应该是O(n * 迭代次数)），MIN为O(n^2 logn)</li>
<li>由于随机初始化，EM可以生成不同的簇；Min始终会生成相同的簇</li>
<li>Min和EM都不能自动确定簇的数量</li>
<li>Min没有任何用户指定的参数；Em有簇的数量，可能还有簇的权重</li>
<li>EM聚类可以看作是一个优化问题；MIN使用数据的图模型</li>
<li>EM或Min均不依赖于数据的顺序</li>
</ul>
<h2 id="DBSCAN和K-means的比较"><a href="#DBSCAN和K-means的比较" class="headerlink" title="DBSCAN和K-means的比较"></a>DBSCAN和K-means的比较</h2><ul>
<li>二者都是分裂性的partitional聚类</li>
<li>k-means能完成；DBSCAN不一定能完成</li>
<li>k-means有基于原型的簇概念；DBSCAN使用基于密度的概念</li>
<li>k-means可以找到没有很好分离的簇。DBSCAN将合并有连接的簇</li>
<li>DBSCAN可以处理不同形状和大小的簇；K-Means更偏向球形簇</li>
<li>DBSCAN可以处理噪声和异常值；K-Means在异常值存在时表现不佳</li>
<li>K-Means只能应用于质心有意义的数据；DBSCAN要求有意义的密度定义</li>
<li>DBSCAN在高维数据上效果较差；KMeans在某些类型的高维数据上效果较好</li>
<li>这两种技术都是为欧几里得数据设计的，但可以扩展到其他类型的数据</li>
<li>DBSCAN不做分布假设；K-means的假设为球面高斯分布spherical Gaussian distributions</li>
<li>k-means具有O(n)时间复杂性；DBSCAN为O（n^2）</li>
<li>由于随机初始化，kmeans发可以生成不同的簇；dbscan总是生成相同的簇</li>
<li>DBSCAN能自动确定簇数；K-means不行</li>
<li>k-means只有一个参数(k)，DBSCAN有两个</li>
<li>k-means聚类可以看作是一个优化问题，也是EM聚类的一个特例；DBSCAN不是基于形式模型的</li>
</ul>
<p>平均移 Mean-shift</p>
<p>OPTICS算法 OPTICS algorithm</p>
<p>概念聚类 Conceptual clustering                                                                               </p>
<p><a href="https://en.wikipedia.org/wiki/Conceptual_clustering" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Conceptual_clustering</a></p>
<p>FCM聚类算法</p>
<p>簇评估</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/机器学习-集成学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/机器学习-集成学习/" itemprop="url">机器学习-集成学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:44:24+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>集成学习(Ensemble Learning)是一种机器学习范式，它试图通过连续调用单个的学习算法，获得不同的基学习器，然后根据规则组合这些学习器来解决同一个问题，可以显著的提高学习系统的泛化能力。组合多个基学习器主要采用(加权)投票的方法，常见的算法有装袋(Bagging) , 提升/推进(Boosting) 等。可应用在分类classification, 聚类clustering, 协同过滤collaborative filtering, 异常检测anomaly detection中</p>
<p>思想是将多个弱分类器组合起来：</p>
<p><img src="/2019/03/08/机器学习-集成学习/image-20190814194548900.png" alt="image-20190814194548900"></p>
<h2 id="弱分类器"><a href="#弱分类器" class="headerlink" title="弱分类器"></a>弱分类器</h2><h3 id="如何得到弱分类器"><a href="#如何得到弱分类器" class="headerlink" title="如何得到弱分类器"></a>如何得到弱分类器</h3><ul>
<li>使用不同的弱学习算法得到不同基本学习器</li>
<li>使用相同的弱学习算法，但用不同的参数</li>
<li>相同输入对象的不同表示凸显事物不同的特征</li>
<li>使用不同的训练集：装袋bagging也称为自举汇聚法boostrap aggregating与提升boosting</li>
</ul>
<h3 id="如何组合弱分类器"><a href="#如何组合弱分类器" class="headerlink" title="如何组合弱分类器"></a>如何组合弱分类器</h3><ul>
<li>多专家组合：一种并行结构，所有的弱分类器都给出各自的预测结果，通过“ 组合器” 把这些预<br>  测结果转换为最终结果。eg.投票（voting）及其变种、混合专家模型</li>
<li>多级组合：一种串行结构，其中下一个分类器只在前一个分类器预测不够准（不够自信）的实例上进行训练或检测。eg.级联算法（cascading）</li>
</ul>
<h2 id="集成work的原因"><a href="#集成work的原因" class="headerlink" title="集成work的原因"></a>集成work的原因</h2><ul>
<li><p>直觉上</p>
<p>  就像人类在决策中会将各种各样的观点结合起来</p>
</li>
<li><p>减少不相关的错误</p>
<p>  <img src="/2019/03/08/机器学习-集成学习/image-20190814195952459.png" alt="image-20190814195952459"></p>
</li>
<li><p>克服单一假设hypothesis的局限性</p>
<p>  目标函数可能不能用单个分类器实现，但可以通过模型平均来近似</p>
<p>  <img src="/2019/03/08/机器学习-集成学习/image-20190814195844353.png" alt="image-20190814195844353"></p>
</li>
</ul>
<h2 id="研究关注点"><a href="#研究关注点" class="headerlink" title="研究关注点"></a>研究关注点</h2><ul>
<li><p>基础模型<br>  提升多样性diversity</p>
</li>
<li><p>组合方案</p>
<p>  一致性Consensus（无监督）</p>
<p>  Learn to combine（有监督）</p>
</li>
<li><p>任务</p>
<p>  分类（有监督或半监督）</p>
<p>  聚类（无监督）</p>
</li>
</ul>
<h2 id="集成学习种类"><a href="#集成学习种类" class="headerlink" title="集成学习种类"></a>集成学习种类</h2><p><img src="/2019/03/08/机器学习-集成学习/image-20190814200601088.png" alt="image-20190814200601088"></p>
<h3 id="分类集成学习-Learn-to-Combine"><a href="#分类集成学习-Learn-to-Combine" class="headerlink" title="分类集成学习-Learn to Combine"></a>分类集成学习-Learn to Combine</h3><p><img src="/2019/03/08/机器学习-集成学习/image-20190814201300907.png" alt="image-20190814201300907"></p>
<p>boosting, stacked generalization, rule ensemble, Bayesian model averaging</p>
<h3 id="分类集成学习-一致性Consensus"><a href="#分类集成学习-一致性Consensus" class="headerlink" title="分类集成学习-一致性Consensus"></a>分类集成学习-一致性Consensus</h3><p><img src="/2019/03/08/机器学习-集成学习/image-20190814201318255.png" alt="image-20190814201318255"></p>
<p>bagging, random forest, random decision tree, model averaging of probabilities</p>
<h3 id="聚类集成学习-一致性Consensus"><a href="#聚类集成学习-一致性Consensus" class="headerlink" title="聚类集成学习-一致性Consensus"></a>聚类集成学习-一致性Consensus</h3><p><img src="/2019/03/08/机器学习-集成学习/image-20190814201419599.png" alt="image-20190814201419599"></p>
<p>direct approach, object-based, cluster-based, objectcluster-based approaches, generative models</p>
<h3 id="半监督集成学习-一Learn-to-Combine"><a href="#半监督集成学习-一Learn-to-Combine" class="headerlink" title="半监督集成学习-一Learn to Combine"></a>半监督集成学习-一Learn to Combine</h3><p><img src="/2019/03/08/机器学习-集成学习/image-20190814201504280.png" alt="image-20190814201504280"></p>
<p>multi-view learning</p>
<h3 id="半监督集成学习-一致性Consensus"><a href="#半监督集成学习-一致性Consensus" class="headerlink" title="半监督集成学习-一致性Consensus"></a>半监督集成学习-一致性Consensus</h3><p><img src="/2019/03/08/机器学习-集成学习/image-20190814201652183.png" alt="image-20190814201652183"></p>
<p>consensus maximization</p>
<h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><p><img src="/2019/03/08/机器学习-集成学习/image-20190814200725610.png" alt="image-20190814200725610"></p>
<h1 id="有监督集成学习"><a href="#有监督集成学习" class="headerlink" title="有监督集成学习"></a>有监督集成学习</h1><script type="math/tex; mode=display">
给定数据集D = \{x_1,x_2,...,x_n\}以及标签L = \{l_1,l_2,...,l_3\}\\
集成算法计算出一系列分类器\{f_1,f_2,...,f_k\}\\
组成成分类器f*减小泛化误差:f^*(x) = w_1f(x_1) + w_2f_2(x)+ ...+w_kf_k(x)</script><p>使用集成的目的是减少variance</p>
<p><img src="/2019/03/08/机器学习-集成学习/image-20190816191229638.png" alt="image-20190816191229638"></p>
<p>产生分类器的方法</p>
<ul>
<li>对训练集采样（比如采样出k个子集，训练k个分类器）</li>
<li>使用不同的学习模型/算法</li>
<li>对特征采样（比如采样出k个特征子集，训练k个分类器）</li>
<li>在学习过程中加入随机因素</li>
</ul>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>将一组弱分类器提升为强分类器，每一步增加了当前分错instance的权重，而对已经对的instance趋向于0，这样后面的树就可以更加关注错分的instance的训练了</p>
<p>boosting在训练的时候会给样本加一个权重，然后使loss function尽量去考虑那些分错类的样本（比如给分错类的样本的权重值加大）</p>
<p><img src="/2019/03/08/机器学习-集成学习/image-20190817151647764.png" alt="image-20190817151647764"></p>
<p>在机器学习中往往是最终要求解某个函数的最优值，但是一般情况下，任意一个函数的最优值求解比较困难，但是对于凸函数来说就可以有效的求解出全局最优值。</p>
<p>学习器之间存在强依赖关系、必须串行生成, 集成方式为加权和;</p>
<p>Adaboost属于Boosting, 采用指数损失函数替代原本分类任务的0/1损失函数;</p>
<p>GBDT属于Boosting的优秀代表, 对函数残差近似值进行梯度下降, 用CART回归树做学习器, 集成为回归模型;</p>
<p>xgboost属于Boosting的集大成者, 对函数残差近似值进行梯度下降, 迭代时利用了二阶梯度信息, 集成模型可分类也可回归. 由于它可在特征粒度上并行计算, 结构风险和工程实现都做了很多优化, 泛化, 性能和扩展性都比GBDT要好</p>
<h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><p>初始时，所有数据设置统一的权重</p>
<p>在每一轮迭代中</p>
<ul>
<li>使用基于当前权重的数据样本</li>
<li>训练一个弱分类器，并在原始的训练集上面测试</li>
<li>分类错误的数据权重会增加，正确的会减少</li>
<li>如果错误率大于0.5，则重新开始</li>
</ul>
<p>最终的预测是所有分类器的加权平均（权值由训练中的准确率决定）</p>
<script type="math/tex; mode=display">
初始时每一条数据权重为\frac{1}{N} \quad N为数据总数\\
对于分类器，它的错误率为\varepsilon_i = \frac{\sum_{j=1}^{N}w_j \delta(C_i(x_j)\ne y_j)}{\sum_{j=1}^{N}w_j} = \sum_{j=1}^{N}w_j \delta(C_i(x_j)\ne y_j)\\
分类器的重要度importance为\alpha_i = \frac{1}{2}\ln (\frac{1-\varepsilon_i}{\varepsilon_i})\\
每条数据的权重更新方式w_j^{(i+1)} = \frac{w_j^{(i)}\exp(-\alpha_iy_jC_i(x_j))}{Z^{(i)}}\\
=  \begin{cases}
\frac{w_j^{(i)}\exp(-\alpha_i)}{Z^{(i)}}& {y_j = C_i(x_j)}\\
\frac{w_j^{(i)}\exp(\alpha_i))}{Z^{(i)}}& {y_j \ne C_i(x_j)}
\end{cases}\\
Z^{(i)} 是规范化因子，用于使所有数据权值和为1\\
Z^{(i)}  = \sum_{j=1}^N w_j^{(i)}\exp(-\alpha_iy_jC_i(x_j))\\
最终预测C^*(x) = {\arg \max}_y \sum_{i=1}^K \alpha_i \delta(C_i(x)=y)\\</script><h4 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h4><p>优点：</p>
<ul>
<li>容易理解、实现简单 </li>
<li>易编码 </li>
<li>分类精度高</li>
<li>可以使用各种回归模型构建基分类器，非常灵活</li>
<li>作为二元分类器是，构造简单、结果可理解、参数少</li>
<li>相对来说，不易过拟合</li>
</ul>
<p>缺点：</p>
<ul>
<li>只能<strong>串行</strong> </li>
<li>对<strong>异常值敏感</strong>（ boosting对异常值都敏感）</li>
</ul>
<h3 id="GBDT-Gradient-Boosting-Decision-Tree"><a href="#GBDT-Gradient-Boosting-Decision-Tree" class="headerlink" title="GBDT Gradient Boosting Decision Tree"></a>GBDT Gradient Boosting Decision Tree</h3><p>这里的GBDT泛指所有梯度提升树算法，包括XGBoost，它也是GBDT的一种变种</p>
<script type="math/tex; mode=display">
从Gradient \ Descend 到 Gradient \ Boosting\\
参数空间|函数空间</script><h4 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h4><p>这里的GBDT特指“ Greedy Function Approximation： A Gradient Boosting Machine” 里提出的算法，它只用了一阶导数信息</p>
<p>GBDT的精髓在于训练的时候都是以上一颗树的残差为目标，这个残差就是上一个树的预测值与真实值的差值</p>
<p>和随机森林的区别</p>
<p>区别：</p>
<p>（1）取样方式</p>
<p>（2）预测时，RF多数投票，GBDT加权累加</p>
<p>（3）样本的关系—&gt;并行和串行</p>
<p>（4）学习器的种类，<strong>GBDT只能用CART回归树</strong> (因为要计算连续梯度)</p>
<p>（5）对异常值的敏感性</p>
<p>（6）通过减少方差/偏差提高性能</p>
<h4 id="GBRT"><a href="#GBRT" class="headerlink" title="GBRT"></a>GBRT</h4><h4 id="XGBOOST"><a href="#XGBOOST" class="headerlink" title="XGBOOST"></a>XGBOOST</h4><h4 id="Catboost"><a href="#Catboost" class="headerlink" title="Catboost"></a>Catboost</h4><h4 id="Gradient-boosting-machine-GBM"><a href="#Gradient-boosting-machine-GBM" class="headerlink" title="Gradient boosting machine(GBM)"></a>Gradient boosting machine(GBM)</h4><h4 id="lightGBM"><a href="#lightGBM" class="headerlink" title="lightGBM"></a>lightGBM</h4><h4 id="正则化贪心森林-Regularized-Greedy-Forest-RGF"><a href="#正则化贪心森林-Regularized-Greedy-Forest-RGF" class="headerlink" title="正则化贪心森林 Regularized Greedy Forest(RGF)"></a>正则化贪心森林 Regularized Greedy Forest(RGF)</h4><h2 id="Bootstrap-aggregating-Bagging"><a href="#Bootstrap-aggregating-Bagging" class="headerlink" title="Bootstrap aggregating (Bagging)"></a>Bootstrap aggregating (Bagging)</h2><ol>
<li><p>从N样本中有放回的采样N个样本（大约包含总数据的63.2%）</p>
</li>
<li><p>对这N个样本在全属性上建立分类器(CART,SVM)</p>
</li>
<li><p>重复上面的步骤，建立m个分类器</p>
</li>
<li><p>预测的时候Aggregation使用投票的方法得到结果</p>
</li>
</ol>
<p>学习器间不存在强依赖关系, 学习器可并行训练生成, 集成方式一般为投票</p>
<p>在以均方误差作为误差的考虑下，bagging可以减少方差variance，让偏差不变</p>
<script type="math/tex; mode=display">
在理想的情况下，总分类器的输出是弱分类器的输出的期望\\
\bar f(x) = E(\hat f_z(x))
误差为\\
E[(Y-\hat f_z(x))^2] = E[(Y-\bar f(x)+\bar f(x)-\hat f_z(x))^2]\\
= E[(Y-\bar f(x))^2] + E[(\bar f(x)-\bar f_z(x))^2] \ge E[(Y-\bar f(x))^2]</script><h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>Random Forest属于Bagging的代表, 放回抽样, 每个学习器随机选择部分特征去优化</p>
<p>随机森林是有很多随机得决策树构成，它们之间没有关联。得到RF以后，在预测时分别对每一个决策树进行判断，最后使用Bagging的思想进行结果的输出（也就是投票的思想）</p>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><p>现在有N个训练样本，每个样本的特征为M个，需要建T颗树<br>对于每一棵树：</p>
<ul>
<li>从N个训练样本中有放回的取N个样本作为一组训练集（其余未取到的样本作为预测分类，评估其误差）</li>
<li>从M个特征中取m个特征左右子集特征(m&lt;&lt;M)（通常m=sqrt(M)或者20%）</li>
<li>对采样的数据使用完全分裂的方式来建立决策树</li>
</ul>
<p>建立森林后，预测时使用投票的方法</p>
<p>使用oob（out-of-bag）进行泛化误差的估计，将各个树的未采样样本作为预测样本（大约有36.8%），使用已经建立好的森林对各个预测样本进行预测，预测完之后最后统计误分得个数占总预测样本的比率作为RF的oob误分率</p>
<h4 id="讨论-1"><a href="#讨论-1" class="headerlink" title="讨论"></a>讨论</h4><ul>
<li>随机性体现在：bagging和特征上的随机选择</li>
<li><p>通过增加多样性diversity，减小方差variances来增加准确率</p>
</li>
<li><p>能够处理大量特征的分类，并且还不用做特征选择</p>
</li>
<li><p>在训练完成之后能给出哪些feature的比较重要</p>
</li>
<li><p>训练速度很快，很容易并行</p>
</li>
<li><p>实现相对来说较为简单</p>
</li>
</ul>
<h4 id="深度森林"><a href="#深度森林" class="headerlink" title="深度森林"></a>深度森林</h4><h3 id="随机决策树"><a href="#随机决策树" class="headerlink" title="随机决策树"></a>随机决策树</h3><p><a href="https://www.linuxidc.com/Linux/2016-12/138376.htm" target="_blank" rel="noopener">https://www.linuxidc.com/Linux/2016-12/138376.htm</a></p>
<ul>
<li>在每一个节点上，随机选择一个un-used特征<ul>
<li>如果是离散特征，如果从根root到当前节点的路径上还没有使用过，就是un-used的</li>
<li>连续型特征可以使用多次，不过每次的阈值不一样</li>
</ul>
</li>
<li>停止条件<ul>
<li>一个节点的数据很少（比如小于等于3个）</li>
<li>树的高度超过某个阈值（比如特征数）</li>
</ul>
</li>
</ul>
<p><img src="/2019/03/08/机器学习-集成学习/image-20190817153748937.png" alt="image-20190817153748937"></p>
<h3 id="extraTree"><a href="#extraTree" class="headerlink" title="extraTree"></a>extraTree</h3><h1 id="无监督集成"><a href="#无监督集成" class="headerlink" title="无监督集成"></a>无监督集成</h1><h2 id="聚类集成"><a href="#聚类集成" class="headerlink" title="聚类集成"></a>聚类集成</h2><script type="math/tex; mode=display">
给定没有标签的数据集D = \{x_1,x_2,...,x_n\}\\\
集成算法计算出一系列聚类方案\{C_1,C_2,...,C_k\},将数据映射为一个簇f_j(x)=m\\
一个统一的聚类方案f*结合consensus了基本的聚类解决方案\\</script><p>目的是将弱聚类组合成一个</p>
<p><img src="/2019/03/08/机器学习-集成学习/image-20190817160257099.png" alt="image-20190817160257099"></p>
<p><img src="/2019/03/08/机器学习-集成学习/image-20190817160334714.png" alt="image-20190817160334714"></p>
<p>挑战：</p>
<ul>
<li>不同方案中的簇之间的对应关系很难找</li>
<li>组合优化问题是np完全的</li>
</ul>
<p>如何得到基本模型：</p>
<ul>
<li>自助Bootstrap采样</li>
<li>不同的特征子集</li>
<li>不同的聚类算法</li>
<li>随机的簇数量</li>
<li>k-maens的随机初始化</li>
<li>将随机噪声加入到簇标签中</li>
<li>对于在线算法online（比如BIRCH），改变数据的顺序</li>
</ul>
<p>如何组合模型：</p>
<p>Correspondence：对应</p>
<p><img src="/2019/03/08/机器学习-集成学习/image-20190817160643741.png" alt="image-20190817160643741"></p>
<h3 id="对应-Correspondence"><a href="#对应-Correspondence" class="headerlink" title="对应 Correspondence"></a>对应 Correspondence</h3><h4 id="硬对应-hard-Correspondence"><a href="#硬对应-hard-Correspondence" class="headerlink" title="硬对应 hard Correspondence"></a>硬对应 hard Correspondence</h4><p>使用Re-labeling+voting</p>
<p>找到划分结果中标签之间的对应关系，并通过投票将具有相同标签的簇融合在一起</p>
<p><img src="/2019/03/08/机器学习-集成学习/image-20190817161017684.png" alt="image-20190817161017684"></p>
<p>具体而言</p>
<ul>
<li>使用匈牙利法在两个不同的聚类解决方案中匹配簇</li>
<li>以成对方式匹配簇</li>
</ul>
<p>困难</p>
<ul>
<li>在很多情况下，簇并没有一对一的对应关系</li>
</ul>
<h4 id="软对应-soft-Correspondence"><a href="#软对应-soft-Correspondence" class="headerlink" title="软对应 soft Correspondence"></a>软对应 soft Correspondence</h4><h1 id="半监督集成"><a href="#半监督集成" class="headerlink" title="半监督集成"></a>半监督集成</h1><h2 id="多视角学习"><a href="#多视角学习" class="headerlink" title="多视角学习"></a>多视角学习</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Chenxr</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">92</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chenxr</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
