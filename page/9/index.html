<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="Chenxr&#39;s blogs">
<meta property="og:url" content="http://yoursite.com/page/9/index.html">
<meta property="og:site_name" content="Chenxr&#39;s blogs">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chenxr&#39;s blogs">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/9/">





  <title>Chenxr's blogs</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chenxr's blogs</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/机器学习-概念:总结篇/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/机器学习-概念:总结篇/" itemprop="url">机器学习-概念/总结篇</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:35:57+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<p>分类回归的区别<br>
定量输出称为回归，或者说是连续变量预测；</p>
<p>定性输出称为分类，或者说是离散变量预测。</p>
<p>生成模型和判别模型的区别<br>
判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。</p>
<p>生成方法：由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。</p>
<p>由生成模型可以得到判别模型，但由判别模型得不到生成模型。</p>
<p>常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场</p>
<p>常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机</p>
<p>有监督机器学习方法可以分为生成方法和判别方法（常见的生成方法有混合高斯模型、朴素贝叶斯法和隐形马尔科夫模型等，常见的判别方法有SVM、LR等），生成方法学习出的是生成模型，判别方法学习出的是判别模型。</p>
<p>监督学习，预测时，一般都是在求p(Y|X)生成模型： 从数据中学习联合概率分布p(X,Y)，然后利用贝叶斯公式求：</p>
<p>判别模型：直接学习P(Y|X)， 它直观输入什么特征X，就直接预测出最可能的Y; 典型的模型包括：LR, SVM,CRF,Boosting,Decision tree…</p>
<p>生成方法的特点：生成方法可以还原联合概率分布，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学习的模型可以更快的收敛于真实的模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。</p>
<p>判别方法的特点：判别方法直接学习的是条件概率或者决策函数，直接面对预测，往往学习的准确率更高；由于直接学习或者，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</p>
<p>精确率（Precision）为TP/(TP+FP)</p>
<p>召回率（Recall）为TP/(TP+FN)</p>
<p>F1值是精确率和召回率的调和均值，即F1=2PR/(P+R）</p>
<p>所以AUC表征的是模型的分类能力。</p>
<p>线性分类器和非线性分类器<br>
常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归<br>
常见的非线性分类器：决策树、RF、GBDT、多层感知机<br>
SVM两种都有(看线性核还是高斯核)</p>
<p>线性分类器速度快、编程方便，但是可能拟合效果不会很好<br>
非线性分类器编程复杂，但是效果拟合能力强</p>
<p>特征比数据量还大时，选择什么样的分类器？<br>
线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分<br>
对于维度很高的特征，你是选择线性还是非线性分类器？<br>
理由同上<br>
对于维度极低的特征，你是选择线性还是非线性分类器？</p>
<p>非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分</p>
<h1>模型分类</h1>
<h2 id="判别模型-discriminative-classifier"><a class="header-anchor" href="#判别模型-discriminative-classifier">¶</a>判别模型 Discriminative classifier</h2>
<p>SVM，深度学习，线性判别分析，线性回归，逻辑回归<br>
$$<br>
假设存在P(Y|X)这样的函数形式\<br>
直接从训练数据中估计出P(Y|X)\<br>
不能对数据的分布进行估计(采样)，因为不能计算P(X) = \sum_y P(y)P(X|y)\<br>
$$</p>
<h2 id="产生式模型-generative-classifier"><a class="header-anchor" href="#产生式模型-generative-classifier">¶</a>产生式模型 Generative classifier</h2>
<p>高斯，贝叶斯，混合多项式，混合高斯模型，混合专家系统，隐马尔可夫模型，马尔可夫随机场<br>
$$<br>
假设存在P(X|Y),P(Y)这样的函数形式\<br>
从训练数据中估计出P(X|Y),P(Y)，然后使用贝叶斯定理计算P(Y|X=x)\<br>
通过贝叶斯定理间接的计算P(Y|X=x)\<br>
可以对数据的分布进行估计(采样)P(X) = \sum_y P(y)P(X|y)\<br>
$$</p>
<h2 id="分类模型-回归模型"><a class="header-anchor" href="#分类模型-回归模型">¶</a>分类模型/回归模型</h2>
<h1>模型比较</h1>
<p><img src="/2019/03/08/机器学习-概念:总结篇/1.png" alt="1"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/机器学习-概率图模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/机器学习-概率图模型/" itemprop="url">机器学习-概率图模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:35:57+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在概率模型中，利用已知变量推测未知变量的分布成为“推断”（inference），其核心是如何基于可观测变量推测出未知变量的条件分布</p>
<p>概率图模型主要分为两类</p>
<ul>
<li>使用DAG表示表示变量间的依赖关系，称为有向图模型或贝叶斯网</li>
<li>使用无向图表示变量间的相关关系，称为无向图模型或者马尔科夫网</li>
</ul>
<p>马尔可夫逻辑网络</p>
<p>马尔可夫决策</p>
<p><a href="http://www.cnblogs.com/jinxulin/p/3517377.html" target="_blank" rel="noopener">http://www.cnblogs.com/jinxulin/p/3517377.html</a></p>
<p><a href="http://blog.csdn.net/zz_1215/article/details/44138843" target="_blank" rel="noopener">http://blog.csdn.net/zz_1215/article/details/44138843</a></p>
<h1>隐马尔可夫模型HMM</h1>
<p>是结构最简单的动态贝叶斯网（dynamic bayesian network）</p>
<p>两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关(一阶马尔可夫模型)</p>
<p><img src="/2019/03/08/机器学习-概率图模型/1.png" alt="1"></p>
<p>HMM有两组变量，上面的Y成为状态变量或者隐变量（hidden variable），它的取值范围通常是N个可能取值的离散空间；下面的X称为观测变量，可以是离散型，也可以是连续型</p>
<p>马尔可夫链（markov chain）：系统下一时刻的状态仅由当前的状态决定，不依赖于以往的任何状态。因此所有变量的概率分布为<br>
$$<br>
P(x_1,y_1,…,x_n,y_n) = P(y_1)P(x_1|y_1)\prod_{i=2}^{n}P(y_i|y_{i-1})P(x_i|y_i)<br>
$$<br>
除了结构信息，欲确定一个隐马尔可夫模型还需一下三组参数</p>
<ul>
<li>
<p>初始状态概率</p>
</li>
<li>
<p>状态转移概率</p>
</li>
<li>
<p>输出观测概率</p>
</li>
</ul>
<h1>最大熵马尔可夫模型MEMM</h1>
<p>克服了观察值之间严格独立产生的问题</p>
<p><img src="/2019/03/08/机器学习-概率图模型/2.png" alt="1"></p>
<p>只在局部做归一化，所以容易陷入局部最优</p>
<p>标记偏置问题</p>
<h1>马尔可夫随机场</h1>
<h1>条件随机场CRF</h1>
<p><a href="https://www.cnblogs.com/wuxiangli/p/7196984.html" target="_blank" rel="noopener">https://www.cnblogs.com/wuxiangli/p/7196984.html</a></p>
<p><a href="https://www.imooc.com/article/27795" target="_blank" rel="noopener">https://www.imooc.com/article/27795</a></p>
<p>解决了标注偏置问题</p>
<p><img src="/2019/03/08/机器学习-概率图模型/3.png" alt="1"></p>
<h2 id="和hmm比较"><a class="header-anchor" href="#和hmm比较">¶</a>和HMM比较</h2>
<ul>
<li>
<p><strong>CRF可以定义数量更多，种类更丰富的特征函数</strong>。HMM模型具有天然具有局部性，以序列标注为例，在HMM模型中，当前的单词只依赖于当前的标签，当前的标签只依赖于前一个标签。这样的局部性限制了HMM只能定义相应类型的特征函数，我们在上面也看到了。但是CRF却可以着眼于整个句子s定义更具有全局性的特征函数</p>
</li>
<li>
<p><strong>CRF可以使用任意的权重</strong> 将对数HMM模型看做CRF时，特征函数的权重由于是log形式的概率，所以都是小于等于0的，而且概率还要满足相应的限制，如<br>
$$<br>
0 &lt;= p(w_i | l_i) &lt;= 1 \<br>
\sum_w p(w_i = w|l_1) = 1<br>
$$<br>
但在CRF中，每个特征函数的权重可以是任意值，没有这些限制</p>
</li>
</ul>
<h2 id="评价"><a class="header-anchor" href="#评价">¶</a>评价</h2>
<ul>
<li>MEMM只在局部做归一化，所以容易陷入局部最优，而CRF模型中，统计了全局概率，在做归一化时，考虑数据在全局的分布，而不是仅仅在局部归一化，解决了MEMM中的标记偏置的问题，可以得到全局最优。他并不在每一个节点进行归一化，而是所有特征进行全局归一化，因此可以求得全局的最优值</li>
<li>CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。但是CRF有明显的缺点：训练代价大、复杂度高</li>
<li>目前，条件随机场的训练和解码的开源工具还只支持链式的序列，复杂的尚不支持，而且训练时间很长，但效果还可以</li>
</ul>
<h1>比较</h1>
<p>以序列标注的建模为例，像词性标注，True casing中</p>
<p>隐马模型一个最大的缺点就是由于其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择</p>
<p>而最大熵隐马模型则解决了这一问题，可以任意的选择特征，但由于其在每一节点都要进行归一化，所以只能找到局部的最优值，同时也带来了标记偏见的问题（label bias），即凡是训练语料中未出现的情况全都忽略掉</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/机器学习-线性模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/机器学习-线性模型/" itemprop="url">机器学习-线性模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:34:29+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1>回归分析</h1>
<h2 id="线性回归"><a class="header-anchor" href="#线性回归">¶</a>线性回归</h2>
<p>$$<br>
y = \theta^T x+b+\epsilon \<br>
\epsilon表示noise,假设\epsilon \thicksim N(0,\sigma ^2) \<br>
线性组合可以表示为\sum_{i=1}^{p+1} \theta_ix_i,其中x_{p+1}=1\<br>
那么Y\thicksim N(\theta^T x,\sigma ^2)\<br>
p(y|x) = \frac{1}{\sigma \sqrt{2\pi}}\exp(-\frac{(y-\theta^T x)<sup>2}{2\sigma</sup>2})\<br>
贝叶斯派Bayesian一般写为p(y|x,\theta,\sigma^2)\<br>
频率学派Frequentist一般写为p_{\theta,\sigma^2}(y|x)\<br>
$$</p>
<p>$$<br>
对于独立同分布数据Independent \ and \ Identically \ Distributed	(iid)\<br>
\mathcal D={(x_1,y1)…(x_i,yi)}<em>{i=1}^n\<br>
P(\mathcal D) = \prod</em>{i=1}^n p(x_i,y_i) = \prod_{i=1}^n p(x_i)p(y_i|x_i)\<br>
可将数据集中的x视为矩阵X \in \mathbb R^{np},y视为向量Y\in \mathbb R^{n},\theta \in R^{p}<br>
那么开头的式子可以重写为Y=X\theta + \epsilon<br>
$$</p>
<h3 id="最小二乘法-模型"><a class="header-anchor" href="#最小二乘法-模型">¶</a>最小二乘法/模型</h3>
<p>最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。</p>
<p>使误差「所谓误差，当然是观察值与实际真实值的差量」平方和达到最小以寻求估计值的方法，就叫做最小二乘法，用最小二乘法得到的估计，叫做最小二乘估计。当然，取平方和作为目标函数只是众多可取的方法之一。</p>
<p>最小二乘使得误差平方和最小，并在各个方程的误差之间建立了一种平衡，从而防止某一个极端误差取得支配地位 计算中只要求偏导后求解线性方程组，计算过程明确便捷 最小二乘可以导出算术平均值作为估计值 是一种数学的优化技术，通过求最小化平方误差来寻找最佳的函数匹配 假设现在有二维的观测数据(x1,y1),(x2,y2)…(xn,yn)(x1,y1),(x2,y2)…(xn,yn)，求y=a+bxy=a+bx的拟合。</p>
<p>训练串行，运行并行</p>
<h4 id="使用极大似然法分析"><a class="header-anchor" href="#使用极大似然法分析">¶</a>使用极大似然法分析</h4>
<p>找出\theta，让概率最大</p>
<p>分析法：使用求导方法</p>
<p>迭代法：（随机）梯度下降<br>
$$<br>
P(\mathcal D) = \prod_{i=1}^n p(x_i,y_i) \<br>
= \prod_{i=1}^n p(x_i)p(y_i|x_i)\<br>
= \prod_{i=1}^n p(x_i)\prod_{i=1}^np(y_i|x_i)第一项都是一样的，因此可省略\<br>
\mathcal L(\theta|\mathcal D) = \prod_{i=1}^np(y_i|x_i)\<br>
= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2\pi}}\exp(-\frac{(y_i-\theta^T x_i)<sup>2}{2\sigma</sup>2})\<br>
= \frac{1}{\sigma^n (2\pi)<sup>{n/2}}\exp(-\frac{\sum_{i=1}</sup>n(y_i-\theta^T x_i)<sup>2}{2\sigma</sup>2})\<br>
$$</p>
<p>$$<br>
我们想要\hat \theta_{MLE} = \arg \max_{\theta \in R^{p}}\mathcal L(\theta|\mathcal D)\<br>
= \arg \max_{\theta \in R^{p}}\mathcal \log \mathcal L(\theta|\mathcal D) \<br>
\log \mathcal L(\theta|\mathcal D) = -\log(\sigma^n (2\pi)^{n/2}) - \frac{\sum_{i=1}<sup>n(y_i-\theta</sup>T x_i)<sup>2}{2\sigma</sup>2}\<br>
去掉常数项可得\<br>
\log \mathcal L(\theta|\mathcal D) = - \sum_{i=1}<sup>n(y_i-\theta</sup>T x_i)^2\<br>
\hat \theta_{MLE} = \arg \min_{\theta \in R^{p}} \sum_{i=1}<sup>n(y_i-\theta</sup>T x_i)^2<br>
$$</p>
<p>可以通过求梯度使其为0</p>
<p><img src="/2019/03/08/机器学习-线性模型/image-20190812135825778.png" alt="image-20190812135825778"><br>
$$<br>
-\triangledown_\theta \log \mathcal L(\theta|\mathcal D) = \triangledown_\theta \sum_{i=1}<sup>n(y_i-\theta</sup>T x_i)^2 \<br>
= -2\sum_{i=1}<sup>n(y_i-\theta</sup>T x_i)x_i\<br>
= -2\sum_{i=1}^ny_ix_i + -2\sum_{i=1}<sup>n(\theta</sup>T x_i)x_i \<br>
= -2X^TY + 2X^TX\theta<br>
$$<br>
为了使得极大似然是凸的，计算二阶导数（Hessian）<br>
$$<br>
-\triangledown_\theta^2 \log \mathcal L(\theta|\mathcal D) = 2X^TX<br>
$$<br>
如果X是满秩的，那么X^TX则是正定矩阵，那么可以让梯度为0，求得<br>
$$<br>
-\triangledown_\theta \log \mathcal L(\theta|\mathcal D) = -2X^TY + 2X^TX\theta = 0\<br>
(X^TX)\hat \theta_{MLE} = X^TY\<br>
\hat \theta_{MLE} = (X<sup>TX)</sup>{-1}X^TY<br>
$$</p>
<h4 id="使用几何的思想分析"><a class="header-anchor" href="#使用几何的思想分析">¶</a>使用几何的思想分析</h4>
<p>对于计算<br>
$$<br>
\hat Y = X\theta<br>
$$<br>
我们可以视作\hat Y是X的列向量张成的空间（col(X)），因为误差，真实的Y不再这个空间中，因此需要找到一个合适的\theta，使得y - \hat y误差最小</p>
<p>一种方法是找到Y在col(X)上的投影\hat y，由投影的性质可知y - \hat y垂直于col(X)，换句话说就是y - \hat y垂直于空间中的x1，x2，可得<br>
$$<br>
(y-\hat y)^T x_1 = (y-X\hat \theta)^T x1= 0\<br>
(y-\hat y)^T x_2 = (y-X\hat \theta)^T x2= 0\<br>
$$<br>
上式合并可得<br>
$$<br>
(y-X\hat \theta)^TX = 0\<br>
X^T(y-X\hat \theta) = 0\<br>
X<sup>Ty-X</sup>TX\hat \theta = 0\<br>
X^TX\hat \theta = X^Ty\<br>
\theta = (X<sup>TX)</sup>{-1}X^Ty<br>
$$</p>
<h4 id="伪逆-pseudo-inverse"><a class="header-anchor" href="#伪逆-pseudo-inverse">¶</a>伪逆 Pseudo-Inverse</h4>
<p>Moore-Penrose Psuedoinverse<br>
$$<br>
X^\dagger = (X<sup>TX)</sup>{-1}X^T<br>
$$<br>
而如果X是n*n矩阵，且可逆的话<br>
$$<br>
X^\dagger = (X<sup>TX)</sup>{-1}X^T = X<sup>{-1}(X</sup>T)<sup>{-1}X</sup>T = X^{-1}<br>
$$</p>
<h4 id="计算mle"><a class="header-anchor" href="#计算mle">¶</a>计算MLE</h4>
<p>$$<br>
\hat \theta_{MLE} = (X<sup>TX)</sup>{-1}X^TY<br>
$$</p>
<p>一般来说不会直接计算X^TX的逆，计算代价太大</p>
<h5 id="直接的方法"><a class="header-anchor" href="#直接的方法">¶</a>直接的方法</h5>
<ul>
<li>
<p>Cholesky分解 Cholesky factorization<br>
$$<br>
\mathtt{solve}<em>{\hat \theta</em>{\mathtt{MLE}}} (\underbrace{X^TX}<em>C) \hat \theta</em>{MLE} = \underbrace{X^TY}<em>d\<br>
计算对称矩阵:C = X^TX = \sum</em>{i=1}<sup>nx_ix_i</sup>T \quad O(np^2)\<br>
计算向量:d = X^TY = \sum_{i=1}^nx_iy_i \quad O(np)\<br>
Cholesky分解:LL^T = C \quad O(p^3) \quad L是下三角矩阵\<br>
\mathtt{Forward \ subs}:Lz = d \quad O(p^2)\<br>
\mathtt{Backward \ subs}:L^T\theta_{\mathtt{MLE}} = z \quad O(p^2)\<br>
\theta_{\mathtt{MLE}} \in \mathbb R^{p} \quad X \in \mathbb R^{np} \quad Y \in \mathbb R^{n} \quad C \in \mathbb R^{pp} \quad d \in \mathbb R^{p} \quad L \in \mathbb R^{pp} \quad z \in \mathbb R^{p}<br>
$$<br>
三角形矩阵等式求解，见<code>数学-数值方法</code></p>
</li>
<li>
<p>QR分解 QR factorization</p>
</li>
<li>
<p>分布式计算</p>
</li>
</ul>
<h5 id="迭代的方法"><a class="header-anchor" href="#迭代的方法">¶</a>迭代的方法</h5>
<p>​	当n接近于p时，计算复杂度时O(n^3)，是不可接受的，因此需要用迭代的方法求解</p>
<ul>
<li>
<p>Krylov subspace methods</p>
</li>
<li>
<p>梯度下降 Gradient Descent<br>
$$<br>
从\tau到0,直到收敛\<br>
\theta^{(\tau+1)} = \theta^{(\tau)} - \rho(\tau)\triangledown_{\theta^{(\tau)}} \log \mathcal L(\theta^{(\tau)}|\mathcal D)\<br>
= \theta^{(\tau)} - \rho(\tau) \frac{1}{n} \sum_{i=1}<sup>n(y_i-\theta</sup>{(\tau)T} x_i)x_i \quad O(np) \quad 这里我怀疑是O(np^2) \<br>
\rho(\tau)是每一轮迭代的学习率<br>
$$</p>
</li>
<li>
<p>随机梯度下降 Stochastic Gradient Descent<br>
$$<br>
随机选取一个i\<br>
\theta^{(\tau+1)} = \theta^{(\tau)} - \rho(\tau) (y_i-\theta^{(\tau)T} x_i)x_i \quad O§ \quad 这里我怀疑是O(p^2)<br>
$$</p>
</li>
</ul>
<h4 id="拟合非线性数据"><a class="header-anchor" href="#拟合非线性数据">¶</a>拟合非线性数据</h4>
<p>转换特征空间<br>
$$<br>
使用非线性转换\phi:\mathbb R^{p} \rightarrow \mathbb R^{k} \<br>
ex:\phi(x) = {1,x,x<sup>2,…,x</sup>k}<br>
还有样条函数splines, 径向基函数radial \ basis \ functions<br>
$$<br>
可能会有过拟合，Bias-Variance Tradeoff见<code>数据科学-过(欠)拟合</code></p>
<h4 id="方差和偏差分析-bias-variance"><a class="header-anchor" href="#方差和偏差分析-bias-variance">¶</a>方差和偏差分析（bias，variance）</h4>
<p>Bias-Variance Tradeoff的基础见<code>数据科学-过(欠)拟合</code><br>
$$<br>
f(x_<em>|D) = x_</em>^T\hat \theta_{MLE}\<br>
假设h(x_<em>) = x_</em>^T\theta<br>
$$</p>
<p>$$<br>
bias = h(x_<em>)-E_{D}[f(x_</em>|D)]\<br>
= x_<em>^T\theta - E_{D}[x_</em>^T\hat \theta_{MLE}]\<br>
= x_<em>^T\theta - E_{D}[x_</em><sup>T(X</sup>TX)<sup>{-1}X</sup>TY]\<br>
= x_<em>^T\theta - E_{D}[x_</em><sup>T(X</sup>TX)<sup>{-1}X</sup>T(X\theta + \epsilon))]\<br>
= x_<em>^T\theta - E_{D}[x_</em><sup>T(X</sup>TX)<sup>{-1}X</sup>TX\theta + x_<em><sup>T(X</sup>TX)<sup>{-1}X</sup>T\epsilon]\<br>
= x_</em>^T\theta - E_{D}[x_<em>^T\theta + x_</em><sup>T(X</sup>TX)<sup>{-1}X</sup>T\epsilon]\<br>
= x_<em>^T\theta - x_</em>^T\theta + x_<em><sup>T(X</sup>TX)<sup>{-1}X</sup>TE_{D}[\epsilon]\<br>
= x_</em>^T\theta - x_*^T\theta\<br>
= 0\<br>
\hat \theta_{MLE} \ is \ unbiased(无偏估计)<br>
$$</p>
<p>$$<br>
Var = E_{D}[(f(x_<em>|D)-E_D[f(x_</em>|D)])^2]\<br>
= E_{D}[(x_<em>^T\hat \theta_{MLE}-x_</em><sup>T\theta)</sup>2]\<br>
= E_{D}[(x_<em><sup>T(X</sup>TX)<sup>{-1}X</sup>TY-x_</em><sup>T\theta)</sup>2]\<br>
= E_{D}[(x_<em><sup>T(X</sup>TX)<sup>{-1}X</sup>T(X\theta + \epsilon)-x_</em><sup>T\theta)</sup>2]\<br>
= E_{D}[(x_<em>^T\theta + x_</em><sup>T(X</sup>TX)<sup>{-1}X</sup>T\epsilon -x_<em><sup>T\theta)</sup>2]\<br>
= E_{D}[(x_</em><sup>T(X</sup>TX)<sup>{-1}X</sup>T\epsilon)^2]\<br>
= E_{D}[(x_<em><sup>T(X</sup>TX)<sup>{-1}X</sup>T\epsilon)(x_</em><sup>T(X</sup>TX)<sup>{-1}X</sup>T\epsilon)]\<br>
= E_{D}[x_<em><sup>T(X</sup>TX)<sup>{-1}X</sup>T\epsilon \epsilon^T (x_</em><sup>T(X</sup>TX)<sup>{-1}X</sup>T)^T]\<br>
= x_<em><sup>T(X</sup>TX)<sup>{-1}X</sup>T E_{D}[\epsilon \epsilon^T] (x_</em><sup>T(X</sup>TX)<sup>{-1}X</sup>T)^T\<br>
= x_<em><sup>T(X</sup>TX)<sup>{-1}X</sup>T \sigma_\epsilon^2 (x_</em><sup>T(X</sup>TX)<sup>{-1}X</sup>T)^T\<br>
= x_<em><sup>T(X</sup>TX)<sup>{-1}X</sup>T \sigma_\epsilon^2 X(x_</em><sup>T(X</sup>TX)<sup>{-1})</sup>T\<br>
= \sigma_\epsilon^2 x_<em><sup>T(X</sup>TX)<sup>{-1}X</sup>T  X(x_</em><sup>T(X</sup>TX)<sup>{-1})</sup>T\<br>
= \sigma_\epsilon^2 x_<em><sup>T(x_*</sup>T(X<sup>TX)</sup>{-1})^T\<br>
= \sigma_\epsilon^2 x_</em><sup>T(X</sup>TX)^{-1}x_*, \ 这里(X<sup>TX)</sup>{-1}是对称矩阵，因此不变<br>
$$</p>
<p>$$<br>
如果假设x_i和x_<em>是N(0,1)分布的,那么\<br>
E_{X,x_</em>}[Var] = \sigma_\epsilon^2 E_{X,x_<em>}[x_</em><sup>T(X</sup>TX)^{-1}x_<em>]\<br>
= \sigma_\epsilon^2 E_{X,x_</em>}[tr(x_<em>x_</em><sup>T(X</sup>TX)^{-1})]\<br>
= \sigma_\epsilon^2 tr(E_{X,x_<em>}[x_<em>x_</em><sup>T(X</sup>TX)^{-1}])\<br>
= \sigma_\epsilon^2 tr(E_{X,x_</em>}[x_<em>x_</em><sup>T]E_{X,x_*}[(X</sup>TX)^{-1}])\<br>
\approx \frac{\sigma_\epsilon^2}{n} tr(E_{X,x_*}[x_<em>x_</em>^T]) \ 注1 \<br>
= \frac{\sigma_\epsilon^2}{n}p<br>
$$</p>
<p>$$<br>
注1:关于这一步，我是这样理解的\<br>
X \in \mathbb R^{np}\<br>
(X^TX)<em>{ij} = \sum</em>{k=1}<sup>nX</sup>T_{ik}X_{kj}\<br>
因为X_{ij}是N(0,1)分布的，且E(X^2) = D(X)+E(X)^2 = \sigma^2 + \mu<sup>2，因此E[X</sup>T_{ij}X_{ji}] = 1 \<br>
E[\sum_{j=1}<sup>nX</sup>T_{ij}X_{ji}] = n\<br>
X<sup>TX,(X</sup>TX)^{-1} \in \mathbb R^{pp}\<br>
设X<sup>TX为A,(X</sup>TX)^{-1}为B\<br>
(AB)<em>{ii} = \sum</em>{k=1}^pA_{ik}B_{ki} = 1\<br>
因为E[A_{ik}] = n,因此E[B_{ki}] = \frac{1}{np}\<br>
设x_<em>x_</em>^T = C,根据E(X^2) = D(X)+E(X)^2 = \sigma^2 + \mu^2可知E[C_{ij}] = 1\<br>
(CB)<em>{ii} = \sum</em>{k=1}^p C_{ik}B_{ki}, E[(CB)_{ii}] = \frac{1}{n}<br>
因此相比之前迹缩小到了\frac{1}{n}<br>
$$</p>
<p>因此，数据越多，方差越小；维度越大，方差越大</p>
<h3 id="高斯-马尔可夫定理-gauss-markov-theorem"><a class="header-anchor" href="#高斯-马尔可夫定理-gauss-markov-theorem">¶</a>高斯-马尔可夫定理 Gauss-Markov Theorem</h3>
<p><a href="https://www.jianshu.com/p/b49f28b1b98c" target="_blank" rel="noopener">https://www.jianshu.com/p/b49f28b1b98c</a></p>
<p>在线性回归模型中，如果误差满足零均值、同方差且互不相关，则回归系数的最佳线性无偏估计(BLUE, Best Linear unbiased estimator)就是普通最小二乘法估计。</p>
<p>基本假设，DW检验，多重共线性，参数估计值方差</p>
<p>线性回归，非线性回归non-Linear regression，logistic回归，岭回归，主成分回归</p>
<p>回归分析就是利用已知数据，产生拟合方程，从而对未知数据进行预测<br>
线性回归分析：一元线性，多元线性，广义线性<br>
非线性回归分析<br>
困难：选定变量（多元），避免多重共线性，避免过拟合，检验模型是否合理</p>
<p>关系<br>
函数关系：确定性关系，如<br>
$$<br>
y = 10x + 5<br>
$$<br>
相关关系：非确定性关系<br>
使用相关系数去衡量线性相关的强弱：</p>
<p><img src="/2019/03/08/机器学习-线性模型/1.png" alt="1"></p>
<p>区间范围是[-1,1]，绝对值越接近1，表示线性程度越高</p>
<p>一元线性回归推导：</p>
<p><img src="/2019/03/08/机器学习-线性模型/2.png" alt="2"><br>
回归方法擅长内推插值，不擅长外推归纳，外推归纳用时间序列分析比较好</p>
<p>多元回归方程，和一元的类似<br>
在选择变量方面，多元回归分析可以采用的一种方法是逐步回归：<br>
向前引入法：从一元回归开始，逐步增加变量，是指标达到最优为止<br>
向后剔除法：从全变量回归方程开始，逐步删去某些变量，使指标值达到最优为止<br>
逐步筛选法：综合上面两种方法<br>
变量选择的评价指标：RSS（残差平方和），R^2（相关系数平方），AIC准则（Akaike information criterion）和BIC准则（Bayesian Information Criterion）</p>
<p><img src="/2019/03/08/机器学习-线性模型/3.png" alt="3"></p>
<p>哑变量/虚拟变量：将枚举变量变为离散型变量<br>
比如男/女变为（1，0）和（0，1）</p>
<p>回归诊断：<br>
样本是否符合正态分布假设（正态分布检验）<br>
是否存在离群值导致模型产生较大误差<br>
线性模型是否合理（残差图判断）<br>
误差是否满足独立性，等方差，正态分布等假设条件<br>
是否存在多重共线性（通过计算特征根，kappa值发现多重共线性）</p>
<p>多重共线性的发现：</p>
<p><img src="/2019/03/08/机器学习-线性模型/4.png" alt="4"></p>
<p><img src="/2019/03/08/机器学习-线性模型/5.png" alt="5"></p>
<p>广义线性回归：<br>
比如logistic回归：<br>
<img src="/2019/03/08/机器学习-线性模型/6.png" alt="6"></p>
<p><img src="/2019/03/08/机器学习-线性模型/7.png" alt="7"></p>
<h1>逻辑回归 Logistic Regression</h1>
<p>是判别式分类器Discriminative classifier</p>
<h2 id="概念"><a class="header-anchor" href="#概念">¶</a>概念</h2>
<p>$$<br>
设X为数据项目(x_1,…,x_n)，Y为分类标签,直接学习P(Y|X)\<br>
再设W为权值(w_1,…,w_n)\<br>
那么sigmoid \ function:\<br>
P_0(X;W) = P(Y=0|X) = \frac{1}{1+e<sup>{-W</sup>TX}}\<br>
P_1(X;W) = P(Y=1|X) = 1-\frac{1}{1+e<sup>{-W</sup>TX}}\<br>
可得\log \frac{P_1(X;W)}{P_0(X;W)} = W^TX<br>
$$</p>
<h2 id="学习算法"><a class="header-anchor" href="#学习算法">¶</a>学习算法</h2>
<p>$$<br>
可以使得训练数据的似然函数最大:\<br>
W = {\arg \max}<em>W  \prod_l P(y_l;X_l,W)\<br>
= {\arg \max}<em>W  \sum_l \ln P(y_l;X_l,W) \ 转为对数似然函数\<br>
对于\sum_l \ln P(y_l;X_l,W),可以定义为l(W):\<br>
l(W) = \sum_l y_l\ln P(y_l=1;X_l,W)+(1-y_l)\ln P(y_l=0;X_l,W)\<br>
= \sum_l y_l \ln \frac{P(y_l=1;X_l,W)}{P(y_l=0;X_l,W)}+\ln P(y_l=0;X_l,W)\<br>
= \sum_l y_l(w_0+\sum</em>{i=1}<sup>nw_ix_{li})-\ln(1+e</sup>{(w_0+\sum</em>{i=1}^nw_ix_{li})})<br>
$$</p>
<p>然而这里并没有一个(近似的)形式解来找到使l(W)最大的W</p>
<p>一种方法就是使用梯度下降<br>
$$<br>
\frac{\partial}{\partial w_i}l(W) = \sum_l y_lx_{li}-\frac{x_{li}e<sup>{(w_0+\sum_{i=1}</sup>nw_ix_{li})}}{1+e<sup>{(w_0+\sum_{i=1}</sup>nw_ix_{li})}}\<br>
= \sum_lx_{li}(y_l - \hat P(y_l=1;X_l,W))\<br>
因此我们可以设出事参数W的所有参数为0，每次对每一个项进行更新:\<br>
w_i = w_i+\eta \sum_lx_{li}(y_l - \hat P(y_l=1;X_l,W))<br>
$$</p>
<h2 id="正则化regularization"><a class="header-anchor" href="#正则化regularization">¶</a>正则化Regularization</h2>
<p>在逻辑回归中，当数据维数大且稀疏时，容易发生过拟合</p>
<p>一种方法是使用正则化的方法<br>
$$<br>
在对数似然中加入惩罚项:\<br>
W = {\arg \max}<em>W  \sum_l \ln P(y_l;X_l,W) - \frac{\lambda}{2}||W^2||\<br>
\frac{\partial}{\partial w_i}l(W) = \sum_lx</em>{li}(y_l - \hat P(y_l=1;X_l,W)) - \lambda w_i\<br>
w_i = w_i+\eta \sum_lx_{li}(y_l - \hat P(y_l=1;X_l,W))- \eta \lambda w_i<br>
$$</p>
<p>sigmoid函数</p>
<p>多项/多类别Logistic回归 Multinomial logistic regression</p>
<p>Fisher的线性判别 Fisher’s linear discriminant</p>
<p>感知 Perceptron（感知机）</p>
<p>逻辑回归是一种分类的方法，主要用于二分类，从训练数据特征学习出一个0/1分类模型，以一个线性组合作为自变量，使用逻辑函数将自变量映射到(0,1)上，LR分类器实际上就是求解一组权值，带入逻辑函数中，得到一个类别为1和类别为0的概率。</p>
<p>可以用梯度下降求解</p>
<h1>比较</h1>
<p>逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。</p>
<p>逻辑回归和线性回归<br>
逻辑回归是一个线性的二分类问题，主要是计算在某个样本特征下事件发生的概率</p>
<p>线性函数+sigmoid函数求得，这个线性和函数权重的特征值的累加以及加上偏置求出来的，所以训练其实就是在训练这个权重。</p>
<p>所以求解问题就变成了这个最大似然函数的最优化问题，这里通常会采样随机梯度下降法和拟牛顿迭代法来进行优化</p>
<p>如果类别之间是否互斥（比如音乐只能属于古典音乐、乡村音乐、摇滚月的一种）就用softmax 否则类别之前有联系（比如一首歌曲可能有影视原声，也可能包含人声，或者是舞曲），这个时候使用k个LR更为合适</p>
<p>优缺点：<br>
Logistic回归优点：</p>
<p>实现简单；<br>
分类时计算量非常小，速度很快，存储资源低；<br>
缺点：</p>
<p>容易欠拟合，一般准确度不太高<br>
只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/数学-优化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/数学-优化/" itemprop="url">数据科学-优化知识</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:32:59+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<p><a href="https://blog.csdn.net/qq_32769481/article/details/84330857" target="_blank" rel="noopener">https://blog.csdn.net/qq_32769481/article/details/84330857</a></p>
<p>梯度下降<br>
梯度下降法<br>
在微积分中，对多元函数的参数求偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。几何意义上来说，梯度就是函数变化增加最快的地方。沿着梯度相反的方向，就是梯度减少最快的地方，最容易找到最小值。</p>
<p>在最小化损失函数时，可以通过梯度下降法来一步步迭代求解，得到最小化的损失函数，和模型参数值。</p>
<p>步长：步长决定了在梯度下降迭代的过程中，每一步沿着梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最容易下山的位置走得那一步的长度</p>
<p>特征：样本中输入部分</p>
<p>假设函数：监督学习中，为了拟合输入样本，而使用的假设函数</p>
<p>损失函数：为了评估模型拟合的好坏，通常用损失函数来度量拟合的成都。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。</p>
<p>算法过程：</p>
<p>确定当前的损失函数的梯度，通过求当前参数的微分<br>
用步长乘以损失函数的梯度，得到当前位置下降的距离<br>
确定是否所有的梯度下降的距离都小于ε，如果小于ε则算法终止，当前的所有参数即为最终结果，否则进入步骤4<br>
更新所有的参数，更新完以后进入步骤1<br>
算法调优：</p>
<p>步长选择，多选一些值，从大到小分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要减小步长<br>
算法参数的初始值选择，初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值。<br>
归一化：由于样本不同特征的取值范围不一样，可能导致迭代很慢。为了减少特征取值的影响，可以对特征归一化或者标准化。<br>
批梯度下降：更新参数使用所有样本进行更新</p>
<p>随机梯度下降：求梯度时没有用所有的m个样本的数据，而仅仅选取一个样本来求梯度。</p>
<p>小批量梯度下降：对于m各样本，采用x个样子来迭代。</p>
<p>和最小二乘法比较：</p>
<p>不同：梯度下降是迭代求解，最小二乘法是解析求解</p>
<p>缺点：需要选择步长，如果样本少的时候，最小二成计算速度块</p>
<p>优点：样本量很大的时候，最小二乘需要去一个超级大的逆矩阵，这时候求解很慢，使用迭代的梯度下降法比较有优势</p>
<p>梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。</p>
<p>牛顿迭代<br>
牛顿迭代<br>
牛顿-拉弗森方法：</p>
<p>切线是曲线的线性逼近。</p>
<p>随便找一个曲线上的点做切线，找到与x周的交点做垂线，然后继续做切线，重复刚才的过程</p>
<p>如果f二阶可导，那么待求的零点x周围存在一个区域，只要起始点位于这个邻近域内，那么牛顿-拉弗森方法必定收敛</p>
<p>但是因为我们不知道根点在哪里，所以起始点的选择就不一定在这个区域内。</p>
<p>驻点：驻点没有根<br>
越来越远离：比如说x的三分之一的幂<br>
循环震荡不收敛：x的绝对值的二分之一幂<br>
注意零点倒数不存在也有可能可以用牛顿方法解<br>
不能完整求出所有根，可能只能求到近一点的根，或者远一点的跟<br>
应用牛顿-拉弗森方法，要注意以下问题：</p>
<p>函数在整个定义域内最好是二阶可导的</p>
<p>起始点对求根计算影响重大，可以增加一些别的判断手段进行试错</p>
<h1>几何概念</h1>
<p>线段<br>
超平面与线性簇<br>
邻域<br>
多面体和多胞形</p>
<p>凸集<br>
一个集合C是，当前仅当任意x,y属于C且0≤Θ≤10≤Θ≤1，都有Θ∗x+(1−Θ)∗yΘ∗x+(1−Θ)∗y属于C</p>
<p>用通俗的话来说C集合线段上的任意两点也在C集合中</p>
<h1>函数概念</h1>
<p>凸函数<br>
一个函数f其定义域(D(f))是凸集，并且对任意x,y属于D(f)和0≤Θ≤10≤Θ≤1都有</p>
<p>f(Θ∗x+(1−Θ)∗y)≤Θ∗f(x)+(1−Θ)∗f(y)f(Θ∗x+(1−Θ)∗y)≤Θ∗f(x)+(1−Θ)∗f(y)</p>
<p>常见的凸函数有：</p>
<p>指数函数f(x)=ax;a&gt;1f(x)=ax;a&gt;1<br>
负对数函数−logax;a&gt;1,x&gt;0−logax;a&gt;1,x&gt;0<br>
开口向上的二次函数等<br>
凸函数的判定：</p>
<p>如果f是一阶可导，对于任意数据域内的x,y满足f(y)≥f(x)+f′(x)(y−x)f(y)≥f(x)+f′(x)(y−x)<br>
如果f是二阶可导，<br>
凸优化应用举例</p>
<p>SVM：其中由max|w|max|w| 转向min(12∗|w|2)min(12∗|w|2)<br>
最小二乘法？</p>
<p>LR的损失函数∑(yi∗log(hw(xi))+(1−yi)∗(log(1−hw(xi))))</p>
<p>仿射函数</p>
<h1>无约束优化</h1>
<h2 id="一维搜索"><a class="header-anchor" href="#一维搜索">¶</a>一维搜索</h2>
<p>​	黄金分割/斐波那契分割/二分法</p>
<p>​	牛顿法</p>
<p>​	割线法</p>
<p>​	画界法</p>
<h2 id="梯度下降方法-最速下降方法"><a class="header-anchor" href="#梯度下降方法-最速下降方法">¶</a>梯度下降方法/最速下降方法</h2>
<p>$$<br>
在机器学习任务中，需要最小化损失函数L(\theta)，其中\theta是要求解的模型参数。\<br>
梯度下降法常用来求解这种无约束最优化问题，它是一种迭代方法：选取初值\theta_0,不断迭代，更新\theta的值，进行损失函数极小化<br>
$$</p>
<p>$$<br>
迭代公式：\theta^t = \theta^{t-1}+\Delta\theta\<br>
将L(\theta<sup>t)在\theta</sup>{t-1}处进行一阶泰勒展开：\<br>
L(\theta^t) = L(\theta^{t-1}+\Delta\theta)\<br>
\approx L(\theta^{t-1}) + L’(\theta^{t-1})\Delta\theta\<br>
要使得L(\theta^t) &lt; L(\theta^{t-1}),可取\Delta\theta = -\alpha L’(\theta^{t-1})\<br>
那么\theta^t = \theta^{t-1}-\alpha L’(\theta^{t-1})\<br>
这里\alpha是 步长，可通过line \ search确定，但一般直接赋一个小的数。<br>
$$</p>
<h3 id="梯度下降的困难"><a class="header-anchor" href="#梯度下降的困难">¶</a>梯度下降的困难</h3>
<ol>
<li>梯度的计算：目标函数经常是求和函数的形式，当样本量极大的时候</li>
<li>学习率的选择：太小导致收敛很慢，太大导致不收敛</li>
</ol>
<h2 id="牛顿法-拟牛顿法"><a class="header-anchor" href="#牛顿法-拟牛顿法">¶</a>牛顿法/拟牛顿法</h2>
<p>牛顿法<br>
$$<br>
将L(\theta<sup>t)在\theta</sup>{t-1}处进行一阶泰勒展开：\<br>
L(\theta^t) = L(\theta^{t-1}+\Delta\theta)\<br>
\approx L(\theta^{t-1}) + L’(\theta^{t-1})\Delta\theta + L’’(\theta^{t-1})\frac {\Delta\theta^2}{2}\<br>
为了简化分析过程，假设参数\theta是标量，则可将一阶和二阶导数分别记为 g 和 h\<br>
L(\theta^{t-1}) + g\Delta\theta + h\frac {\Delta\theta^2}{2}\<br>
所以让L(\theta^t)极小等价于让g\Delta\theta + h\frac {\Delta\theta^2}{2}极小\<br>
可令\frac{\partial (g\Delta\theta + h\frac {\Delta\theta^2}{2})}{\partial \Delta\theta} = 0\<br>
可得\Delta\theta = -\frac{g}{h},所以\theta^t = \theta^{t-1}-\frac{g}{h}\<br>
$$</p>
<p>$$<br>
把参数\theta推广到向量形式，迭代公式：\theta<sup>{t-1}-H</sup>{-1}g\<br>
这里H是海森矩阵<br>
$$</p>
<p>缺点：</p>
<ol>
<li>要求计算目标函数的二阶导数，高纬特征情况下，矩阵太大，计算和存储都有问题</li>
<li>小批量的情况下，牛顿法的计算结果噪音太大</li>
<li>目标函数非凸时，牛顿法更容易收敛到鞍点</li>
</ol>
<h2 id="bgfs"><a class="header-anchor" href="#bgfs">¶</a>BGFS</h2>
<h2 id="共轭方向法"><a class="header-anchor" href="#共轭方向法">¶</a>共轭方向法</h2>
<h2 id="求解线性方程组"><a class="header-anchor" href="#求解线性方程组">¶</a>求解线性方程组</h2>
<h3 id="最小二乘分析"><a class="header-anchor" href="#最小二乘分析">¶</a>最小二乘分析</h3>
<h3 id="kaczmarz算法"><a class="header-anchor" href="#kaczmarz算法">¶</a>Kaczmarz算法</h3>
<h2 id="神经网络"><a class="header-anchor" href="#神经网络">¶</a>神经网络</h2>
<h2 id="全局搜索算法"><a class="header-anchor" href="#全局搜索算法">¶</a>全局搜索算法</h2>
<p>###	Nelder-Mead单纯形法</p>
<p>###	模拟退火/粒子群优化/遗传算法</p>
<h1>有约束的线性规划</h1>
<h2 id="单纯形法"><a class="header-anchor" href="#单纯形法">¶</a>单纯形法</h2>
<h2 id="对偶线性规划"><a class="header-anchor" href="#对偶线性规划">¶</a>对偶线性规划</h2>
<h2 id="非单纯形法"><a class="header-anchor" href="#非单纯形法">¶</a>非单纯形法</h2>
<h3 id="khachiyan算法"><a class="header-anchor" href="#khachiyan算法">¶</a>Khachiyan算法</h3>
<h3 id="仿射尺度法"><a class="header-anchor" href="#仿射尺度法">¶</a>仿射尺度法</h3>
<h3 id="karmarkar算法"><a class="header-anchor" href="#karmarkar算法">¶</a>Karmarkar算法</h3>
<h2 id="整数规划"><a class="header-anchor" href="#整数规划">¶</a>整数规划</h2>
<h3 id="幺模矩阵"><a class="header-anchor" href="#幺模矩阵">¶</a>幺模矩阵</h3>
<h3 id="gomory割平面法"><a class="header-anchor" href="#gomory割平面法">¶</a>Gomory割平面法</h3>
<h1>有约束的非线性规划</h1>
<h2 id="仅含等式约束的优化"><a class="header-anchor" href="#仅含等式约束的优化">¶</a>仅含等式约束的优化</h2>
<h3 id="牛顿方法"><a class="header-anchor" href="#牛顿方法">¶</a>牛顿方法</h3>
<h3 id="切线空间-法线空间"><a class="header-anchor" href="#切线空间-法线空间">¶</a>切线空间/法线空间</h3>
<h3 id="拉格朗日条件"><a class="header-anchor" href="#拉格朗日条件">¶</a>拉格朗日条件</h3>
<h3 id="二阶条件"><a class="header-anchor" href="#二阶条件">¶</a>二阶条件</h3>
<h1>含不等式约束的优化：</h1>
<h2 id="kkt条件"><a class="header-anchor" href="#kkt条件">¶</a>KKT条件</h2>
<h2 id="二阶条件-v2"><a class="header-anchor" href="#二阶条件-v2">¶</a>二阶条件</h2>
<h2 id="求解"><a class="header-anchor" href="#求解">¶</a>求解</h2>
<h3 id="投影法"><a class="header-anchor" href="#投影法">¶</a>投影法</h3>
<h3 id="拉格朗日法"><a class="header-anchor" href="#拉格朗日法">¶</a>拉格朗日法</h3>
<h3 id="罚函数法"><a class="header-anchor" href="#罚函数法">¶</a>罚函数法</h3>
<h2 id="凸优化"><a class="header-anchor" href="#凸优化">¶</a>凸优化</h2>
<h3 id="凸集-凸函数"><a class="header-anchor" href="#凸集-凸函数">¶</a>凸集/凸函数</h3>
<p>###	 对偶</p>
<h3 id="拉格朗日松弛"><a class="header-anchor" href="#拉格朗日松弛">¶</a>拉格朗日松弛</h3>
<h3 id="内点法"><a class="header-anchor" href="#内点法">¶</a>内点法</h3>
<h3 id="半定规划"><a class="header-anchor" href="#半定规划">¶</a>半定规划</h3>
<h2 id="多目标优化"><a class="header-anchor" href="#多目标优化">¶</a>多目标优化</h2>
<h3 id="帕累托解"><a class="header-anchor" href="#帕累托解">¶</a>帕累托解</h3>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/数据科学-特征工程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/数据科学-特征工程/" itemprop="url">数据科学-特征工程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:32:59+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1>降维</h1>
<h2 id="维数灾难-curse-of-dimensionality"><a class="header-anchor" href="#维数灾难-curse-of-dimensionality">¶</a>维数灾难 Curse of Dimensionality</h2>
<p>当维度增加时，数据会变得稀疏</p>
<p>因此数据密度（density）和距离的意义会降低，尤其对于聚类和离群点检测</p>
<p><img src="/2019/03/08/数据科学-特征工程/image-20190811133934360.png" alt="image-20190811133934360"></p>
<p><img src="/2019/03/08/数据科学-特征工程/image-20190811134006388.png" alt="image-20190811134006388"></p>
<h2 id="主成分分析-pca-principal-component-analysis-pca"><a class="header-anchor" href="#主成分分析-pca-principal-component-analysis-pca">¶</a>主成分分析（PCA） Principal component analysis(PCA)</h2>
<p>通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫主成分（Goal is to find a projection that captures the largest amount of variation in data）</p>
<p>主成分回归（PCR） Principal component regression(PCR)</p>
<p>因子分析 Factor analysis</p>
<p>核化线性降维（核PCA）</p>
<p>流形学习</p>
<p>度量学习</p>
<p>独立成分分析</p>
<p>非线性成分分析</p>
<p>奇异值分解</p>
<p>稀疏学习</p>
<p>降纬与度量学习</p>
<p>线性判别分析（LDA）（Linear Discriminate Analysis）</p>
<p>FM，FFM算法</p>
<p><a href="https://www.cnblogs.com/peizhe123/p/7412364.html" target="_blank" rel="noopener">https://www.cnblogs.com/peizhe123/p/7412364.html</a></p>
<p>SIFT feature（图像）</p>
<p>张量分解</p>
<p>模型训练复杂度很大程度上依赖于特征维数，通常情况下，我们可以通过<strong>特征选择</strong>和<strong>数据降维</strong>的方法降低特征维数，以降低模型的训练复杂度。下面是对特征降维的简要概要介绍。</p>
<p>特征降维(feature dimension reduction)是一个从初始高维特征集合中选出低维特征集合，以便根据一定的评估准则最优化缩小特征空间的过程，通常是机器学习的预处理步骤。当面临高维数据时，特征降维对于机器学习任务非常必要，通过降维有效地消除无关和冗余特征，提高挖掘任务的效率，改善预测精确性等学习性能，增强学习结果的易理解性。</p>
<p>1、降维概述</p>
<p>通常，高维特征集合存在以下几方面问题:<br>
-大量的特征<br>
-许多与给定任务无关的特征，即存在许多与类别仅有微弱相关度的特征<br>
-许多对于给定任务冗余的特征，如特征相互之间存在强烈的相关度<br>
-噪声数据<br>
特征降维，可以分为<strong>特征抽取</strong>和<strong>特征选择</strong>两种方式。特征抽取涉及到语义上的分析，而目前自然语言语义处理技术尚不发达，用特征抽取方法进行特征降维的效果并不显著。相比之下，特征选择选出的特征集合是原始特征集的子集，所以更易实现，方法也更加多样，典型的有DF、IG、 MI、CHI。</p>
<p>(1)特征抽取<br>
特征抽取也被称为特征重参数化(feature reparameterization)。由于自然语言中存在大量的多义词、同义词现象，特征集无法生成一个最优的特征空间对数据内容进行描述。特征抽取通过对原始特征空间进行变换，重新生成一个维数更小、各维之间更加独立的特征空间。特征抽取方法主要有如下几种:</p>
<table>
<thead>
<tr>
<th style="text-align:center">有无指导</th>
<th style="text-align:center">线性</th>
<th style="text-align:center">非线性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">无</td>
<td style="text-align:center">主成分分析PCA</td>
<td style="text-align:center">Kohonen匹配</td>
</tr>
<tr>
<td style="text-align:center">无</td>
<td style="text-align:center">独立成分分析ICA</td>
<td style="text-align:center">非线性PCA网络</td>
</tr>
<tr>
<td style="text-align:center">无</td>
<td style="text-align:center">投影追踪</td>
<td style="text-align:center">Sammon投影</td>
</tr>
<tr>
<td style="text-align:center">有</td>
<td style="text-align:center">线性判别分析</td>
<td style="text-align:center">非线性区别分析</td>
</tr>
</tbody>
</table>
<p>(2)特征选择<br>
特征选择是从特征集T={1… ,ts}中选择一个真子集T’={t1’,… ,ts’},满足(s’&lt;&lt;s) 。其中: s为原始特征集的大小; s’是选择后的特征集大小。特征选择不改变原始特征空间的性质，只是从原始特征空间中选择一部分重要的特征，组成一个新的低维空间。</p>
<p>(3)特征降维策略<br>
从策略上可以将特征降维划分为<strong>局部降维</strong>和<strong>全局降维</strong>。局部降维是指对每个类别选择若干个最能识别它的特征作为新特征，有所有这些新特征构成新的特征空间，从而达到对原始特征空间的降维。全局降维是指选择对整个分类最有用的若干个特征构成新的特征空间，从而达到对原特征空间的降维。对于不同的降维方法，可采用的降维策略可能不同，但是通过特殊处理(如带权均值、最大值)后，特征对特定类地重要性也可以转换成特征对整个分类的重要性。</p>
<p>2、降维模型<br>
现有的特征降维模型大致分为过滤模型、包裹模型及其他改进模型。</p>
<p>(1)过滤模型<br>
过滤模型(filter model)的基本思想是:根据训练数据的一般特性进行特征选择，在特征选择的过程中并不包含任何学习算法。早期的过滤算法依赖于标记数据，通过分析标记数据来决定哪些特征在区分分类标签时最有用，因此传统过滤模型只适用于有指导的学习。随着应用领域的扩展，在很多数据挖掘应用中无法获得类标签，因此将传统过滤模型集合聚类思想，如层次聚类、分割聚类、光谱聚类、矩阵分解算法，可以产生许多新的适合无指导学习的过滤模型。基于过滤模型的算法主要有两类:特征权重和子集搜索。这两类算法的不同之处在于是对单个特征进行评价还是对整个特征子集进行评价。</p>
<p><strong>特征权重算法</strong>对每个特征指定一个权值， 并按照它与目标概念的相关度对其进行排序，如果一个特征的相关度权值大于某个阈值，则认为该特征优秀，并且选择该特征。该算法缺点在于:他们可以捕获特征与目标概念间的相关性，却不能发现特征间的冗余性。而经验证明除了无关特征对学习任务的影响，冗余特征同样影响学习算法的速度和准确性，也应尽可能消除冗余特征。Relief算法是一个比较著名的特征权重类方法。</p>
<p><strong>子集搜索算法</strong>通过在一定的度量标准指导下遍历候选特征子集，对每个子集进行优劣评价，当搜索停止时即可选出最优(或近似最优)的特征子集。现有子集搜索算法的时间复杂度至少为维度的平方，所以在处理高维数据时不具有强可量测性。</p>
<p>考虑到各种过滤方法的优劣，可以使用多层过滤模型分别消除无关特征和冗余特征。</p>
<p>(2)包裹模型<br>
包裹模型(wrapper model)最初思想为依据一个有指导的归纳算法，搜索最佳特征子集;对于每一个新的特征子集，包裹模型都需要学习一个假设(或一个分类器、包裹器)，即需要元学习者遍历特征集合空间，并利用该学习算法的性能来评价和决定选择哪些特征。目前研究中包裹模型的搜索过程主要依据一个聚类算法。包裹模型包含聚类过程反馈，将聚类执行效果量化为性能指数，通过最大化该性能指数更好地找出哪些更适合预定学习算法的特征，具有较高的学习性能。</p>
<p>(3) 混合模型<br>
过滤模型和包裹模型的发展都经历了一个由有指导学习向无指导学习转变的过程，因此现代过滤模型与包裹模型的根本区别在于对学习算法的使用方式。过滤模型首先利用数据的内在特性(如词频、词性)而不是聚类算法对原始特征集进行初步选择;最后将选出的特征子集用于聚类。反之，包裹模型将聚类算法与特征搜索、选择过程相结合，将无指导的学习算法应用于每个候选特征子集，利用聚类结果对特征子集进行评价，最终形成优化特征子集。<br>
混合模型着眼于使用-种特殊的算法将过滤模型与包裹模型相结合以获得尽可能好的性能，并且使得时间复杂度与过滤算法相近。</p>
<p>3、特征评价标准<br>
如何评价待选特征与降维目标的相关度是特征降维的关键问题之一。从评测对象上可以分为单边度量与双边度量两种。单边度量只考虑正特征，即最能标示其成员资格的特征，而忽略负特征即最能标示其非成员资格的特征，如相关性系数CC和几率评测OR。双边度量将正负特征结合考虑，如信息增益IG和卡方检测CHI (Chi-square) 。事实上,因为负特征在数据中的出现，较大程度地说明了该数据的无关性，所以负特征有助于确定消除无关数据，在不平衡的数据集合中对负特征的分析显得更为重要。<br>
在特征子集的优化选择过程中，使用不同的特征评价准则可能会得出不同的结果。常用的评价度量方法分为一致性度量和相关性度量两个大类。<br>
特征评价标准本身并不受特征子集选取策略的影响，即所有度量方法可以用于有指导的特征选取，也适用于无指导的特征选取。其区别在于:有指导的选择过程度量特征子集在分类中的能力;无指导的选择过程度量特征子集在聚类中的能力。</p>
<p>(1)一致性度量<br>
一致性度量致力于找出能够与完整特征集分类效果一致的最小特征子集，也可以将一致性 用相对的不一致性来解释。如果得到的不一致性为0, 则认为一致性为100%。<br>
(2)相关性度量<br>
相关度也被称为规范化相关性、相关系数、皮尔森关联、余弦相似度，被广泛用于描述模式分类和信号处理问题中两个向量之间的相似性。相关性度量基于以下思想:如果一个特征与某个类的关联性高到(或可预言到)使该特征与此类相关，同时此特征与其他相关特征的关联性不能达到任何相关特征都可以预言该特征的水平，则认为这个特征是对该分类任务的优秀特征。可以将国际上常用的相关度度量分为传统的线性相关性度量和基于信息理论的相关性度量。</p>
<p>·传统线性相关性度量<br>
在早期的研究中通常使用距离函数度量变量的相似性，例如欧氏距离和马氏距离。马氏距离相比欧式距离有很多优点。它不受量纲的影响，两点之间的马氏距离与原始数据的测量单位无关，由标准化数据和中心化数据(即原始数据与均值之差)计算出的两点之间的马氏距离相同。马氏距离还可以排除变量之间的相关性干扰，它的缺点是夸大了变化微小的变量的作用。<br>
线性相关系数又叫简单相关系数，一般用r表示，用来度量定量变量间的线性相关关系。将线性相关系数依据具体应用环境作适当校正可产生各种新的评价准则，有效提高特征选取的准确率，如最小平方回归误差、最大信息压缩指数。<br>
选择线性相关性作为分类中的特征评价准则有以下优点:有助于消除与类别相关度接近0的特征，即消除无关特征;有助于减小选中特征的冗余度，消除冗余特征。线性相关的缺点在于需要所有特征具有数值表示才能进行计算，并且不能捕获现实世界中非线性的关联。在简单相关系数的基础上又发展出了复相关系数、偏相关系数、典型相关系数等相关性度量方法。复相关又叫多重相关系数，是指因变量与多个自变量之间的相关关系，如某种商品的需求量与预期价格水平、职工收入水平等现象之间呈现多重相关关系。偏相关系数又被称为部分相关系数，反映校正其他变量后某- -变量与另–变量的相关关系。偏相关系数的假设检验等同于偏回归系数的t检验;复相关系数的假设检验等同于回归方程的方差分析。典型相关系数是指先对原来各组变量进行主成分分析，得到新的线性无关的综合指标，再用两组之间综合指标的直线相关系数来研究原两组变量间的相关关系。</p>
<p>·基于信息理论的相关性度量<br>
信息论是一门用数理统计方法研究信息度量、传递和变换规律的科学。基于信息理论的相关性度量关键在于评测从特征中获取的信息，如果从特征X中获取的信息比特征Y多，则可以认为特征X优于Y。目前使用最广泛的信息度量可以分为熵度量和互信息两大类。<br>
熵用于测量随机变量的不确定性;度量的是消息中所含的信息量，其中去除了由消息固有结构所决定的部分，如语言结构的冗余性以及语言中字母、词的使用频度等统计特性。X熵值的变化反映了给定Y的条件下X的额外信息，并将其称为信息增益IG:l(G(XlY)= H(X)-H(X|Y)。如果IG(XIY)&gt;IG(ZIY), 则认为特征Y与X之间的相关度高于特征Y与Z的相关度。信息增益对于任意的两个变量都具有对称性，然而信息增益计算会偏向于具有更多取值的特征,因此必须对其值进行规范化来保证信息增益具有可比性。FCBF算法使用对称不确定性SU补偿信息增益对多值特征的偏差，并将信息增益取值规范至[0, 1]。SU(X,Y) = 2[IG(X[Y)/ (H(X) +H(Y)]。SU值为1说明任-特征取值的信息都能完全预测其他特征值;若值为0,说明特征X与Y相互独立。使用SU度量的FCBF能够同时消除无关特征和冗余特征，算法时间复杂度为O(mnlogn)。其中: n为数据集中的特征数; m为数据集中的实例数。基于熵的度量方法也可以应用于评测连续性特征间的相关度，但要求提前将特征取值进行适当离散化。<br>
互信息是另一种常用的信息度量,用于评测随机变量间的依赖性，它总是具有对称、非负性，互信息的值越大说明变量间的依赖性越强。互信息取值为0当且仅当变量间相互独立。MIFS算法利用变量间的互信息在有指导的神经网络学习中进行特征选择，并且使用贪心策略进行搜索。</p>
<p>4、总结<br>
总的来说，特征降维模型，可以分为过滤模型和包裹模型两类，其区别在于是基于特征的内在特性还是基于学习算法的性能对特征进行选取。特征子集的搜索过程和选用的特征评价标准是特征降维的两个关键问题,根据具体应用环境制定适当的搜索策略与一定特征度量准则相结合能够有效地去除无关特征、冗余特征，实现高效的特征降维，提高机器学习的效率。随着自然语言处理技术的发展，以语义分析为基础的特征抽取技术必将得到进一步发展;如何捕捉现实世界中非线性的关联，将特征判别从距离空间转向相关度度量空间依然是机器学习的研究热点。特征降维的应用领域也从传统的静态文本分类、聚类转向对半结构化网络资源的数据挖掘，对音频、视频等多媒体资源的机器学习，以及对生物基因特征的分析识别等。</p>
<p>特征选择：</p>
<p>Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</p>
<p>Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</p>
<p>Embedded：集成方法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。</p>
<h1>特征选择</h1>
<p>特征选择是从特征集T={1… ,ts}中选择一个真子集T’={t1’,… ,ts’},满足(s’&lt;&lt;s) 。其中: s为原始特征集的大小; s’是选择后的特征集大小。特征选择不改变原始特征空间的性质，只是从原始特征空间中选择一部分重要的特征，组成一个新的低维空间</p>
<ul>
<li>冗余特征 Redundant features
<ul>
<li>purchase price of a product and the amount of sales tax paid</li>
</ul>
</li>
<li>无关特征 Irrelevant features
<ul>
<li>students’ ID is often irrelevant to the task of predicting students’ GPA</li>
</ul>
</li>
</ul>
<h2 id="特征创建-feature-creation"><a class="header-anchor" href="#特征创建-feature-creation">¶</a>特征创建 Feature Creation</h2>
<h3 id="特征抽取-feature-extraction"><a class="header-anchor" href="#特征抽取-feature-extraction">¶</a>特征抽取 Feature extraction</h3>
<p>Example: extracting edges from images</p>
<h3 id="特征构造-feature-construction"><a class="header-anchor" href="#特征构造-feature-construction">¶</a>特征构造 Feature construction</h3>
<p>Example: dividing mass by volume to get density</p>
<h3 id="映射到新空间-mapping-data-to-new-space"><a class="header-anchor" href="#映射到新空间-mapping-data-to-new-space">¶</a>映射到新空间 Mapping data to new space</h3>
<p>小波变换，傅立叶变换，短时傅立叶变换&lt;<a href="https://www.sohu.com/a/154009298_465219" target="_blank" rel="noopener">https://www.sohu.com/a/154009298_465219</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/数学-微积分/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/数学-微积分/" itemprop="url">数学-微积分</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:32:26+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<p><a href="https://blog.csdn.net/qq_32769481/article/details/84330857" target="_blank" rel="noopener">https://blog.csdn.net/qq_32769481/article/details/84330857</a></p>
<h1>极限</h1>
<p>序列极限/函数极限</p>
<p>连续函数</p>
<p>多元函数了连续性/极限</p>
<h1>微分</h1>
<p>无穷小量</p>
<p>一阶微分</p>
<p>高阶导数与高阶微分</p>
<p>多元函数微分</p>
<p>偏导数/全微分</p>
<p>复合函数/隐函数的微分法</p>
<p>方向导数/梯度</p>
<p>多元函数的微分中值定理与泰勒公式</p>
<p>隐函数存在定理</p>
<p>导数矩阵<br>
微分法则</p>
<p>Hessian矩阵</p>
<h2 id="泰勒公式-泰勒展开"><a class="header-anchor" href="#泰勒公式-泰勒展开">¶</a>泰勒公式/泰勒展开</h2>
<p>泰勒公式是一个用函数在某点的信息描述其附近取值的公式，拥有局部有效性<br>
$$<br>
n阶可微函数f(x)在x=a处的展开为\<br>
f(x) = \frac{f(a)}{0!} + \frac{f’(a)}{1!}(x-a) + \frac{f’’(a)}{2!}(x-a)<sup>2+…+\frac{f</sup>n(a)}{n!}(x-a)^n + R_n(x)\<br>
余项R_n(x) = \frac{f<sup>{(n+1)}(\xi)}{(n+1)!}(x-a)</sup>{(n+1)}\<br>
必须满足\lim_{x \to a}R_n(x) = 0\<br>
一般常取的在 x=0 处的展开（也称作麦克劳林的展开）<br>
$$</p>
<p>$$<br>
(1+z)^a进行泰勒展开，在z=0处展开，必须满足|z|&lt;1，才能让余项趋于0<br>
$$</p>
<p>$$<br>
一阶泰勒展开：f(x) = f(a) + f’(a)(x-a)\<br>
二阶泰勒展开：f(x) = f(a) + f’(a)(x-a) + \frac{f’’(a)}{2}(x-a)^2\<br>
迭代形式：假设x^t = x^{t-1} + \Delta t\<br>
展开f(x^t) = f(x^{t-1} + \Delta t)\<br>
\approx f(x^{t-1}) + f’(x^{t-1})\Delta t + \frac{f’’(x^{t-1})}{2}(\Delta t)^2<br>
$$</p>
<p><a href="https://blog.csdn.net/lanchunhui/article/details/51750194" target="_blank" rel="noopener">https://blog.csdn.net/lanchunhui/article/details/51750194</a></p>
<p><a href="https://blog.csdn.net/qq_42452134/article/details/83095736" target="_blank" rel="noopener">https://blog.csdn.net/qq_42452134/article/details/83095736</a></p>
<p>泰勒级数</p>
<h1>积分</h1>
<p>定积分</p>
<p>变上限定积分</p>
<p>不定积分</p>
<p>分部积分法</p>
<p>微分中值定理/柯西中值定理/洛必达法则</p>
<h1>向量代数/空间解析几何</h1>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/数学-离散数学/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/数学-离散数学/" itemprop="url">数学-离散数学</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:32:26+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li>数理逻辑。包括命题、公式的等价和蕴含等。</li>
<li>集合和关系。包括集合和集合的运算、关系等</li>
<li>数函数和递推关系。数函数的母函数、递推关系等。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/数学-数值方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/数学-数值方法/" itemprop="url">数学-数值方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:31:55+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<ul>
<li>
<p>Cholesky分解 Cholesky factorization</p>
<ul>
<li>以线性回归为例<br>
$$<br>
\mathtt{solve}<em>{\hat \theta</em>{\mathtt{MLE}}} (\underbrace{X^TX}<em>C) \hat \theta</em>{MLE} = \underbrace{X^TY}<em>d\<br>
计算对称矩阵:C = X^TX = \sum</em>{i=1}<sup>nx_ix_i</sup>T \quad O(np^2)\<br>
计算向量:d = X^TY = \sum_{i=1}^nx_iy_i \quad O(np)\<br>
Cholesky分解:LL^T = C \quad O(p^3) \quad L是下三角矩阵\<br>
\mathtt{Forward \ subs}:Lz = d \quad O(p^2)\<br>
\mathtt{Backward \ subs}:L^T\theta_{\mathtt{MLE}} = z \quad O(p^2)\<br>
\theta_{\mathtt{MLE}} \in \mathbb R^{p} \quad X \in \mathbb R^{np} \quad Y \in \mathbb R^{n} \quad C \in \mathbb R^{pp} \quad d \in \mathbb R^{p} \quad L \in \mathbb R^{pp} \quad z \in \mathbb R^{p}<br>
$$</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>QR分解 QR factorization</p>
</li>
<li>
<p>求解三角矩阵的等式 Solving Triangular System</p>
<p><img src="/2019/03/08/数学-数值方法/image-20190813092151836.png" alt="image-20190813092151836"></p>
</li>
</ul>
<p><img src="/2019/03/08/数学-数值方法/image-20190813092211223.png" alt="image-20190813092211223"></p>
<p>从下往上，依次求解</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/数学-线性代数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/数学-线性代数/" itemprop="url">数学-线性代数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:31:55+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<p><strong>A的秩是ra B的秩是rb 问AB的秩是多少</strong></p>
<p>奇异值分解SVD<br>
奇异值分解（Singular Value Decomposition, SVD）是线性代数中一种重要的矩阵分解，在信号处理、统计学等领域有重要应用。</p>
<p>PCA 的实现一般有两种，一种是通过特征值分解实现，另一种就是用奇异值分解来实现。但特征值分解只是针对方针而言的，当处理 m*n 的矩阵时，就需要特征值分解了。</p>
<p>在吴军老师的《数学之美》中也有提到用 SVD 做文本分类的篇幅，提到的例子是这样的：首先用一个 M * N 的大型矩阵 A 来描述一百万篇文章和五十万个词的关联性。矩阵中，每一行对应一篇文章，每一列对应一个词。</p>
<p>（挂掉的img）</p>
<p>在该矩阵中，M=1,000,000 ，N=500,000 ，第 i 行第 j 列的元素是字典中第 j 个词在第 i 篇文章中出现的加权词频（比如，TF/IDF) 。奇异值分解就是把这样一个大矩阵，分解成三个小矩阵相乘：</p>
<p>（挂掉的img）（矩形面积的大小也对应了信息量的大小～）</p>
<p>这个大矩阵被分解成一个100万乘100的矩阵 X ，一个100乘100的矩阵 B ，以及一个100乘50万的矩阵 Y 。分解后，相应的存储量和计算量都小了三个数量级以上。</p>
<p>第一个矩阵 X 中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的相关性，数值越大越相关。最后一个矩阵 Y 中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章雷之间的相关性。因此，我们只要对关联矩阵 A 进行一次奇异值分解，w 我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。</p>
<p>求矩阵的<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E9%80%86%E7%9F%A9%E9%98%B5%23%E4%BC%B4%E9%9A%8F%E7%9F%A9%E9%98%B5%E6%B3%95" target="_blank" rel="noopener">逆矩阵</a>的常用方法有两种：</p>
<ol>
<li>伴随矩阵法</li>
<li>初等变换法</li>
</ol>
<p>高斯消元法<br>
高斯消元法（Gaussian Elimination），是线性代数中的一个算法，可用来为线性方程组求解，求出矩阵的秩，以及求出可逆方阵的逆矩阵。当用于一个矩阵时，高斯消元法会产生出一个“行梯阵式”。</p>
<p>值得提一下的是，虽然该方法以数学家卡尔·高斯命名，但最早出现于中国古籍《九章算术》，成书于约公元前150年。</p>
<p>复杂度：高斯消元法的算法复杂度是 O（n^3）；这就是说，如果系数矩阵的是 n × n，那么高斯消元法所需要的计算量大约与 n^3成比例。</p>
<p>乘积、内积、秩</p>
<h1>矩阵与方程组</h1>
<p>线性方程组，行阶梯形，矩阵算术，矩阵代数，初等矩阵，分块矩阵，逆矩阵</p>
<h1>行列式</h1>
<p>克拉默法则，<a href="http://blog.csdn.net/vernice/article/details/48512203" target="_blank" rel="noopener">http://blog.csdn.net/vernice/article/details/48512203</a></p>
<h1>向量空间</h1>
<p>定义，子空间，线性无关，线性空间，基变换，行空间和列空间，秩</p>
<h1>线性变换</h1>
<p>定义，线性变换的矩阵表示，相似性</p>
<h1>正交性</h1>
<p>Rn中的标量积，正交子空间，最小二乘问题，内积空间，正交集，格拉姆施密特正交化过程，正交多项式，正交投影</p>
<h1>特征值</h1>
<p>特征值，特征向量，相似性</p>
<p>奇异性，奇异值分解</p>
<p>线性微分方程组</p>
<p>对角化</p>
<p>主成分分析</p>
<p>二次型</p>
<p>Hermite矩阵，对称矩阵</p>
<p>正定矩阵，半正定矩阵</p>
<p>酋矩阵，酋相似，酋等价</p>
<p>正矩阵/非负矩阵</p>
<p>标准型</p>
<p>Jordan标准型</p>
<p>特征值的位置与摄动</p>
<p>gersgorin圆盘</p>
<h1>向量/矩阵范数</h1>
<p>内积和范数</p>
<p>矩阵范数</p>
<h1>对称矩阵</h1>
<ul>
<li>如果A是对称矩阵，那么A的逆矩阵是对称矩阵<br>
$$<br>
A = A^T\<br>
(A<sup>{-1})</sup>T = (A<sup>T)</sup>{-1} = A^{-1}\<br>
因此A^{-1}是对称矩阵<br>
$$</li>
</ul>
<h1>矩阵的迹</h1>
<p><a href="http://blog.csdn.net/acdreamers/article/details/44662633" target="_blank" rel="noopener">http://blog.csdn.net/acdreamers/article/details/44662633</a></p>
<p><a href="https://blog.csdn.net/kavin_star/article/details/80467179" target="_blank" rel="noopener">https://blog.csdn.net/kavin_star/article/details/80467179</a><br>
$$<br>
一个n\times n的矩阵A的迹是指A的主对角线上各元素的和,记作tr(A)\<br>
即tr(A) = \sum_{i = 1}^n a_{ii}<br>
$$</p>
<p>$$<br>
定理:tr(A) = tr(A^T)\<br>
定理:tr(A+B) = tr(A)+tr(B)\<br>
定理:tr(rA) = r \times tr(A)\<br>
$$</p>
<p>$$<br>
定理:tr(AB) = tr(BA) \ A \in \mathbb R^{nm} \ B \in \mathbb R^{mn}\<br>
证明:tr(AB) = \sum_{i=1}^n(AB)<em>{ii} = \sum</em>{i=1}<sup>n\sum_{j=1}</sup>mA_{ij}B_{ji} = \sum_{j=1}<sup>m\sum_{i=1}</sup>nB_{ji}A_{ij} = \sum_{j=1}^m(BA)_{jj} = tr(BA) \<br>
引理:tr(ABC) = tr(CAB) = tr(BCA) \ 把AB,BC当做整体\<br>
但是tr(ABC) \ne tr(ACB)\<br>
$$</p>
<p>矩阵的秩</p>
<p>SVD奇异值分解</p>
<p>齐次坐标，齐次矢量（增广矢量），齐次点，齐次方程</p>
<p>自由度</p>
<p>二次约束</p>
<p>正交旋转矩阵</p>
<p>仿射</p>
<p>齐次矩阵，尺度量</p>
<p>群</p>
<p>完整同态映射</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/数学-信息论/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chenxr">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chenxr's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/08/数学-信息论/" itemprop="url">数学-信息论</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-08T22:30:35+08:00">
                2019-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="熵值"><a class="header-anchor" href="#熵值">¶</a>熵值</h3>
<p>一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。所以说，信息熵可以被认为是系统有序化程度的一个度量。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/10/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Chenxr</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">92</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chenxr</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
